# Eigendecomposition {#sec-matrixdc-eigen}

We start off with the so called _eigendecomposition_.
The main idea is to compute a decomposition (or factorization) of a square matrix into a canonical form.
We will represent a matrix by its _eigenvalues_ and _eigenvectors_.
In order to do so we need to recall some more definitions from our favourite linear algebra book, such as @GoluLoan.

::: {.callout appearance="simple"}
## Definition - determinant
If $A = (a) \in \mathbb{R}^{1\times1}$ (a single number), then its _determinant_ is given by $\det(A) = a$. Now, the determinant of $A \in \mathbb{R}^{n\times n}$ is defined in terms of order-$(n-1)$ determinants:
$$
\det(A) = \sum_{j=1}^n (-1)^{j+1} a_{1j}\det(A_1j),
$$
where $A_1j$ is a $(n-1) \times (n-1)$ matrix obtained by deleting the first row and the $j$th column of $A$.

[Compare @GoluLoan, pp. 66-67]
:::

Some important properties of the determinant are:

1. for two matrices $A$ and $B$ in $\mathbb{R}^{n\times n}$
$$
\det(AB) = \det(A)\det(B),
$$
1. for the transpose $A^T$ of a matrix $A$
$$
\det(A^T) = \det(A),
$$
1. for a scalar $\alpha\in\mathbb{R}$ and a matrix $A$
$$
\det(\alpha A) = \alpha^n\det(A),
$$
1. we call a matrix $A$ _nonsingular_ (i.e. invertible) if and only if
$$
\det(A) \not= 0,
$$
1. for an invertible matrix $A^{-1}$
$$
\det\left(A^{-1}\right) = \frac{1}{\det(A)},
$$

So let us check in `numpy`:
```{python}
#| classes: styled-output
#| error: true
import numpy as np
from numpy import linalg as LA

A = np.array([[1, 2, 3], 
              [2, 5, 1],
              [2, 7, 9]])
det_A = LA.det(A)
print(f"{det_A=}")

A_T = np.transpose(A)
det_A_T = LA.det(A_T)
print(f"{det_A_T=}")

X = LA.inv(A)
det_X = LA.det(X)
print(f"{det_X=}")

print(f"{det_X*det_A=}")

det_Am = LA.det(-A)
print(f"{det_Am=}")
```

::: {.callout appearance="simple"}
## Definition - eigenvalues
For a matrix $A \in \mathbb{C}^{n\times n}$ the _eigenvalues_ are the roots of the _characteristic polynomial_ 
$$
p(\lambda) = \det(A - \lambda I).
$$
Consequently, every $n\times n$ matrix has exactly $n$ eigenvalues. Note that for a real matrix the eigenvalues might still be in $\mathbb{C}$.

The set of all eigenvalues of $A$ is denoted by
$$
\lambda(A) = \left\{\lambda : \det(A-\lambda I)=0\right\}.
$$
If all of the eigenvalues are real (in $\mathbb{R}$) we can sort them from largest to smallest
$$
\lambda_n(A) \leq \cdots \leq \lambda_2(A) \leq \lambda_1(A),
$$
and we denote the largest by $\lambda_{max}(A) = \lambda_1(A)$ and the smallest by $\lambda_{min}(A) = \lambda_n(A)$, respectively.

[Compare @GoluLoan, pp. 66-67]
:::

From the above properties of the determinant we can conclude that, if $X \in \mathbb{C}^{n\times n}$ is nonsingular and $B = X^{-1} A X$, then $A$ and $B$ are called _similar_ and two similar matrices have exactly the same eigenvalues.

```{python}
#| classes: styled-output
#| error: true
lam = LA.eigvals(A)
lam_sort = np.sort(lam)

print(f"{lam_sort=}")
```

::: {.callout appearance="simple"}
## Definition - eigenvector
If $\lambda \in \lambda(A)$, then there exists a nonzero vector $v$ so that 
$$
Av = \lambda v \iff (A - \lambda I)v = 0.
$$
Such a vector is called _eigenvector_ of $A$ associated with $\lambda$.
:::

::: {.callout appearance="simple"}
## Definition - eigendecomposition
If $A \in \mathbb{C}^{n\times n}$ has $n$ (linear) independent eigenvectors $v_1, \ldots, v_n$ and $Av_i=\lambda_i v_i$, for $i=1:n$, than $A$ is _diagonalizable_. 

If we combine the eigenvectors to a matrix
$$
V = \left[v_1 | \cdots | v_n\right],
$$
then
$$
V^{-1} A V = \operatorname{diag}(\lambda_1, \ldots, \lambda_n) = \Lambda = \left[
    \begin{array}{cccc} \lambda_1 & 0 & \dots & 0\\
                        0 & \lambda_2 & \ddots & \vdots \\  
                        \vdots & \ddots & \ddots & 0\\
                        0 & \dots & 0 & \lambda_n \\  \end{array}
\right].
$$
This is called the _eigendecomposition_ of the matrix $A$.

[Compare @GoluLoan, pp. 66-67]
:::

Not all matrices $A \in \mathbb{R}^{n\times n}$ are diagonalizable.

```{python}
#| classes: styled-output
#| error: true
lam, V = LA.eig(A)
A_eigen = np.diag(lam)
print(f"{A_eigen=}")
print(f"{V=}")

v_1 = (A @ V[:, 0]) / lam[0]
print(f"{v_1=}")

A_eigen2 = LA.inv(V) @ A @ V
print(f"{A_eigen2=}")

np.testing.assert_almost_equal(A_eigen, A_eigen2)
```

::: {.callout-caution appearance="simple" collapse="true" icon=false}
## {{< fa circle-check >}} Example - matrix decomposition - system of linear equations

For a system of linear equations $Ax=b$ we get
$$
\begin{array}{lll}
A x & = b  & \iff \\
V \Lambda V^{-1} x & = b  & \iff \\
\Lambda V^{-1} x & = V^{-1} b  & \iff \\
V^{-1} x & = \Lambda^{-1}V^{-1} b  & \iff \\
x & = V\Lambda^{-1}V^{-1} b  & \iff
\end{array}
$$
As $\Lambda^{-1} = \operatorname{diag}\left(\lambda_1^{-1}, \ldots, \lambda_n^{-1}\right)$ this is easy to compute once we have the eigenvalue decomposition.
:::

We can use what we have learned about the basis of a vector space and the transformation between two bases, see @sec-intro-linearalgebra-basis to get a different interpretation of the eigendecomposition.

We recall, if $x$ is represented in the standard basis with matrix $I$ we can change to the basis represented by $V$ and therefore $\hat{x} = V^{-1}x$.

Consequently, the equation
$$
A = V \Lambda V^{-1} 
$$
means that in the basis created by $V$ the matrix $A$ is represented by a diagonal matrix.

::: {.callout-caution appearance="simple" collapse="true" icon=false}
## {{< fa circle-check >}} Example - matrix decomposition - differential equation

In this example we use the eigendecomposition to efficiently solve a system of differential equations
$$
\dot{x} = A x.
$$
By changing to the basis $V$ and using the notation $\hat{x}=z$
$$
z = V^{-1}x \iff x = Vz,
$$
and if follows
$$
\begin{array}{lll}
\dot{x} = A x & \iff V \dot{z} &= A V z \\
              & \iff \dot{z} &= V^{-1} A V z \\
              & \iff \dot{z} &= \Lambda z
\end{array}.
$$
So for an initial value $z_0$ the solution in $t$ is
$$
z(t) = \operatorname{diag}\left(e^{t\lambda_1}, \ldots, e^{t\lambda_n}\right) z_0.
$$

We often say that it is now a _decoupled_ differential equation.
:::

## Low-rank approximation of a square matrix

We can use the eigen 