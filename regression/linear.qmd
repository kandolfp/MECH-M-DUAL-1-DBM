# Linear Regression {#sec-regression-linear}

The general idea of linear regression is to approximate a _point cloud_ by a mathematical function, this is often called _curve fitting_ and it is closely related to optimization techniques, [compare @Brunton2022, Section 4.1].


Let us assume that we want to establish a relationship between our $n$ observations with $m$ independent variables. In order to get the notation down correctly we should describe the variables more closely.

We have $n$ observations (the $k$-th representation is denoted by $y_k$) resulting in a vector:
$$
y = \left[
    \begin{array}{c} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{array}
    \right].
$$

We have $m$ independent variables and consequently for each of this $n$ observations this results in a matrix
$$X = \left[
    \begin{array}{c} X_{1-} \\ X_{2-} \\ X_{3-} \\ \vdots \\ X_{n-} \end{array}
    \right] = \left[X_{-1}, \dots, X_{-m}\right],$$
where the second representation more closely fits to our idea of the $m$ independent variables. 

Let us model the relation in a linear fashion with the parameters $b$
as 
$$
y_k \overset{!}{=} X_{k1} b_1 + X_{k2} b_2 + \cdots + X_{km} b_m, \quad 1\leq k \leq n,
$$
or in short
$$
y = Xb = f(X, b).
$$


Let us assume for a moment we already have a realisation of $f$, i.e. we know the  parameters $b$, we get the error of the approximation as
$$
\mathbf{e} = y - f(X, b)\quad \Leftrightarrow \quad \mathbf{e}_k = y_k - f(X_{k-}, b).
$$ {#eq-fiterror}

There are various possibilities to select the metric for minimizing $\mathbf{e}$ and therefore characterizing the quality of the _fit_.
This is done by selecting the underlying norm.
Most commonly we use the $1$-norm, the $2$-norm and the $\infty$-norm, i.e.
$$
E_1 (f) = \frac1n \sum_{k=1}^n|f(X_{k-}, b) - y_k |,
$$ {#eq-fit1}
for the $1$-norm or mean absolute error,
$$
E_2 (f) = \sqrt{\frac1n \sum_{k=1}^n|f(X_{k-},b) - y_k |^2},
$$ {#eq-fit2}
for the $2$-norm or least-square error,
$$
E_\infty (f) = \max_{k}|f(X_{k-}, b) - y_k |
$$ {#eq-fit3}
for the $\infty$-norm or maximum error.
Of course $p$-norms work as well
$$
E_p (f) = \left(\frac1n \sum_{k=1}^n|f(X_{k-}, b) - y_k |^p\right)^{1/p}.
$$
If we go back and we want to solve @eq-fiterror for $b$ we have different

Depending on the selected norm we get different realisations.
To illustrate this we use the example from [@Brunton2022, pp. 136-167].

```{python}
#| label: fig-linear_reg
#| fig-cap: "Line fit with different norms. Top without outliers, bottom with one outlier."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
import numpy as np
import scipy.optimize
%config InlineBackend.figure_formats = ["svg"]

# Function definitions
def fit1(x0, t):
    x, y = t
    return np.max(np.abs(x0[0] * x + x0[1] - y))
def fit2(x0, t):
    x, y = t
    return np.sum(np.abs(x0[0] * x + x0[1] - y))
def fit3(x0, t):
    x, y = t
    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))

# The data
x = np.arange(1, 11)
y = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])
z = np.array([0.2, 0.5, 0.3, 3.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])
t = (x, y)
t2 = (x, z)

x0 = np.array([1, 1])

p1 = scipy.optimize.fmin(fit1, x0, args=(t,), disp=False)
p2 = scipy.optimize.fmin(fit2, x0, args=(t,), disp=False)
p3 = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)

p12 = scipy.optimize.fmin(fit1, x0, args=(t2,), disp=False)
p22 = scipy.optimize.fmin(fit2, x0, args=(t2,), disp=False)
p32 = scipy.optimize.fmin(fit3, x0, args=(t2,), disp=False)

xf = np.arange(0, 11, 0.1)
y1 = np.polyval(p1, xf)
y2 = np.polyval(p2, xf)
y3 = np.polyval(p3, xf)

y12 = np.polyval(p12, xf)
y22 = np.polyval(p22, xf)
y32 = np.polyval(p32, xf)


fig = plt.figure()
ax = fig.add_subplot(2, 1, 1)

ax.plot(x, y, "o", color="r", label="observations")
ax.plot(xf, y1, label=r"$E_\infty$")
ax.plot(xf, y2, "--", linewidth=2, label=r"$E_1$")
ax.plot(xf, y3, ":", linewidth=2, label=r"$E_2$")
ax.set_ylim(0, 4)
ax.set_xlim(0, 11)
ax.set_title("Observations with no outliers.")
ax.legend()

ax = fig.add_subplot(2, 1, 2)
ax.plot(x, z, "o", color="r", label="observations")
ax.plot(xf, y12, label=r"$E_\infty$")
ax.plot(xf, y22, "--", linewidth=2, label=r"$E_1$")
ax.plot(xf, y32, ":", linewidth=2, label=r"$E_2$")
ax.set_ylim(0, 4)
ax.set_xlim(0, 11)
ax.set_title("Observations with outliers.")
# ax.legend()
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)
plt.show()
```
If we use @eq-fit2 it is called the _least square fit_.
