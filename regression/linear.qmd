---
lightbox: true
---
# Linear Regression {#sec-regression-linear}

The general idea of linear regression is to approximate a _point cloud_ by a mathematical function, this is often called _curve fitting_ and it is closely related to optimization techniques, [compare @Brunton2022, Section 4.1].


Let us assume that we want to establish a relationship between our $n$ observations with $m$ independent variables. In order to get the notation down correctly we should describe the variables more closely.

We have $n$ observations (the $k$-th representation is denoted by $y_k$) resulting in a vector:
$$
y = \left[
    \begin{array}{c} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_n \end{array}
    \right].
$$

We have $m$ independent variables and consequently for each of this $n$ observations this results in a matrix
$$X = \left[
    \begin{array}{c} X_{1-} \\ X_{2-} \\ X_{3-} \\ \vdots \\ X_{n-} \end{array}
    \right] = \left[X_{-1}, \dots, X_{-m}\right],$$
where the second representation more closely fits to our idea of the $m$ independent variables. 

Let us model the relation in a linear fashion with the parameters $b$
as 
$$
y_k \overset{!}{=} X_{k1} b_1 + X_{k2} b_2 + \cdots + X_{km} b_m, \quad 1\leq k \leq n,
$$
or in short
$$
y = Xb = f(X, b).
$$

::: {.callout-note}
In literature the parameters are most of the time called $\beta$ but for the sake of keeping notation consistent we opt for $b$.
:::

::: {.callout-tip}
Usually the convention is to define $X_{-1}=1$ as constant 1 at each position.
The corresponding $b_1$ is called the _intercept_.
We even find this convention if theory tells us $b_1$ is $0$ as some procedures assume this term to exist.
:::

::: {.callout-note}
We talk of a linear model, as long as it is linear in the parameters $b$.
It might happen that regressors have a non linear relation.
:::

Let us assume for a moment we already have a realisation of $f$, i.e. we know the  parameters $b$, we get the error of the approximation as
$$
\mathbf{e} = y - f(X, b)\quad \Leftrightarrow \quad \mathbf{e}_k = y_k - f(X_{k-}, b).
$$ {#eq-fiterror}

There are various possibilities to select the metric for minimizing $\mathbf{e}$ and therefore characterizing the quality of the _fit_.
This is done by selecting the underlying norm.
Most commonly we use the $1$-norm, the $2$-norm and the $\infty$-norm, i.e.
$$
E_1 (f) = \frac1n \sum_{k=1}^n|f(X_{k-}, b) - y_k |,
$$ {#eq-fit1}
for the $1$-norm or mean absolute error,
$$
E_2 (f) = \sqrt{\frac1n \sum_{k=1}^n|f(X_{k-},b) - y_k |^2},
$$ {#eq-fit2}
for the $2$-norm or least-square error,
$$
E_\infty (f) = \max_{k}|f(X_{k-}, b) - y_k |
$$ {#eq-fit3}
for the $\infty$-norm or maximum error.
Of course $p$-norms work as well
$$
E_p (f) = \left(\frac1n \sum_{k=1}^n|f(X_{k-}, b) - y_k |^p\right)^{1/p}.
$$
If we go back and we want to solve @eq-fiterror for $b$ we have different realisations, depending on the selected norm.

To illustrate this we use the example from [@Brunton2022, pp. 136-167].

```{python}
#| label: fig-linear_reg
#| fig-cap: "Line fit with different norms. Top without outliers, bottom with one outlier."
#| fig-subcap:
#|   - "Observations with no outliers."
#|   - "Observations with outliers." 
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
import numpy as np
import scipy.optimize
%config InlineBackend.figure_formats = ["svg"]

# Function definitions
def fit1(x0, t):
    x, y = t
    return np.max(np.abs(x0[0] * x + x0[1] - y))
def fit2(x0, t):
    x, y = t
    return np.sum(np.abs(x0[0] * x + x0[1] - y))
def fit3(x0, t):
    x, y = t
    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))

# The data
x = np.arange(1, 11)
y = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])
z = np.array([0.2, 0.5, 0.3, 3.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])
t = (x, y)
t2 = (x, z)

x0 = np.array([1, 1])

p1 = scipy.optimize.fmin(fit1, x0, args=(t,), disp=False)
p2 = scipy.optimize.fmin(fit2, x0, args=(t,), disp=False)
p3 = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)

p12 = scipy.optimize.fmin(fit1, x0, args=(t2,), disp=False)
p22 = scipy.optimize.fmin(fit2, x0, args=(t2,), disp=False)
p32 = scipy.optimize.fmin(fit3, x0, args=(t2,), disp=False)

xf = np.arange(0, 11, 0.1)
y1 = np.polyval(p1, xf)
y2 = np.polyval(p2, xf)
y3 = np.polyval(p3, xf)

y12 = np.polyval(p12, xf)
y22 = np.polyval(p22, xf)
y32 = np.polyval(p32, xf)


fig = plt.figure()

plt.plot(x, y, "o", color="r", label="observations")
plt.plot(xf, y1, label=r"$E_\infty$")
plt.plot(xf, y2, "--", linewidth=2, label=r"$E_1$")
plt.plot(xf, y3, ":", linewidth=2, label=r"$E_2$")
plt.ylim(0, 4)
plt.xlim(0, 11)
plt.legend(loc="upper left")
plt.gca().set_aspect(1)
plt.grid(visible=True)
plt.show()

plt.plot(x, z, "o", color="r", label="observations")
plt.plot(xf, y12, label=r"$E_\infty$")
plt.plot(xf, y22, "--", linewidth=2, label=r"$E_1$")
plt.plot(xf, y32, ":", linewidth=2, label=r"$E_2$")
plt.ylim(0, 4)
plt.xlim(0, 11)
plt.legend(loc="upper left")
plt.gca().set_aspect(1)
plt.grid(visible=True)
plt.show()
```
If we use @eq-fit2 the regression fit is called the _least square fit_.

## Ordinary Least Square

It is worth looking into the least square solution 
$$
E_2 (f) = \sqrt{\frac1n \sum_{k=1}^n|\underbrace{f(X_{k-},b) - y_k}_{\mathbf{e}_k} |^2},
$$
more closely.
We can interpret it as the optimization problem
$$
b = \underset{v}{\operatorname{argmin}} \| y - Xv\|_2
$$
and with some linear algebra we get
$$
\begin{array}{ccl}
b &= &\underset{v}{\operatorname{argmin}} \| y - Xv\|_2,\\
  &= &\underset{v}{\operatorname{argmin}} \langle y -Xv, y -Xv\rangle,\\
  &= &\underset{v}{\operatorname{argmin}} (y -Xv)^T (y -Xv),\\
  &= &\underset{v}{\operatorname{argmin}} y^Ty - y^T X v - v^T X^T y + v^T X^T X v.\\
\end{array}
$$
In order to find a solution we compute the derivative with respect to $v$ set it to $0$ and simplify, i.e.
$$
\frac{\mathrm{d}}{\mathrm{d}\,v} y^Ty- y^T X v - v^T X^T y + v^T X^T X v = - 2X^Ty + 2 X^TXv 
$$
and
$$
v = (A^TA)^{-1}A^Ty \equiv A^\dagger y.
$$
We recall, that $A^\dagger$ is called the Moore-Penrose pseudo-inverse, see @def-pinv.

### Alternative computation

The pseudo-inverse provides us with the optimal solution but for large systems the computation can be inefficient, or more precisely, there are more efficient ways to get the same results.

Following [@Brunton2022, pp. 137-138] we can find an alternative for the above example in @fig-linear_reg.

We want to fit the data points $(x_i, y_i)$ with the function $f(x) = b_2 x + b_1$ resulting in the error
$$
E_2(f) = \sqrt{\frac1n \sum_{k=1}^n(b_2 x_k + b_1 - y_k )^2}.
$$
A solution that minimizes the above equation also minimizes
$$
E_2 = \sum_{k=1}^n(b_2 x_k + b_1 - y_k )^2
$$
and we find the solution by partial differentiation
$$
\frac{\mathrm{d} E_2}{\mathrm{d}\,b_1} = 0 \Leftrightarrow \sum_{k=1}^n 2 (b_2 x_k + b_1 - y_k ) = 0,
$$
$$
\frac{\mathrm{d} E_2}{\mathrm{d}\,b_2} = 0 \Leftrightarrow \sum_{k=1}^n 2 (b_2 x_k + b_1 - y_k ) x_k = 0,
$$
and this results in the system
$$
\left[
\begin{array}{cc}
n & \sum_k x_k \\
\sum_k x_k & \sum_k x_k^2
\end{array}
\right]
\left[
\begin{array}{c}
b_1\\b_2
\end{array}
\right]
= 
\left[
\begin{array}{c}
\sum_k y_k\\\sum_k x_k y_k
\end{array}
\right].
$$ {#eq-alternativeregression}
This ansatz can be extended to polynomials of degree $k$, where the result is always a $(k+1) \times (k+1)$ matrix.

::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-lregfallenobject}

## Example - parameter estimation of a falling object

Just because we deal with linear regression this does not means that the model needs to be linear too.
As long as we are linear in the  parameters $b$ we can apply our findings, even for non linear _independent variables_.

To illustrate this, let us consider an object falling without aerodynamic drag, described by the differential equation
$$
m \ddot{y}(t) = -m g,
$$
for the gravitational constant $g$.
Integration with respect to $t$ results in 
$$
y(t) = y(0) + v(0) t - \frac{g}{2} t^2.
$$
So we get
$$
X{k-} = \left[1 t_k -\frac{g}{2} t^2\right]
\quad \text{and} \quad
b = \left[
\begin{array}{c}
\sum_k y_0\\ v_0 \\ g
\end{array}
\right]
$$
or in the long form, for $t_{k+1}-t_k = 0.1$
$$
X = \left[
\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0.1 & -0.005 \\
1 & 0.2 & -0.020 \\
1 & 0.3 & -0.045 \\
1 & 0.4 & -0.080 \\
\vdots & \vdots & \vdots 
\end{array}
\right]
$$
and we can, for example, estimate our unknowns $y_0$, $v_0$, and $g$ by
$$
b = X^\dagger b.
$$

::::
:::

## Data Linearization

Quite often it is possible to linearize the our model at hand.
For example if we want to fit for 
$$
f(x, b) = b_2 \exp(b_1 x),
$$ {#eq-lregnonlinear}

and use the same derivation as for @#eq-alternativeregression we end up with the corresponding system as
$$
b_2 \sum_k x_k \exp(2 b_1 x_k) - \sum_k x_k y_k \exp(b_1 x_k) =0,
$$
$$
b_2 \sum_k \exp(2 b_1 x_k) - \sum_k y_k \exp(b_1 x_k) =0.\\
$$

This non-linear system can not be solved in a straight forward fashion but we can avoid it by linearization with the simple transformation
$$
\begin{array}{r}
\hat{y} = \ln(x), \\
\hat{x} = x, \\
b_3 = \ln b_2,
\end{array}
$$
and taking the natural logarithm of both sides of @#eq-lregnonlinear and simplifying
$$
\ln y = \ln(b_2 \exp(b_1 x)) = \ln(b_2) + b_1 x.
$$
Now all that is left to apply $\ln$ to the data $y$ and solve the linear problem.
In order to apply it to the original function the parameters transform needs to be reversed.

::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-population}

## Example - population of the world

We take a look at the population growth were the data is kindly provided by @owid-population-growth. Have a look at there excellent work on [ourworldindata.org](https://ourworldindata.org/).


```{python}
#| label: fig-reg_wp
#| fig-cap: "Line fit with different norms. Top without outliers, bottom with one outlier."
#| fig-subcap: 
#|   - "World population with regression lines normal scale."
#|   - "World population with regression lines logarithmic scale." 
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Show the code for the figure"

from owid.catalog import charts
import numpy as np
import scipy.optimize
import plotly.graph_objects as go
from plotly.subplots import make_subplots

df = charts.get_data("https://ourworldindata.org/grapher/population?country=~OWID_WRL")
data = df[df["entities"] == "World"]
x = data["years"].to_numpy()
y = data["population"].to_numpy()
ylog = np.log(y)

def fit3(x0, t):
    x, y = t
    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))

start = [-np.inf, 0, 1700, 1900, 1980]

yest = []

for s in start:
    filter = x >= s
    t = (x[filter], ylog[filter])
    x0 = np.array([1, 1])
    b = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)
    yest.append(np.exp(b[1]) * np.exp(b[0] * x))

fig = go.Figure()
fig2 = go.Figure()
fig.add_trace(go.Scatter(mode="markers", x=data["years"], y=data["population"], name="data"))
fig2.add_trace(go.Scatter(mode="markers", x=data["years"], y=data["population"], name="data"))

for i, ye in enumerate(yest):
    fig.add_trace(go.Scatter(x=x, y=ye, name=f"fit from {start[i]}"))
    fig2.add_trace(go.Scatter(x=x, y=ye, name=f"fit from {start[i]}"))


fig.update_xaxes(title_text="year", range=[1700, 2023])
fig.update_yaxes(title_text="population")
fig.update_layout(legend=dict(
    yanchor="top",
    y=0.99,
    xanchor="left",
    x=0.01
))
fig.show()

fig2.update_xaxes(title_text="year", range=[1700, 2023])
fig2.update_yaxes(title_text="population", type="log", range=[8.5,10])
fig2.update_layout(legend=dict(
    yanchor="top",
    y=0.99,
    xanchor="left",
    x=0.01
))
fig2.show()
```
::::
:::

Next, we are going to look into actual nonlinear regression.