---
lightbox: true
---
# Optimizers {#sec-regression-optimizers}

As we have seen in the previous section the task of regression usually results in an optimization problem.
It is worth investigating this further by looking closely on the 
$$
A x = b
$$ {#eq-axb}
problem for different dimensions of $A$.

We investigate the impact of restricting our solution not just by @eq-axb but with the help of the $l_1$ and $l_2$ norm imposed on the solution $x$.
As we have seen in @fig-linear_reg the choice of norm has an implication on the result and the same is true here.

::: {.callout-important}
This section is mainly based on @Brunton2022, Section 4.3.
:::

## Over-Determined Systems

We speak of an over-determined system if we have more rows than columns, i.e. $A$ is tall and skinny and in general there is no solution to @eq-axb but rather we minimize the error according to a norm, see @ec-regr-linear-ols.
If we further impose a restriction on $x$ we can select a more specific solution.

The generalized form is
$$
x = \underset{v}{\operatorname{argmin}} \|Av - b\|_2 + \lambda_1 \|v\|_1 + \lambda_2\|v\|_2
$$ {#eq-reg-optimizers}
where the parameters $\lambda_1$ and $\lambda_2$ are called the _penalization coefficients_, with respect to the norm.
Selecting these coefficients is the first step towards _model selection_.

Let us have a look at this in action for solving a _random_ system with different parameters $\lambda_1$ and setting $\lambda_2$.

### LASSO 

The _least absolute shrinkage and selection operator_ LASSO solves @eq-reg-optimizers with $\lambda_1 > 0$ and $\lambda_2=0$, i.e. only optimizing with the $l_1$ norm.
The theory tells us that for increasing $\lambda_1$ we should get more and more zeros in our solution $x$.

```{python}
#| label: fig-opt_lambda1
#| fig-cap: "LASSO regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

m = 500
n = 100
A = np.random.rand(m, n)
b = np.random.rand(m)
x0= np.linalg.pinv(A) @ b

optimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\
    lam * np.linalg.norm(x, ord=norm[1])

fig = plt.figure()
axs = []
axs.append(fig.add_subplot(4, 1, 1))
axs.append(fig.add_subplot(4, 3, 10))
axs.append(fig.add_subplot(4, 1, 2))
axs.append(fig.add_subplot(4, 3, 11))
axs.append(fig.add_subplot(4, 1, 3))
axs.append(fig.add_subplot(4, 3, 12))

for i, lam in enumerate([0, 0.1, 0.5]):
    res = minimize(optimize, args=(A, b, lam, [2, 1]), x0=x0)
    axs[i * 2].bar(range(n), res.x)
    axs[i * 2].text(5, 0.05, rf"$\lambda={lam}$")
    axs[i * 2].set_xlim(0, 100)
    axs[i * 2].set_ylim(-0.1, 0.1)
    axs[i * 2 + 1].hist(res.x, 20)
    axs[i * 2 + 1].text(-0.08, 50, rf"$\lambda={lam}$")
    axs[i * 2 + 1].set_xlim(-0.1, 0.1)
    axs[i * 2 + 1].set_ylim(0, 70)
axs[0].set_xticks([])
axs[2].set_xticks([])
axs[3].set_yticks([])
axs[5].set_yticks([])


plt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)
plt.show()
```

The last row of @fig-opt_lambda1 confirms this quite impressively, interesting enough the solution also becomes positive.

### RIDGE 

The Ridge Regression solves @eq-reg-optimizers with $\lambda_1 = 0$ and $\lambda_2 > 0$, i.e. only optimizing with the $l_2$ norm.
The theory tells us that for for increasing $\lambda_1$ we should get more and more zeros in our solution $x$.

```{python}
#| label: fig-opt_lambda2
#| fig-cap: "Ridge regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two."
#| code-fold: true
#| code-summary: "Show the code for the figure"
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize
%config InlineBackend.figure_formats = ["svg"]
np.random.seed(6020)

m = 500
n = 100
A = np.random.rand(m, n)
b = np.random.rand(m)
x0= np.linalg.pinv(A) @ b

optimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\
    lam * np.linalg.norm(x, ord=norm[1])

fig = plt.figure()
axs = []
axs.append(fig.add_subplot(4, 1, 1))
axs.append(fig.add_subplot(4, 3, 10))
axs.append(fig.add_subplot(4, 1, 2))
axs.append(fig.add_subplot(4, 3, 11))
axs.append(fig.add_subplot(4, 1, 3))
axs.append(fig.add_subplot(4, 3, 12))

for i, lam in enumerate([0, 0.1, 0.5]):
    res = minimize(optimize, args=(A, b, lam, [2, 2]), x0=x0)
    axs[i * 2].bar(range(n), res.x)
    axs[i * 2].text(5, 0.05, rf"$\lambda={lam}$")
    axs[i * 2].set_xlim(0, 100)
    axs[i * 2].set_ylim(-0.1, 0.1)
    axs[i * 2 + 1].hist(res.x, 20)
    axs[i * 2 + 1].text(-0.08, 15, rf"$\lambda={lam}$")
    axs[i * 2 + 1].set_xlim(-0.1, 0.1)
    axs[i * 2 + 1].set_ylim(0, 20)
axs[0].set_xticks([])
axs[2].set_xticks([])
axs[3].set_yticks([])
axs[5].set_yticks([])


plt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)
plt.show()
```

