[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Warning\n\n\n\nThe notes presented here are still under construction and can change without warning.\n\n\n\nPreface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.\n\n\nAcknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger and Matthias Panny for testing, checking, suggestions and general proofreading.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basics behind modern day Data Science. We will always try to not only discuss the theory but also use Python to illustrate it and work on it programmatically.\nFor the class we will orient ourself on the first couple of chapters of Brunton and Kutz (2022), where we will highlight sections with lose similarity.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R},\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n.\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\nC = A^{\\mathsf{T}}, \\quad \\Rightarrow \\quad c_{ij} = a_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define two more mappings.\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor a \\({m \\times p}\\) matrix \\(A\\) and a \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition 1.1 (Vector space) For a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following properties need to hold true to form a vector space.\nFor the vector addition we need to have\n\nassociativity \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity \\[u + v = v + u,\\]\nthere needs to exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector such that \\[v + 0 =  v,\\]\nthere needs to exist an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] and this element is usually denoted by \\(-v\\).\n\nFor the scalar multiplication we need to have\n\nassociativity \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\ndistributivity with respect to the vector addition \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v,\\]\nand there needs to exist a multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v.\\]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n Exercise - Vector space in Python\n\n\n\n\n\nCreate some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix as vector space?\n\n\n\n\n\nHow do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Linear map) A linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a field \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w),\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Dot product) The dot product or scalar product of two vectors \\(v\\) and \\(w\\) as is defined by \\[\\langle v, w\\rangle = v \\cdot w = v^{\\mathsf{T}} w = \\sum_i v_i w_i.\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Outer product) We also have the outer product defined as: \\[\nv w^{\\mathsf{T}} = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix multiplication?\n\n\n\n\n\nShow that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\n\nDefinition 1.5 (Norm) A norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac{1}{p}},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAnd for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |a_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{\\sum_i \\sum_j |a_{ij}|^2},\\]\nthe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(\\sum_i \\sum_j |a_{ij}|^p\\right)^{\\frac1p},\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |a_{ij}|.\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "href": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\n\nDefinition 1.6 (Basis) A set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\}, b_i \\in \\mathbb{R}^n\\):\n\nis called linear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}.\\]\n\nThe set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\nFor a shorter notation we often associate the matrix \\[\nB = \\left[b_1 | \\cdots | b_n\\right]\n\\] with the basis.\n\n\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\) and its associated matrix is \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & \\dots & 0\\\\\n                        0 & 1 & \\ddots & \\vdots \\\\  \n                        \\vdots & \\ddots & 1 & 0\\\\\n                        0 & \\dots & 0 & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n},\n\\] and called the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix.\nThe easiest way to create one of standard basis vectors, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\nand the identity matrix by\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n Example - Standard basis\n\n\n\n\n\n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\n# compute the coefficients\na = np.dot(x, e_1) / np.dot(e_1, e_1)\nb = np.dot(x, e_2) / np.dot(e_2, e_2)\nc = np.dot(x, e_3) / np.dot(e_3, e_3)\ny = a * e_1 + b * e_2 + c * e_3\nprint(f\"{y=}\")\nprint(f\"{np.allclose(x, y)=}\")\nprint(f\"{LA.norm(x-y)=}\")\n\nx=array([0.62944377, 0.69277956, 0.18840445])\ny=array([0.62944377, 0.69277956, 0.18840445])\nnp.allclose(x, y)=True\nLA.norm(x-y)=np.float64(0.0)\n\n\nSee numpy.testing for more ways of testing in numpy.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\n\nDefinition 1.7 (Matrix inverse) For matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\n\n\nThe following holds true for the inverse of matrices:\n\nthe inverse of a product is the product of the inverses \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^{\\mathsf{T}} = (A^{mathsf{T}})^{-1} \\equiv A^{-mathsf{T}}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nprint(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n# Note that the equality is hard to achieve for floats\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\nA=array([[0.28441614, 0.85329116, 0.67955636],\n       [0.71767172, 0.96074123, 0.54985784],\n       [0.7853337 , 0.89740592, 0.70247032]])\nX=array([[-2.13044797, -0.12241864,  2.15677768],\n       [ 0.84915673,  3.92029451, -3.89006406],\n       [ 1.29695771, -4.87131756,  3.98191387]])\nA @ X=array([[ 1.00000000e+00, -1.22479736e-16,  1.11245876e-16],\n       [ 5.22279226e-17,  1.00000000e+00, -1.83650149e-16],\n       [-1.39548831e-16,  2.44660910e-17,  1.00000000e+00]])\nnp.allclose(A @ X, np.eye(A.shape[0]))=True\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 8\n      6 print(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n      7 # Note that the equality is hard to achieve for floats\n----&gt; 8 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.7/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 8 / 9 (88.9%)\nMax absolute difference among violations: 4.4408921e-16\nMax relative difference among violations: 4.4408921e-16\n ACTUAL: array([[ 1.000000e+00, -1.224797e-16,  1.112459e-16],\n       [ 5.222792e-17,  1.000000e+00, -1.836501e-16],\n       [-1.395488e-16,  2.446609e-17,  1.000000e+00]])\n DESIRED: array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n\n\n\n\n\n\nDefinition 1.8 (Change of basis) If we associate the matrices \\(B\\) and \\(C\\) with the matrix consisting of the basis vectors of two bases \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) of a vector space we can define the transformation matrix \\(T_{\\mathcal{C}}^{\\mathcal{B}}\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) as \\[\nT_{\\mathcal{C}}^{\\mathcal{B}} = C^{-1}B.\n\\]\nSo if we have a vector \\(b\\) represented in \\(\\mathcal{B}\\) we can compute its representation in \\(\\hat{b}\\) in \\(\\mathcal{C}\\) as \\[\n\\hat{b} = T_{\\mathcal{C}}^{\\mathcal{B}} b = C^{-1}B b.\n\\]\nA special form is if we have the standard basis \\(I\\) and move to a basis \\(C\\) we get \\[\n\\hat{b} = T_{C}^{I} b = C^{-1} b.\n\\]\n\n\n\n\n\n\n\n\n\n\n Example - basis change\n\n\n\n\n\nFor \\[\nB = \\left[\n    \\begin{array}{ccc} 1 & 3 & 2 \\\\\n                       0 & 1 & 1 \\\\  \n                       2 & 0 & 1 \\\\  \\end{array}\n\\right]\\quad \\text{and}\\quad\nC = \\left[\n    \\begin{array}{ccc} 1 & 0 & 1 \\\\\n                       0 & 1 & 1 \\\\  \n                       1 & 1 & 0 \\\\  \\end{array}\n\\right]\n\\] we get \\[\nT_{C}^{B} = \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right],\n\\] and for a \\(v = 2 b_1 - b_2 + 3 b_3\\) we can compute its representation in \\(C\\) as \\[\n\\hat{v} = T_{C}^{B} v\n= \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} 5 \\\\ 2 \\\\ 0 \\end{array}\n\\right],\n\\] and therefore, \\(v = 5c_1 + 2c_2 + 0 c_3\\).\n(Compare Wikipedia)\n\n\n\nThere are special basis vectors, respectively matrices that allow for easy computation of the inverse.\n\n\n\n\n\n\n\nDefinition 1.9 (Orthonormal vector) We call a set of vectors \\(\\mathcal{V}=\\{u_1, u_2, \\ldots, u_m\\}\\) orthonormal if and only if \\[\n\\forall i,j: \\langle u_i, u_j \\rangle = \\delta_{ij}\n\\] where \\(\\delta_{ij}\\) is called the Kronecker delta which is \\(1\\) if and only if \\(i=j\\) and \\(0\\) otherwise. This is true for a inner product, see Definition 1.3.\n\n\n\n\nExtending this to a matrix (and to that end a basis) as follows.\n\n\n\n\n\n\n\nDefinition 1.10 (Orthogonal matrix) We call a matrix \\(Q\\in\\mathbb{R}^{n\\times n}\\), here the real and square is important, orthogonal if its columns and rows are orthonormal vectors. This is the same as \\[\nQ^{\\mathsf{T}} Q = Q Q^{\\mathsf{T}} = I\n\\] and this implies that \\(Q^{-1} = Q^{\\mathsf{T}}\\).\n\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/sets.html",
    "href": "basics/sets.html",
    "title": "2  Data sets",
    "section": "",
    "text": "2.1 Basic properties of a data set\nFirst we are looking at the total net rent, i.e. the row nm.\nFor a vector \\(v \\in \\mathbb{R}^n\\) we have:\nnm_max = np.max(data['nm'])\nprint(f\"{nm_max=}\")\n\nnm_min = np.min(data['nm'])\nprint(f\"{nm_min=}\")\n\nnm_mean = np.mean(data['nm'])\n# round to 2 digits\nnm_mean_r = np.around(nm_mean, 2)\nprint(f\"{nm_mean_r=}\")\n\nnm_median = np.median(data['nm'])\nprint(f\"{nm_median=}\")\n\nnm_quartiles = np.quantile(data['nm'], [1/4, 1/2, 3/4])\nprint(f\"{nm_quartiles=}\")\n\nnm_max=np.float64(1789.55)\nnm_min=np.float64(77.31)\nnm_mean_r=np.float64(570.09)\nnm_median=np.float64(534.3)\nnm_quartiles=array([389.95, 534.3 , 700.48])\nFrom this Python snippet we know that for tenants the rent varied between 77.31 and 1789.55, with an average of 570.09 and a median of 534.3. Of course there are tricky questions that require us to dig a bit deeper into these functions, e.g. how many rooms does the most expensive flat have? The surprising answer is 3 and it was built in 1994, but how do we obtain these results?\nWe can use numpy.argwhere or a function which returns the index directly like numpy.argmax.\nmax_index = np.argmax(data['nm'])\nrooms = int(data['rooms'][max_index])\nyear = int(data['bj'][max_index])\nprint(f\"{rooms=}, {year=}\")\n\nrooms=3, year=1994",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#basic-properties-of-a-data-set",
    "href": "basics/sets.html#basic-properties-of-a-data-set",
    "title": "2  Data sets",
    "section": "",
    "text": "the maximal value, i.e. the maximum \\[\nv^{max} = \\max_i v_i,\n\\]\nthe minimal value, i.e. the minimum \\[\nv^{min} = \\min_i v_i,\n\\]\nthe mean of all values (often called the arithmetic mean) \\[\n\\overline{v} = \\frac1n \\sum_{i=1}^n v_i = \\frac{v_1 + v_2 + \\cdots + v_n}{n},\n\\]\nthe median, i.e. the value where half of all the other values are bigger and the other half is smaller, for a sorted \\(v\\) this is \\[\n\\widetilde{v} = \\begin{cases}\n              v_{(n+1)/2}& n\\quad \\text{odd}\\\\\n              \\frac{v_{n/2} + v_{n/2+1}}{2}& n\\quad \\text{even}\n              \\end{cases},\n\\]\nmore general, we have quantiles. For a sorted \\(v\\) and \\(p\\in(0,1)\\) \\[\n\\overline{v}_p = \\begin{cases}\n               \\frac12\\left(v_{np} + v_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n              v_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n              \\end{cases}.\n\\] Some quantiles have special names, like the median for \\(p=0.5\\), the lower and upper quartile for \\(p=0.25\\) and \\(p=0.75\\) (or first, second (median) and third quartile), respectively.\n\n\n\n\n\n\n2.1.1 Visualization\n\n\n\n\n\n\nTip\n\n\n\nThere are various ways of visualizing data in Python. Two widely used packages are matplotlib and plotly.\n\n\nIt often helps to visualize the values to see differences and get an idea of their use.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nnm_sort = np.sort(data[\"nm\"])\nx = np.linspace(0, 1, len(nm_sort), endpoint=True,)\n\nplt.plot(x, nm_sort, label=\"net rent\")\nplt.axis((0, 1, np.round(nm_min/100)*100, np.round(nm_max/100)*100))\nplt.xlabel('Scaled index')\nplt.ylabel('Net rent - nm')\n\nplt.plot([0, 0.25, 0.25], [nm_quartiles[0], nm_quartiles[0], nm_min], \n         label='1st quartile')\nplt.plot([0, 0.5, 0.5], [nm_quartiles[1], nm_quartiles[1], nm_min],\n         label='2st quartile')\nplt.plot([0, 0.75, 0.75], [nm_quartiles[2], nm_quartiles[2], nm_min],\n         label='3st quartile')\nplt.plot([0, 1], [nm_mean, nm_mean],\n         label='mean')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the different measurements.\n\n\n\n\n\nWhat is shown in Figure 2.1 is often combined into a single boxplot (see Figure 2.2) that provides way more information at once.\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"Standard\"))\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"With points\", boxpoints=\"all\"))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.2: Boxplot done in plotly with whiskers following 3/2 IQR.\n\n\n\n\nThe plot contains the box which is defined by the 1st quantile \\(Q_1\\) and the 3rd quantile \\(Q_3\\), with the median as line in between these two. Furthermore, we can see the whiskers which help us identify so called outliers. By default they are defined as \\(\\pm 1.5(Q_3 - Q_1)\\), where (\\(Q_3 - Q_1\\)) is often called the interquartile range (IQR).\n\n\n\n\n\n\nNote\n\n\n\nFigure 2.2 is an interactive plot in the html version.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#spread",
    "href": "basics/sets.html#spread",
    "title": "2  Data sets",
    "section": "2.2 Spread",
    "text": "2.2 Spread\nThe spread (or dispersion, variability, scatter) are measures used in statistics to classify how data is distributed. Common examples are variance, standard deviation, and the interquartile range that we have already seen above.\n\n\n\n\n\n\n\nDefinition 2.1 (Variance) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the variance is defined as \\[\n\\operatorname{Var}(v) = \\frac1n \\sum_{i=1}^n (v_i - \\mu)^2, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}\n\\] or directly \\[\n\\operatorname{Var}(v) = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j&gt;i} (v_i - v_j)^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Standard deviation) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\frac1n \\sum_{i=1}^n (v_i - \\mu)^2}, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] If we interpret \\(v\\) as a sample this is often also called uncorrected sample standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Interquartile range (IQR)) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the interquartile range is defined as the difference of the first and third quartile, i.e. \\[\nIQR = \\overline{v}_{0.75} - \\overline{v}_{0.25}.\n\\]\n\n\n\n\nWith numpy they are computed as follows\n\nnm_var = np.var(data[\"nm\"])\nprint(f\"{nm_var=}\")\n\nnm_std = np.std(data[\"nm\"])\nprint(f\"{nm_std=}\")\n\nnm_IQR = nm_quartiles[2] - nm_quartiles[0]\nprint(f\"{nm_IQR=}\")\n\nnm_var=np.float64(60208.75551600402)\nnm_std=np.float64(245.37472468859548)\nnm_IQR=np.float64(310.53000000000003)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#histogram",
    "href": "basics/sets.html#histogram",
    "title": "2  Data sets",
    "section": "2.3 Histogram",
    "text": "2.3 Histogram\nWhen exploring data it is also quite useful to draw histograms. For the net rent this makes not much sense but for rooms this is useful.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['rooms'])\nplt.xlabel('rooms')\nplt.ylabel('# of rooms')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of the number of rooms in our dataset.\n\n\n\n\n\nWhat we see in Figure 2.3 is simply the amount of occurrences of \\(1\\) to \\(6\\) in our dataset. Already we can see something rather interesting, there are flats with \\(5.5\\) rooms in our dataset.\nAnother helpful histogram is Figure 2.4 showing the amount of buildings built per year.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['bj'])\nplt.xlabel('year of building')\nplt.ylabel('# of buildings')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Histogram of buildings built per year.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#correlation",
    "href": "basics/sets.html#correlation",
    "title": "2  Data sets",
    "section": "2.4 Correlation",
    "text": "2.4 Correlation\nIn statistics, the terms correlation or dependence describe any statistical relationship between bivariate data (data that is paired) or random variables.\nFor our dataset we can, for example, check:\n\nthe living area in \\(m^2\\) - wfl vs. the net rent - nm\nthe year of construction - bj vs. if central heating - zh0 is available\nthe year of construction - bj vs. the city district - bez\n\n\n\nShow the code for the figure\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=data[\"wfl\"], y=data[\"nm\"], mode=\"markers\"), row=1, col=1)\nfig.update_xaxes(title_text=\"living area in m^2\", row=1, col=1)\nfig.update_yaxes(title_text=\"net rent\", row=1, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"zh0\"], mode=\"markers\"), row=2, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=2, col=1)\nfig.update_yaxes(title_text=\"central heating\", row=2, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"bez\"], mode=\"markers\"), row=3, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=3, col=1)\nfig.update_yaxes(title_text=\"city district\", row=3, col=1)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.5: Scatterplot to investigate correlations in the data set.\n\n\n\n\nIn the first plot of Figure 2.5 we see that the rent tends to go up with the size of the flat but there are for sure some rather cheap options in terms of space.\nThe second plot of Figure 2.5 tells us that central heating became a constant around \\(1966\\). Of course we can also guess that the older buildings with central heating were renovated, but we have no data to support this claim.\nThe third plot of Figure 2.5 does not yield an immediate correlation.\nMore formally, we can describe possible correlations using the covariance. The covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nDefinition 2.4 (Covariance) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the covariance is defined as \\[\n\\operatorname{cov}(v, w) = \\frac1n \\langle v -\\overline{v}, w - \\overline{w}\\rangle.\n\\]\n\n\n\n\nThe covariance is tricky to interpret, e.g. the unities of the two must not make sense. In the example below, we have rent per square meter, which makes some sense.\nFrom the covariance we can compute the correlation.\n\n\n\n\n\n\n\nDefinition 2.5 (Correlation) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the correlation is defined as \\[\n\\rho_{v,w} = \\operatorname{corr}(v, w) = \\frac{\\operatorname{cov}(v, w)}{\\sigma_v \\sigma_w},\n\\] where \\(\\sigma_v\\) and \\(\\sigma_w\\) are the standard deviation of these vectors, see Definition 2.2.\n\n\n\n\nIn numpy the function numpy.cov computes a matrix where the diagonal is the variance of the values and the off-diagonals are the covariances of the \\(i\\) and \\(j\\) samples. Consequently, numpy.corrcoef is a matrix as well.\n\ncov_nm_wfl = np.cov(data[\"nm\"], data[\"wfl\"])\nprint(f\"{cov_nm_wfl[0, 1]=}\")\n\ncorr_nm_wfl = np.corrcoef(data[\"nm\"], data[\"wfl\"])\nprint(f\"{corr_nm_wfl[0, 1]=}\")\n\ncov_nm_wfl[0, 1]=np.float64(4369.1195844122)\ncorr_nm_wfl[0, 1]=np.float64(0.7074626685750687)\n\n\nThe above results, particularly \\(\\rho_{\\text{nm},\\text{wfl}}=0.707\\) suggest that the higher the rent, the more space you get.\n\n\n\n\n\n\nTip\n\n\n\nCorrelation and causation are not the same thing!\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe showed some basic tests for correlation, there are more elaborate methods but they are subject to a later chapter.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/epilogue.html",
    "href": "basics/epilogue.html",
    "title": "3  Epilogue",
    "section": "",
    "text": "This sums up our basic introduction. We introduced the basic mathematical constructs to use in further sections and learned how to work with them in Python.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Epilogue</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html",
    "href": "matrixdc/eigen.html",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1 Examples for the application\nTo get a better idea what the eigendecomposition can do we look into some examples.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#examples-for-the-application",
    "href": "matrixdc/eigen.html#examples-for-the-application",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1.1 Solving system of linear equations\nFor a system of linear equations \\(Ax=b\\) we get \\[\n\\begin{array}{lll}\nA x & = b  & \\iff \\\\\nV \\Lambda V^{-1} x & = b  & \\iff \\\\\n\\Lambda V^{-1} x & = V^{-1} b  & \\iff \\\\\nV^{-1} x & = \\Lambda^{-1}V^{-1} b  & \\iff \\\\\nx & = V\\Lambda^{-1}V^{-1} b  & \\iff\n\\end{array}\n\\] As \\(\\Lambda^{-1} = \\operatorname{diag}\\left(\\lambda_1^{-1}, \\ldots, \\lambda_n^{-1}\\right)\\) this is easy to compute once we have the eigenvalue decomposition.\n\n\n\n\n\n\nNote\n\n\n\nThe computation of the eigendecomposition is not cheap, therefore this is not always worth the effort and there are other ways of solving linear systems.\n\n\n\n\n4.1.2 Linear Ordinary Differential Equations\nIn this example we use the eigendecomposition to efficiently solve a system of differential equations \\[\n\\dot{x} = A x,\\quad x(0) = x_0\n\\] By changing to the basis \\(V\\) and using the notation \\(\\hat{x}=z\\) we have the equivalent formulations \\[\nz = V^{-1}x \\iff x = Vz,\n\\] and if follows \\[\n\\begin{array}{lll}\n\\dot{x} = A x & \\iff V \\dot{z} &= A V z \\\\\n              & \\iff \\dot{z} &= V^{-1} A V z \\\\\n              & \\iff \\dot{z} &= \\Lambda z\n\\end{array}.\n\\] So for an initial value \\(z_0\\) the solution in \\(t\\) is \\[\nz(t) = \\operatorname{diag}\\left(e^{t\\lambda_1}, \\ldots, e^{t\\lambda_n}\\right) z_0.\n\\]\nWe often say that it is now a decoupled differential equation.\n\n\n4.1.3 Higher Order Linear Differential Equations\nIf we have a higher order linear ODE such as \\[\nx^{(n)} + a_{n-1} x^{(n-1)} + \\cdots + a_2 \\ddot{x} + a_1 \\dot{x} + a_0 x = 0.\n\\tag{4.2}\\] we can stack the derivatives into a vector \\[\n\\begin{array}{ccc}\nx_1 & = & x\\\\\nx_2 & = & \\dot{x}\\\\\nx_3 & = & \\ddot{x}\\\\\n\\vdots & = & \\vdots \\\\\nx_{n-1} & = & x^{(n-2)} \\\\\nx_{n} & = & x^{(n-1)} \\\\\n\\end{array}\n\\quad\n\\Leftrightarrow\n\\quad\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} x \\\\ \\dot{x} \\\\ \\ddot{x} \\\\ \\vdots \\\\ x^{(n-2)} \\\\ x^{(n-1)} \\end{array}\n\\right],\n\\] and taking the derivative of this vector yields the following system \\[\n\\underbrace{\n\\frac{d}{d t}\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_{\\dot{x}}\n=\n\\underbrace{\\left[\n    \\begin{array}{cccccc} 0 & 1 & 0 & \\dots & 0 & 0\\\\\n                          0 & 0 & 1 & \\dots & 0 & 0\\\\  \n                          0 & 0 & 0 & \\dots & 0 & 0\\\\\n                          \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n                          0 & 0 & 0 & \\dots & 0 & 1\\\\\n                          -a_0 & -a_1 & -a_2 & \\dots & -a_{n-2} & -a_{n-1}\\\\\n    \\end{array}\n\\right]\n}_{A}\n\\underbrace{\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_x.\n\\]\nWe transformed it into a system of coupled 1st order ODEs \\(\\dot{x}=Ax\\) and we can solve this as seen above. More importantly, the characteristic polynomial of Equation 4.2 is equal to the characteristic polynomial of Definition 4.2 and the eigenvalues are the roots of this polynomial.\n\n\n4.1.4 Generalized eigenvalue problem\nLet us motivate this by the example of modal analysis. If we consider the free vibrations of a undamped system we get the equation \\[\nM\\ddot{u} + K u = 0, \\quad u(0) = u_0,\n\\] with the mass matrix \\(M\\) and the stiffness matrix \\(K\\) and \\(u(t)\\) being the displacement. As we know, the solution of this linear differential equation has the form \\(u(t)=e^{i\\omega t}u_0\\) and thus we get \\[\n(-\\omega^2 M + K) u_0 = 0.\n\\tag{4.3}\\]\n\n\n\n\n\n\n\nDefinition 4.5 (Generalized eigenvalue problem) If \\(A, B \\in \\mathbb{C}^{n\\times n}\\), then the set of all matrices of the form \\(A-\\lambda B\\) with \\(\\lambda\\in\\mathbb{C}\\) is a pencil. The generalized eigenvalues of \\(A-\\lambda B\\) are elements of the set \\(\\lambda(A,B)\\) defined by \\[\n\\lambda(A,B) = \\{z\\in\\mathbb{C}: \\det(A-zB)=0\\}.\n\\] If \\(\\lambda \\in \\lambda(A,B)\\) and \\(0\\neq v\\in\\mathbb{C}^n\\) satisfies \\[\nA v = \\lambda B v,\n\\tag{4.4}\\] then \\(v\\) is an eigenvector of \\(A-\\lambda B\\). The problem of finding a nontrivial solution to Equation 4.4 is called the generalized eigenvalue problem.\n(Compare Golub and Van Loan 2013, chap. 7.7)\n\n\n\n\nIn our example Equation 4.3 the eigenvalues are \\(\\lambda = \\omega^2\\) and correspond to the square of the natural frequencies and the eigenvectors \\(v=u\\) correspond to the modes of vibration.\nIf \\(M\\) is invertible we can write \\[\nM^{-1}(K -\\omega^2 M ) u_0 = (M^{-1}K -\\omega^2 I ) u_0 = 0.\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases inverting the matrix is not recommended, to directly solve the generalized eigenvalue problem, see (Golub and Van Loan 2013, chap. 7.7) for details.\n\n\nLet us walk through this with an example from (Downey and Micheli 2024, sec. 8.3).\n\n\n\n\n\n\n\nExample 4.1 (Two Story Building)  \n\n\n\n\n\n\nFigure 4.1: Two story frame where the floors have different dynamic properties.\n\n\n\nIn Figure 4.1 we can see a two story building consisting of a two column frame with a floor. The first floor columns are fixed at the base and have a height \\(h\\), the second floor is fixed to the first and has the same height. The columns of each frame are modelled as beams with flexural rigidity \\(EI\\) and \\(2 EI\\), respectively Where \\(E\\) is called Young’s modulus and \\(I\\) the second moment of area. The mass is centred at the floor level.\nThis allows us to model such a building as a system with two degrees of freedom \\(x_1\\) and \\(x_2\\) as the displacement indicated in blue. The dashed blue line would be such a displacement for the frame.\nThe resulting equations of motion become \\[\n\\begin{array}{r}\nm_1 \\ddot{x_1} + k_1 x_1 + k_2 (x_2 - x_1) = 0,\\\\\nm_2 \\ddot{x_1} + k_2 (x_2 - x_1) = 0,\n\\end{array}\n\\] resulting in the matrices \\[\nM = \\left[\n\\begin{array}{cc}\nm_1 & 0 \\\\\n0 & m_2\n\\end{array}\n\\right], \\quad\nK = \\left[\\begin{array}{cc}\nk_1 + k_2 & -k_2 \\\\\n-k_2 & k_2\n\\end{array}\n\\right].\n\\]\nFor the derivation of the stiffness coefficients we refer to (Downey and Micheli 2024, 188) and recall the result here as follows \\[\nk_1 = \\frac{48 EI}{h^3}, \\quad k_2 = \\frac{24 EI}{h^3}.\n\\] This results in the stiffness matrix \\[\nK = \\underbrace{\\frac{24 EI}{h^3}}_{=k}\n\\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right].\n\\]\nNow we can manually compute \\[\n\\det(-\\omega^2 M + K) = 0 \\Leftrightarrow 2m^2\\omega^4 - 5 m k \\omega^2 + 2 k^2 = 0,\n\\] and solving this equation for \\(\\omega^2\\) results in \\[\n\\omega_1^2 = \\frac{k}{2m}, \\quad \\omega_2^2 = \\frac{2k}{m}.\n\\]\nTo compute the eigenvectors \\(v_1\\) and \\(v_2\\) we use Equation 4.1, i.e. for \\(v_1\\) this reads as \\[\n-\\frac{k}{2m} \\left[\n\\begin{array}{cc}\n2m & 0 \\\\\n0 & m\n\\end{array}\n\\right]\n+\nk \\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n=\n\\left[\\begin{array}{cc}\n2 k & -k \\\\\n-k & \\frac{k}{2}\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n\\overset{!}{=}\n\\left[\\begin{array}{c}\n0\\\\\n0\n\\end{array}\n\\right],\n\\] and results in the relation \\(2v_{11} = v_{21}\\). We can select a solution as \\[\nv_1 = \\left[\\begin{array}{c}\n\\frac12\\\\\n1\n\\end{array}\n\\right].\n\\] For \\(v_2\\) we proceed similarly and derive a solution as: \\[\nv_2 = \\left[\\begin{array}{c}\n-1\\\\\n1\n\\end{array}\n\\right].\n\\]\nThe eigenvectors illustrate how the displacement functions and are not just some theoretical value. The following figure visualizes the two modes.\n\n\n\n\n\n\n\n\n\n\n\n(a) First mode\n\n\n\n\n\n\n\n\n\n\n\n(b) Second mode\n\n\n\n\n\n\n\nFigure 4.2: Model of a two story building with the shape of the modes according to the modal analysis.\n\n\n\nTwo wrap up the example our overall temporal response consists of the time invariant part defined by our eigenvectors \\(v_1\\), and \\(v_2\\), as well as the time dependent part with our eigenfrequencies \\(\\omega_1\\) and \\(\\omega_2\\) as well as the constants \\(A_1\\), \\(A_2\\), \\(\\phi_1\\), \\(\\phi_2\\) depending on the initial condition (see Downey and Micheli 2024, chap. 5). \\[\n\\left[\\begin{array}{c}\nx_1(t)\\\\\nx_2(t)\n\\end{array}\n\\right]\n=\n\\left[\nv_1, v_2\n\\right]\n\\left[\\begin{array}{c}\nA_1 \\sin(\\omega_1 t + \\phi_1)\\\\\nA_2 \\sin(\\omega_2 t + \\phi_2)\n\\end{array}\n\\right]\n\\]\n(Compare Downey and Micheli 2024, chap. 8.3, pp. 189-191)\n\n\n\n\n\n\n4.1.5 Low-rank approximation of a square matrix\nWe can use the eigenvalue decomposition to approximate a square matrix.\nLet us sort the eigenvalues in \\(\\Lambda\\) and let us call \\(U^{\\mathsf{T}}=V^{-1}\\) then we can write \\[\nA = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^n\\lambda_i v_i u_i^{\\mathsf{T}}\n\\] where \\(v_i\\) and \\(u_i\\) correspond to the rows of the matrices. Now we can define the rank \\(r\\) approximation of \\(A\\) as \\[\nA\\approx A_r = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^r\\lambda_i v_i u_i^{\\mathsf{T}}.\n\\]\nTo make this a bit easier to understand the following illustration is helpful:\n\n\n\nLow Rank Approximation\n\n\nThis approximation can be used to reduce the storage demand of an image.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_cut = im_gray[1500:3001,1500:3001] / 255\n\nlam_, V_ = LA.eig(im_cut)\norder = np.argsort(np.abs(lam_))\nlam = lam_[order[::-1]]\nV = V_[:, order[::-1]]\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\nVinv = LA.inv(V)\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3,1,1)\nax_eig.plot((np.abs(lam)))\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\lambda_i|$\")\n#ax_eig.set_xlabel(\"$i$\")\nax_eig.set_title(\"absolute value of the eigenvalues\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(lam) * p))\n    A_r = np.real(V[:, 0:r] @ np.diag(lam[0:r], 0) @ Vinv[0:r, :])\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_cut, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.3: Image of MCI I and the reconstruction with approximated matrix.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#summary",
    "href": "matrixdc/eigen.html#summary",
    "title": "4  Eigendecomposition",
    "section": "4.2 Summary",
    "text": "4.2 Summary\nThe eigenvalue decomposition is an important tool but it has its limitations:\n\nthe matrices involved need to be square\neigenvalues might be complex, even if the problem at hand is real\nwe only get a diagonal matrix \\(\\Lambda\\) if all eigenvectors are linear independent\nthe computation of \\(V^{-1}\\) is non-trivial unless \\(A\\) is symmetric and \\(V\\) becomes unitary (\\(V^{-1} = V^{\\mathsf{T}}\\)).\n\nTherefore, we will look into a generalized decomposition called the singular value decomposition in the next section.\n\n\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A Practical Introduction for Mechanical, Civil, and Aerospace Engineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html",
    "href": "matrixdc/svd.html",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "5.1 Low rank approximation\nAgain, we can cut of the reconstruction at a certain point and create an approximation. More formally this is defined in the next definition.\nWe can use this for image compression.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_scale = im_gray[1500:3001, 1500:3001] / 255\n\nU, s, Vh = LA.svd(im_scale, full_matrices=False)\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3, 1, 1)\nax_eig.plot(s)\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\sigma_i|$\")\nax_eig.set_title(\"singular value\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(s) * p))\n    A_r = U[:, :r] @ np.diag(s[:r]) @ Vh[:r, :]\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_scale, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.1: Image of MCI I and the reconstruction with reduced rank matrices.\nAs the matrices \\(U\\) and \\(V\\) are orthogonal, they also define a basis of the corresponding (sub) vector spaces. As mentioned before, the SVD automatically selects these and they are optimal.\nConsequently, the matrices \\(U\\) and \\(V\\) can be understood as reflecting patterns in the image. We can think of the columns of \\(U\\) and \\(V\\) as the vertical respectively horizontal patterns of \\(A\\).\nWe can illustrate this by looking at the modes of our decomposition \\[\nM_k = U(:, k) V^{\\mathsf{T}}(k, :).\n\\]\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nrec = [0, 1, 2, 3, 4, 5]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(3, 3, 1))\naxs.append(fig.add_subplot(3, 3, 2))\naxs.append(fig.add_subplot(3, 3, 3))\naxs.append(fig.add_subplot(3, 3, 4))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 6))\n\nfor i, r in enumerate(rec):\n    M_r = np.outer(U[:, r], Vh[r, :])\n    axs[i].imshow(M_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r + 1\n    axs[i].set_title(f\"${r=}$\")\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Modes of the SVD decomposition of the MCI I image.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#low-rank-approximation",
    "href": "matrixdc/svd.html#low-rank-approximation",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "Definition 5.2 (Low-Rank Approximation) If \\(A \\in \\mathbb{R}^{m\\times n}\\) and has the SVD \\(A = U\\Sigma V^{\\mathsf{T}}\\) than \\[\nA_k = U(:, 1:k)\\, \\Sigma(1:k, 1:k)\\, V^{\\mathsf{T}}(1:k, :)\n\\] is the optimal low-rank approximation of \\(A\\) with rank \\(k\\). This is often called the truncated SVD.\n(See Golub and Van Loan 2013, Corollary 2.4.7 p. 79)\n\n\n\n\n\n\n\nTruncated SVD\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we compare this to Figure 4.3 we can see that we get a much better result for smaller \\(r\\). Let us have a look why.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe big advantage here is, that the selection is optimal. A disadvantage is that the need to store the basis separately and this increases the necessary storage. We will see in later sections about wavelets and Fourier decomposition how a common basis can be used to reduce the storage by still keeping good reconstructive properties.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#principal-component-analysis",
    "href": "matrixdc/svd.html#principal-component-analysis",
    "title": "5  Singular Value Decomposition",
    "section": "5.2 Principal Component Analysis",
    "text": "5.2 Principal Component Analysis\nOn of the most important applications of SVD is in the stable computation of the so called principal component analysis (PCA). It is a common technique in data exploration, analysis, visualization, and preprocessing.\nThe main idea of PCM is to transform the data in such a way that the main directions (principal components) capture the largest variation. In short we perform a change of the basis, see Definition 1.8.\nLet us investigate this in terms of a (artificial) data set.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: Noisy Gaussian Data, pp. 25-27).\n\n\nWe generate a noisy cloud (see Figure 5.3) that consists of \\(10000\\) points in 2D, generated from a normal distribution with zero mean and unit variance. The data is than:\n\nscaled by \\(2\\) in the first direction and by \\(\\frac12\\) in second,\nrotated by \\(\\frac\\pi3\\)\ntranslation in the direction \\(\\left[2\\ 1\\right]^{\\mathsf{T}}\\).\n\nThe resulting matrix \\(X\\) is a long and skinny matrix with each measurement (or experiment) stacked next to each other. This means, each column represents a new set, e.g. a time step, and each row corresponds to the same sensor.\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is a slight adaptation (for nicer presentation in these notes) of the (Brunton and Kutz 2022, Code 1.4) also see notebook on github.\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nxC = np.array([2, 1])      # Center of data (mean)\nsig = np.array([2, 0.5])   # Principal axes\n\ntheta = np.pi / 3            # Rotate cloud by pi/3\n\nR = np.array([[np.cos(theta), -np.sin(theta)],     # Rotation matrix\n              [np.sin(theta), np.cos(theta)]])\n\nnPoints = 10000            # Create 10,000 points\nX = R @ np.diag(sig) @ np.random.randn(2, nPoints) + np.diag(xC) @ np.ones((2, nPoints))\n\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(X[0, :], X[1, :], '.', color='k')\nax1.grid()\nax1.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\n## f_ch01_ex03_1b\n\nXavg = np.mean(X, axis=1)                  # Compute mean\nB = X - np.tile(Xavg, (nPoints, 1)).T       # Mean-subtracted data\n\n# Find principal components (SVD)\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nS = S / np.sqrt(nPoints - 1)\n\nax2 = fig.add_subplot(122)\nax2.plot(X[0, :], X[1, :], '.', color='k')   # Plot data to overlay PCA\nax2.grid()\nax2.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\ntheta = 2 * np.pi * np.arange(0, 1, 0.01)\n\n# 1-std confidence interval\nXstd = U @ np.diag(S) @ np.array([np.cos(theta), np.sin(theta)])\n\nax2.plot(Xavg[0] + Xstd[0, :], Xavg[1] + Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 2 * Xstd[0, :], Xavg[1] + 2 * Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 3 * Xstd[0, :], Xavg[1] + 3 * Xstd[1, :], '-', color='r', linewidth=3)\n\n# Plot principal components U[:,0]S[0] and U[:,1]S[1]\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,0] * S[0]]),\n         np.array([Xavg[1], Xavg[1] + U[1,0] * S[0]]), '-', color='cyan', linewidth=5)\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,1] * S[1]]),\n         np.array([Xavg[1], Xavg[1] + U[1,1] * S[1]]), '-', color='cyan', linewidth=5)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.3: Principal components of the mean-subtracted Gaussian data on the left as, as well as the first three standard deviation ellisoids and the two scaled left singular vectors.\n\n\n\n\n\n\n5.2.1 Computation\nFor the computation we follow the outline given in (Brunton and Kutz 2022, chap. 1.5). First we need to center our matrix \\(X\\) according to the mean per feature, in our case per row. \\[\n\\overline{x}_j = \\frac1n \\sum_{i=1}^n X_{ij}\n\\] and our mean matrix is the outer product with the one vector \\[\n\\overline{X} = \\left[\\begin{array}{c}1\\\\\\vdots\\\\1\\end{array}\\right] \\overline{x}\n\\] which can be used to compute the centred matrix \\(B = X - \\overline{X}\\).\nThe PCA is the eigendecomposition of the covariance matrix \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}} B\n\\tag{5.2}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe normalization factor of \\(n-1\\) in Equation 5.2 an not \\(n\\) is called Bassel’s correction and compensates for the bias in the estimation of the population variance.\n\n\nAs \\(C\\) is symmetric and positive semi-definite, therefore it has non-negative real eigenvalues and the matrix \\(V\\) of the eigendecomposition satisfies \\(V^{-1} = V^{\\mathsf{T}}\\) (i.e. it is orthogonal Definition 1.10). The principal components are the eigenvectors and the eigenvalue are the variance along these components.\nIf we instead compute the SVD of \\(B = U\\Sigma V^{\\mathsf{T}}\\) we get \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}}B = \\frac{1}{n-1} V \\Sigma V^{\\mathsf{T}} = \\frac{1}{n-1} V (\\Lambda^{\\mathsf{T}}\\Lambda) V^{\\mathsf{T}}\n\\] leading to a way of computing the principal components in a robust way as \\[\n\\lambda_k = \\frac{\\sigma_k^2}{n-1}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIf the sensor ranges of our matrix are very different in magnitude the correlation matrix is scaled by the row wise standard deviation of \\(B\\) similar as for the mean.\n\n\nIn our example we get our scaled \\(\\sigma_1=1.988\\approx 2\\) and \\(\\sigma_2=0.496\\approx \\frac12\\). These results recover our given parameters very well. Additionally we can see that our rotation matrix is closely matched by \\(U\\) (up to signs) from our SVD: \\[\nR_{\\frac\\pi3} = \\left[\n\\begin{array}{cc} 0.5&0.866\\\\-0.866&0.5\\end{array}\n\\right], \\quad U = \\left[\n\\begin{array}{cc}-0.501&-0.865\\\\-0.865&0.501\\end{array} \\right]\n\\]\n\n\n5.2.2 Example Eigenfaces\nWe combine SVD/PCA in a illustrative example called eigenfaces as introduced in (Brunton and Kutz 2022, Sec 1.6, pp. 28-34).\nThe idea is to apply the PCA techniques to a large set of faces to extract the dominate correlations between the images and create a face basis that can be used to represent an image in these coordinates. For example you can reconstruct a face in this space by projecting onto the eigen vectors or it can be used for face recognition as similar faces usually cluster under this projection.\nThe images are taken from the Yale Face Dataset B, in our case we use a GitHub that provides Julia Pluto notebooks for Chapter 1 to 4 of Brunton and Kutz (2022).\nOur training set, so to speak, consists of the first 36 people in the dataset. We compute the average face and subtract it from our dataset to get our matrix \\(B\\). From here a SVD provides us with our basis \\(U\\). To test our basis we use individual 37 and a portion of the image of the MCI Headquarter (to see how well it performs on objects). For this we use the projection \\[\n\\tilde{x} = U_r U_r^{\\mathsf{T}} x.\n\\] If we split this up, we first project onto our found patterns (encode) and than reconstruct from them (decode).\n\n\n\n\n\n\nNote\n\n\n\nWe can understand this as encoding and decoding our test image, which is the general setup of an autoencoder (a topic for another lecture).\nThe correlation coefficients \\(x_r = U_r^{\\mathsf{T}} x\\) might reveal patterns for different \\(x\\). In the case of faces, we can use this for face recognition, i.e. if the coefficients of \\(x_r\\) are in the same cluster as other images, they are probably from the same person.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is an adaptation of the (Brunton and Kutz 2022, Code 1.7 and 1.9).\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\ntrainingFaces = faces[:, : np.sum(nfaces[:36])]\navgFace = np.mean(trainingFaces, axis=1)\n\nB = trainingFaces - np.tile(avgFace, (trainingFaces.shape[1], 1)).T\nU, _, _ = LA.svd(B, 'econ')\n\ntestFace = faces[:, np.sum(nfaces[:36])]\ntestFaceMS = testFace - avgFace\nrec = [25, 100, 400]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(2, 4, 1))\naxs.append(fig.add_subplot(2, 4, 2))\naxs.append(fig.add_subplot(2, 4, 3))\naxs.append(fig.add_subplot(2, 4, 4))\naxs.append(fig.add_subplot(2, 4, 5))\naxs.append(fig.add_subplot(2, 4, 6))\naxs.append(fig.add_subplot(2, 4, 7))\naxs.append(fig.add_subplot(2, 4, 8))\n\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    axs[i].set_title(f\"${r=}$\")\n\naxs[3].imshow(np.reshape(testFace, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\naxs[3].set_axis_off()\naxs[3].set_title(f\"Original image\")\n\nshift = 1500\ntestFaceMS = np.reshape(im_gray[shift:shift+n, shift:shift+m].T, n*m) - avgFace\nrec = [100, 400, 1600]\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[4 + i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[4 + i].set_axis_off()\n    axs[4 + i].set_title(f\"${r=}$\")\naxs[7].imshow(im_gray[shift:shift+n, shift:shift+m], cmap=plt.get_cmap(\"gray\"))\naxs[7].set_axis_off()\naxs[7].set_title(f\"Original image\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.4: Approximate reconstruction of a test face and an object using the eigenfaces basis for different order r.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to resource limitations the above computation can not be done for each build. We try to make sure that the code matches the image but if something is different if you try it yourself we apologise for that.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#further-applications-of-the-svd",
    "href": "matrixdc/svd.html#further-applications-of-the-svd",
    "title": "5  Singular Value Decomposition",
    "section": "5.3 Further applications of the SVD",
    "text": "5.3 Further applications of the SVD\nThere are many more applications of the SVD but we want to highlight some regarding systems of linear equations, \\[\nA x = b\n\\tag{5.3}\\] where the matrix \\(A\\), as well as the vector \\(b\\) is known an \\(x\\) is unknown.\nDepending on the structure of \\(A\\) and the specific \\(b\\) we have no, one, or infinitely many solutions. For now the interesting case is where \\(A\\) is rectangular and therefore we have either an\n\nunder-determined system \\(m\\ll n\\), so more unknowns than equations,\nover-determined system \\(m\\gg n\\), so more equations than unknowns.\n\nFor the second case (more equations than unknowns) we often switch to solving the optimization problem that minimizes \\[\n\\|Ax-b\\|_2^2.\n\\tag{5.4}\\] This is called the least square solution. The least square solution will also minimize \\(\\|Ax-b\\|_2\\). For an under-determined system we might seek the solution which minimizes \\(\\|x\\|_2\\) called the minimum norm solution.\nIf we us the SVD decomposition for \\(A = U \\Sigma V^{\\mathsf{T}}\\) we can define the following\n\n\n\n\n\n\n\nDefinition 5.3 (Pseudo-inverse) We define the matrix \\(A^\\dagger \\in \\mathbb{R}^{m\\times n}\\) by \\(A^\\dagger = V\\Sigma^\\dagger U^{\\mathsf{T}}\\) where \\[\n\\Sigma^\\dagger = \\operatorname{diag}\\left(\\frac{1}{\\sigma_1}. \\frac{1}{\\sigma_2}, \\ldots, \\frac{1}{\\sigma_r}, 0, \\ldots, 0\\right) \\in \\mathbb{R}^{m\\times n}, \\quad r=\\operatorname{rank}(A).\n\\]\nThe matrix \\(A^\\dagger\\) is often called the Moore-Penrose left pseudo-inverse as it fulfils the Moore-Penrose conditions conditions. It is also the matrix to provides the minimal Frobenius norm solution to \\[\n\\min_{X \\in \\mathbb{R}^{m\\times n}}\\| A X - I_n\\|_F.\n\\]\n(Compare Golub and Van Loan 2013, 290)\n\n\n\n\nIf we only use the truncated version, i.e. where we only use non-zero singular values, we can use it to find good solutions to Equation 5.4.\n\nIn numpy it can be computed by numpy.linalg.pinv.\n\n\n\n\n\n\n\n\nDefinition 5.4 (Condition number) The condition number of a matrix provides a measure how sensitive the solution of Equation 5.3 is to perturbations in \\(A\\) and \\(b\\). For a square matrix \\(A\\) the condition number is defined as \\[\n\\kappa(A) = \\|A\\| \\left\\|A^{-1}\\right\\|,\n\\] for an appropriate underlying norm. For the 2-norm \\(\\kappa_2\\) is \\[\n\\kappa_2(A) = \\|A\\|_2 \\left\\|A^{-1}\\right\\|_2 = \\frac{\\sigma_{max}}{\\sigma_{min}}.\n\\]\nTo get a better idea on what this means think of it in this way. For the perturbed linear system \\[\nA(x + \\epsilon_x) = b + \\epsilon_b,\n\\] we can outline the worst case, where \\(\\epsilon_x\\) aligns with the singular vector of the largest singular vector and \\(x\\) with the smallest singular value, i.e. \\[\nA(x + \\epsilon_x) = \\sigma_{min}x + \\sigma_{max}\\epsilon_x.\n\\] Consequently, the output signal-to-noise \\(\\|b\\|/\\|\\epsilon_b\\) is equivalent with the input signal-to-noise \\(\\|x\\|/\\|\\epsilon_x\\) and the factor between those two is \\(\\kappa_2(A)\\).\nIn this sense \\(\\kappa_2\\) can be extended for more general matrices.\n(Compare Golub and Van Loan 2013, 87; and Brunton and Kutz 2022, 18–19)\n\n\n\n\n\n5.3.1 Linear regression with SVD\nBefore we go into more details about regression in the next section we give a brief outlook in terms of how to solve such a problem with SVD.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: One-Dimensional Linear Regression, Example: Cement Heat Generation Data, pp. 19-22).\n\n\n\n5.3.1.1 Linear Regression (see Brunton and Kutz 2022, 19–21)\nFirst we just take a linear correlation that we augment with some Gaußian Noise. So our matrix \\(A\\) is simple a vector with our \\(x\\)-coordinates and \\(b\\) is the augmented image under our linear correlation. \\[\n\\left[\n    \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{array}\n\\right]x =\n\\left[\n    \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{array}\n\\right]\n\\quad\n\\Leftrightarrow\n\\quad\nU\\Sigma V^{\\mathsf{T}} x = b\n\\quad\n\\Leftrightarrow\n\\quad\nx = A^\\dagger b\n\\]\nFor this example \\(\\Sigma = \\|a\\|_2\\), \\(V=1\\), and \\(U=\\tfrac{a}{\\|a\\|_2^2}\\). This is basically just the projection of \\(b\\) along our basis \\(a\\) and this is \\[\nx = \\frac{a^{\\mathsf{T}} b}{a^{\\mathsf{T}} a}.\n\\]\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nk = 3\nA = np.arange(-2, 2, 0.25).reshape(-1, 1)\nb = k*A + np.random.randn(*A.shape) * 0.5\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(A, k*A, color=\"k\", label=\"Target\")\nplt.plot(A, b, 'x', color=\"r\", label=\"Noisy data\")\nplt.plot(A, A*x, '--', color=\"b\", label=\"Regression line\")\nplt.legend()\nplt.xlabel(\"$a$\")\nplt.ylabel(\"$b$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.5: Linear regression with SVD.\n\n\n\n\n\nOur reconstructed unknown \\(x=\\) 2.855 and is a reasonable good match for \\(k=\\) 3.0\n\n\n5.3.1.2 Multi-Linear Regression (see Brunton and Kutz 2022, 21–23)\nThe second example is based on the Portland Cement Data build in with MATLAB. In Python we again use the dataset provided on GitHub. The data set contains the heat generation during the hardening of 12 cement mixtures comprised of 4 basic ingredients, i.e. \\(A\\in \\mathbb{R}^{13\\times 4}\\). The aim is to determine the weights \\(x\\) that relate the proportion of the ingredients to the heat generation in the mixture.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\nimport requests\nimport io\n%config InlineBackend.figure_formats = ['svg']\n\n# Transform the content of the file into a numpy.ndarray\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_ingredients.csv\")\n# Transform the content of the file into a numpy.ndarray\nA = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_heat.csv\")\nb = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(b, color=\"k\", label=\"Target - Heat data\")\nplt.plot(A@x, '--', color=\"b\", label=\"Regression\")\nplt.legend()\nplt.xlabel(\"mixture\")\nplt.ylabel(\"Heat[cal/g]\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.6: Estimate for hardening in cement mixtures.\n\n\n\n\n\nThis concludes our investigation of matrix decompositions, we will investigate further decompositions of signals later, but for now we dive deeper into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "regression/linear.html",
    "href": "regression/linear.html",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1 Ordinary Least Square\nIt is worth looking into the least square solution \\[\nE_2 (f) = \\sqrt{\\frac1n \\sum_{k=1}^n|\\underbrace{f(X_{k-}, c) - y_k}_{\\mathbf{e}_k} |^2},\n\\] more closely. We can interpret it as the optimization problem \\[\nc = \\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2\n\\] and with some linear algebra we get \\[\n\\begin{array}{ccl}\nc &= &\\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} \\langle y -Xv, y -Xv\\rangle,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} (y -Xv)^{\\mathsf{T}} (y -Xv),\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} y^{\\mathsf{T}}y - y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v.\\\\\n\\end{array}\n\\] In order to find a solution we compute the derivative with respect to \\(v\\) set it to \\(0\\) and simplify, i.e. \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\,v} y^{\\mathsf{T}}y- y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v = - 2X^{\\mathsf{T}}y + 2 X^{\\mathsf{T}}Xv\n\\tag{6.5}\\] and \\[\nv = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y \\equiv X^\\dagger y.\n\\] We recall, that \\(X^\\dagger\\) is called the Moore-Penrose pseudo-inverse, see Definition 5.3.\nSee Figure 6.1 for the result when using the pseudo-inverse.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#sec-regr-linear-ols",
    "href": "regression/linear.html#sec-regr-linear-ols",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1.1 Alternative computation\nThe pseudo-inverse provides us with the optimal solution but for large systems the computation can be inefficient, or more precisely, there are more efficient ways to get the same results.\nFollowing (Brunton and Kutz 2022, 137–38) we can find an alternative for the above example in Figure 6.1.\nWe want to fit the data points \\((x_i, y_i)\\) with the function \\(f(x) = c_2 x + c_1\\) resulting in the error \\[\nE_2(f) = \\sqrt{\\frac1n \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2}.\n\\] A solution that minimizes the above equation also minimizes \\[\nE_2 = \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2\n\\] and we find the solution by partial differentiation \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_1} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) = 0,\n\\] \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_2} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) x_k = 0,\n\\] and this results in the system \\[\n\\left[\n\\begin{array}{cc}\nn & \\sum_k x_k \\\\\n\\sum_k x_k & \\sum_k x_k^2\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\nc_1 \\\\ c_2\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\n\\sum_k y_k\\\\\\sum_k x_k y_k\n\\end{array}\n\\right].\n\\tag{6.6}\\] This ansatz can be extended to polynomials of degree \\(k\\), where the result is always a \\((k+1) \\times (k+1)\\) matrix.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#polynomial-regression",
    "href": "regression/linear.html#polynomial-regression",
    "title": "6  Linear Regression",
    "section": "6.2 Polynomial Regression",
    "text": "6.2 Polynomial Regression\nPolynomial regression, despite its name, is linear regression with a special function \\(f\\) where the relation is polynomial in \\(x= \\left[a_1, \\dots, x_m\\right]^{\\mathsf{T}}\\) \\[\ny_k = x_k^0 + x_k^1 c_1 + x_k^2 c_2 + \\cdots + x_k^m c_m, \\quad 1\\leq k \\leq n,\n\\] With the matrix form \\[\nX = \\left[\n\\begin{array}{cccc}\n1 & x_1    &x_1^2  & \\cdots &x_1^m  \\\\\n1 & x_2    &x_2^2  & \\cdots &x_2^m  \\\\\n1 & \\vdots &\\vdots & \\ddots &\\vdots \\\\\n1 & x_n    &x_n^2  & \\cdots &x_n^m\n\\end{array}\n\\right]\n\\tag{6.7}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe matrix Equation 6.7 is called the Vandermonde matrix.\n\n\nThis can be solved in the same ways as the before with \\(X^\\dagger\\) or the direct system, but it should not as Equation 6.7 is badly conditioned. There are other methods like divided differences, Lagrange interpolation for this task.\n\n\n\n\n\n\n\nExample 6.1 (Parameter estimation of a falling object) Just because we deal with linear regression this does not means that the model needs to be linear too. As long as we are linear in the parameters \\(c\\) we can apply our findings, even for non linear independent variables.\nTo illustrate this, let us consider an object falling without aerodynamic drag, described by the differential equation \\[\nm \\ddot{y}(t) = -m g,\n\\] for the gravitational constant \\(g\\). Integration with respect to \\(t\\) results in \\[\ny(t) = y(0) + v(0) t - \\frac{g}{2} t^2.\n\\] So we get \\[\nX{k-} =\n\\left[\n    \\begin{array}{ccc}\n    1 & t_k & -\\frac{1}{2} t_k^2 \\\\\n    \\end{array}\n\\right],\n\\quad \\text{and} \\quad\ny = \\left[\n\\begin{array}{c}\ny^{(0)}\\\\ v^{(0)} \\\\ g\n\\end{array}\n\\right]\n\\] or in the long form, for \\(t_{k+1}-t_k = 0.1\\) \\[\nX = \\left[\n\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0.1 & -0.005 \\\\\n1 & 0.2 & -0.020 \\\\\n1 & 0.3 & -0.045 \\\\\n1 & 0.4 & -0.080 \\\\\n\\vdots & \\vdots & \\vdots\n\\end{array}\n\\right]\n\\] and we can, for example, estimate our unknowns \\(y^{(0)}\\), \\(v^{(0)}\\), and \\(g\\) by \\[\nc = X^\\dagger y.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 6.2 (Polynomial regression) In the following example we generate an artificial sample of \\(n=100\\) points resulting in the samples \\[\ny_k = \\frac12 x_k^2 + x_k + 2 + \\epsilon_k\n\\] where \\(\\epsilon_k\\) is a random number that simulates the error. We perform the interpolation with \\(X^\\dagger\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore', np.exceptions.RankWarning)\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 100\nx = 6 * np.random.rand(m) - 3\ny = 1/2 * x ** 2 + x + 2 + np.random.randn(m)\n\nX1 = np.vander(x, 2)\nX2 = np.vander(x, 3)\nX3 = np.vander(x, 16)\n\np1 = np.linalg.pinv(X1) @ y\np2 = np.linalg.pinv(X2) @ y\np3 = np.linalg.pinv(X3) @ y\np4 = np.polyfit(x, y, 300)\n\nxf = np.arange(-3, 3, 0.1)\ny1 = np.polyval(p1, xf)\ny2 = np.polyval(p2, xf)\ny3 = np.polyval(p3, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\", linewidth=3)\nplt.plot(xf, y1, label=r\"$m=1$\")\nplt.plot(xf, y2, label=r\"$m=2$\")\nplt.plot(xf, y3, label=r\"$m=16$\")\nplt.plot(xf, y4, label=r\"$m=300$\")\n\nplt.ylim(0, 10)\nplt.xlim(-3, 3)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.legend(loc=\"upper left\")\n#plt.gca().set_aspect(0.25)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6.2: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe condition of the Vandermonde matrix increases rapidly:\n\n\n\ndegree\n\\(m=2\\)\n\\(m=3\\)\n\\(m=5\\)\n\\(m=10\\)\n\\(m=15\\)\n\\(m=20\\)\n\n\n\n\n\\(\\kappa_2\\)\n6.07e0\n1.63e1\n1.89e2\n154e5\n1.24e8\n1.41e11\n\n\n\nThe result of the \\(m=300\\) is unstable and we can not compute it via \\(X^\\dagger\\).\n\n\nAs can be seen in Figure 6.2 we do not necessarily get a good result if we use a higher degree polynomial. This is especially true if we extrapolate and not interpolate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#data-linearization",
    "href": "regression/linear.html#data-linearization",
    "title": "6  Linear Regression",
    "section": "6.3 Data Linearization",
    "text": "6.3 Data Linearization\nQuite often it is possible to linearize our model at hand. For example if we want to fit for \\[\nf(x, c) = c_2 \\exp(c_1 x),\n\\tag{6.8}\\]\nand use the same derivation as for Equation 6.6 we end up with the corresponding system as \\[\nc_2 \\sum_k x_k \\exp(2 c_1 x_k) - \\sum_k x_k y_k \\exp(c_1 x_k) =0,\n\\] \\[\nc_2 \\sum_k \\exp(2 c_1 x_k) - \\sum_k y_k \\exp(c_1 x_k) =0.\\\\\n\\]\nThis non-linear system can not be solved in a straight forward fashion but we can avoid it by linearization with the simple transformation \\[\n\\begin{array}{ccl}\n\\hat{y} &=& \\ln(y), \\\\\n\\hat{x} &=& x, \\\\\nc_3 &=& \\ln c_2,\n\\end{array}\n\\] and taking the natural logarithm of both sides of Equation 6.8 and simplifying \\[\n\\ln y = \\ln(c_2 \\exp(c_1 x)) = \\ln(c_2) + c_1 x.\n\\] Now all that is left to apply \\(\\ln\\) to the data \\(y\\) and solve the linear problem. In order to apply it to the original function the parameters transform needs to be reversed.\n\n\n\n\n\n\n\nExample 6.3 (Example - population of the world) We take a look at the population growth were the data is kindly provided by Ritchie et al. (2023). Have a look at there excellent work on ourworldindata.org.\n\nShow the code for the figure\nfrom owid.catalog import charts\nimport numpy as np\nimport scipy.optimize\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndf = charts.get_data(\"https://ourworldindata.org/grapher/population?country=~OWID_WRL\")\ndata = df[df[\"entities\"] == \"World\"]\nx = data[\"years\"].to_numpy()\ny = data[\"population\"].to_numpy()\nylog = np.log(y)\n\ndef fit3(x0, t):\n    x, y = t\n    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))\n\nstart = [-np.inf, 0, 1700, 1900, 1980]\n\nyest = []\n\nfor s in start:\n    filter = x &gt;= s\n    t = (x[filter], ylog[filter])\n    x0 = np.array([1, 1])\n    b = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)\n    yest.append(np.exp(b[1]) * np.exp(b[0] * x))\n\nfig = go.Figure()\nfig2 = go.Figure()\nfig.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"], y=data[\"population\"], name=\"data\"))\nfig2.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"], y=data[\"population\"], name=\"data\"))\n\nfor i, ye in enumerate(yest):\n    fig.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n    fig2.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n\n\nfig.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig.update_yaxes(title_text=\"population\")\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig.show()\n\nfig2.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig2.update_yaxes(title_text=\"population\", type=\"log\", range=[8.5,10])\nfig2.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig2.show()\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) World population with regression lines normal scale.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) World population with regression lines logarithmic scale.\n\n\n\n\n\n\n\nFigure 6.3: Line fit with different norms. Top without outliers, bottom with one outlier.\n\n\n\n\n\n\n\nNext, we are going to look into actual non-linear regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber, Esteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population Growth.” Our World in Data.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html",
    "href": "regression/nonlinear.html",
    "title": "7  Non-linear Regression",
    "section": "",
    "text": "7.1 Gradient Descent\nFor a higher dimensional system or function \\(f\\) the gradient must be zero\n\\[\n\\nabla f(x) = 0\n\\] to know that we are in an extrema. Since we can have saddle points this is not the sole criteria but a necessary one. Gradient descent, as the name suggest uses the gradient as direction in an iterative algorithm to find a minimum.\nThe idea is basically, if you are lost on a mountain in the fog and you can not see the path, the fastest and a reliable way that only uses local information is to follow the steepest slope down.\nWe express this algorithm in terms of the iterations \\(x^k\\) for guesses of the minimum with the updates \\[\nx^{k+1} = x^k - \\delta\\, \\nabla f(x^k)\n\\] where the parameter \\(\\delta\\) defines how far along the gradient descent curve we move. This formula is an update for a Newton method where we use the derivative as the update function. This leaves us with the problem to find an algorithm to determine \\(\\delta\\).\nAgain, we can view this as an optimization problem for a new function \\[\nF(\\delta) = f(x^{k+1}(\\delta))\n\\] and \\[\n\\partial_\\delta F = -\\nabla f(x^{k+1})\\nabla f(x^k) = 0.\n\\tag{7.1}\\]\nNow the interpretation of Equation 7.1 is that we want that the gradient of the current step is orthogonal to the gradient of the next step.\nIn order to make it clearer we follow the example given in (Brunton and Kutz 2022, sec. 4.2,pp. 141-144).\nIn order to get a better idea on how this is working for curve fitting we apply the gradient decent method to our curve fitting from Section 6.1.\nIn Equation 6.5 we computed the gradient and instead of computing \\(X^\\dagger\\) with high cost we get the low cost iterative solver:\n\\[\nc^{(k+1)} = c^{(k)} - \\delta (2 X^{\\mathsf{T}} X c^{(k)} - 2 X^{\\mathsf{T}} y)\n\\]\nAs \\(\\delta\\) is tricky to compute we go ahead and introduce we do not update it but prescribe it. This will not grant us the optimal convergence (if there is convergence) but if we choose it right we still get convergence.\nSo lets try it with our example from Figure 6.1.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef gd(c, delta, X, y, n, stop=1e-10):\n    diff = 1\n    for _ in range(1, n):\n        cnew = update(c, delta, X, y)\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = gd(c, delta, X, y, 50)\nc_20 = gd(c_10, delta, X, y, 50)\nc_30 = gd(c_20, delta, X, y, 200)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"$n=50$\")\nplt.plot(xf, y2, label=r\"$n=100$\")\nplt.plot(xf, y3, label=r\"$n=300$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.2: Line fit with gradient decent for different number of iterations and learning rate 2e-3.\nThe above algorithm uses the entire set \\(X\\) for the computation. For a large enough set \\(X\\) this is quite cost intense, even if it is still cheaper than computing \\(X^\\dagger\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "href": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "title": "7  Non-linear Regression",
    "section": "",
    "text": "Warning\n\n\n\nA function does not necessarily experience gravity in the same way as we do, so please do not try this in real live, i.e. cliffs tend to be hard to walk down.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.1 (Gradient decent) For the function \\[\nf(x) = x_1^2 + 3 x_2^2\n\\] we can compute the gradient as \\[\n\\nabla f (x)= \\left[ \\begin{array}{c} \\partial_{x_1} f(x)\\\\ \\partial_{x_2} f(x) \\end{array} \\right] = \\left[ \\begin{array}{c} 2 x_1 \\\\ 6 x_2 \\end{array} \\right]\n\\] Resulting in \\[\nx^{k+1} = x^{(k)} - \\delta \\, \\nabla f(x^{(k)}) =\n\\left[ \\begin{array}{c} (1 - 2 \\delta) x^{(k)}_1 \\\\ (1 - 6 \\delta)x^{(k)}_2 \\end{array} \\right].\n\\] Consequently \\[\nF(\\delta) = (1-2\\delta)^2 x_1^2 + (1-6\\delta)^2 x_2^2,\n\\] \\[\n\\partial_\\delta F = -2^2(1-2\\delta)x_1^2 - 6^2(1-6\\delta)x_2^2,\n\\] and \\[\n\\partial_\\delta F(\\delta) = 0 \\Leftrightarrow \\delta = \\frac{x_1^2 + 9 x_2^2}{2 x_1^2 + 54 x_2^2}.\n\\]\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\nimport numpy as np\n\nx_ = np.linspace(-3, 3, 20)\ny_ = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_, y_)\n\nf = lambda x, y: np.pow(x, 2) + 3 * np.pow(y, 2)\ngrad_f = lambda x: x * np.array([2, 6]).reshape(x.shape)\ndelta = lambda x: (x[0]**2 + 9 * x[1]**2)/(2 * x[0]**2 + 54 * x[1]**2)\n\nZ = f(X, Y)\n\nfig = go.Figure()\nfig.add_trace(go.Surface(z=Z, x=X, y=Y, colorscale='greys', name=\"Function\"))\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\", project_z=True))\nx = np.array([3, 2]).reshape((1, 2))\nz = np.array(f(x[0, 0], x[0, 1]))\ndiff = 1\n\nwhile diff &gt; 1e-10:\n    x_new = x[-1, :] - delta(x[-1, :]) * grad_f(x[-1, :])\n    z = np.hstack((z, f(x_new[0], x_new[1])))\n    diff = np.linalg.norm(z[-1] - z[-2])\n    x = np.vstack((x, x_new))\n\nfig.add_scatter3d(x=x[:, 0], y=x[:, 1], z=z, line_color='red', name=\"Decent path\")\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 7.1: Gradient decent applied for the function \\(f(x) = x_1^2 + 3x_2^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can not compute the gradient analytically there are numerical methods to help do the computation.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn machine learning the parameter \\(\\delta\\) is often called the learning rate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#stochastic-gradient-descent",
    "href": "regression/nonlinear.html#stochastic-gradient-descent",
    "title": "7  Non-linear Regression",
    "section": "7.2 Stochastic Gradient Descent",
    "text": "7.2 Stochastic Gradient Descent\nIn order to reduce cost we can randomly select some points of our training set and only train with those. Obviously the computation of the gradient becomes much faster. We call this method Stochastic Gradient Decent (SGD).\nIn Figure 7.3 we see the convergence for randomly selecting 1, 3, and 6 indices of our possible 10.\nThe downside of the SGD algorithm is that the algorithm does not settle down for a long time and will jump. In the other side it might get less stuck in local minima.\nOne possibility to try to get the strength of both is to use SDG to get a good guess for your initial value and SD for the fine tuning.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef sgd(c, delta, X, y, n, indices=-1, stop=1e-10):\n    if indices == -1:\n        indices = X.shape[0]\n    diff = 1\n    for _ in range(1, n):\n        i = np.random.choice(X.shape[0], size=indices, replace=False)\n        i.sort()\n        cnew = update(c, delta, X[i, :], y[i])\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = sgd(c, delta, X, y, 200, 1)\nc_20 = sgd(c, delta, X, y, 200, 3)\nc_30 = sgd(c, delta, X, y, 200, 5)\nc_ft = gd(c_20, delta, X, y, 150, -1)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\nyft = np.polyval(c_ft, xf)\ny4 = np.polyval(p4, xf)\n\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"#I=$1$\")\nplt.plot(xf, y2, label=r\"#I=$3$\")\n#plt.plot(xf, y3, label=r\"#I=$5$\")\nplt.plot(xf, yft, label=r\"#I=$3$ GD $n=100$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\n\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.3: Line fit with stochastic gradient decent with 1 or 3 samples and 200 iterations as well as the 3 sample version as initial guess for GD with 100 iterations.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#categorical-variables",
    "href": "regression/nonlinear.html#categorical-variables",
    "title": "7  Non-linear Regression",
    "section": "7.3 Categorical Variables",
    "text": "7.3 Categorical Variables\nEven with our excursion to non-linear regression we still had somewhat regular data to work with. This is not always the case. Sometimes there are trends in the data, like per month, or day. The inclusion of categorical variables can help to control for trends in the data.\nWe can integrate such variables to the regressor by adding columns to the matrix \\(X\\) for each of the categories. Note, they can be interpreted as to correspond to the offset (the constant \\(1\\)) so this column can be omitted and each category gets a separate offset.\nWe can see this in action in the following example. We investigate the unemployment data in Austria. There is a strong seasonality Figure 7.4 (b) in the data. This is largely due to the fact that the Austrian job market has a large touristic sector with its season and the construction industry employs less people during summer.\nFor the regression Figure 7.4 (b) we can see that this captures the seasonal change quite well.\nThe data is taken from Arbeitsmarktdaten online.\n\n\n\n\n\n\n\n\n\n\n\n(a) Regression with categorical variables per month.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Seasonality of the unemployment average over the years.\n\n\n\n\n\n\n\nFigure 7.4: Unemployment data from Austria for the years 2010 to 2017.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is also quite a difference between man and woman that could be categorized separately.\n\n\nWe wrap up this section about regression by talking more abstract about the regression of linear systems and some general thoughts about the selection of the model and consequences.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html",
    "href": "regression/optimizers.html",
    "title": "8  Optimizers",
    "section": "",
    "text": "8.1 Over-Determined Systems\nWe speak of an over-determined system if we have more rows than columns, i.e. \\(A\\) is tall and skinny and in general there is no solution to Equation 8.1 but rather we minimize the error according to a norm, see (ex-regr-linear-ols?). If we further impose a restriction on \\(x\\) we can select a more specific solution.\nThe generalized form is \\[\nx = \\underset{v}{\\operatorname{argmin}} \\|Av - b\\|_2 + \\lambda_1 \\|v\\|_1 + \\lambda_2\\|v\\|_2\n\\tag{8.2}\\] where the parameters \\(\\lambda_1\\) and \\(\\lambda_2\\) are called the penalization coefficients, with respect to the norm. Selecting these coefficients is the first step towards model selection.\nLet us have a look at this in action for solving a random system with different parameters \\(\\lambda_1\\) and setting \\(\\lambda_2\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#over-determined-systems",
    "href": "regression/optimizers.html#over-determined-systems",
    "title": "8  Optimizers",
    "section": "",
    "text": "8.1.1 LASSO\nThe least absolute shrinkage and selection operator LASSO solves Equation 8.2 with \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2=0\\), i.e. only optimizing with the \\(l_1\\) norm. The theory tells us that for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0= np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 1]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 50, rf\"$\\lambda={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 70)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.1: LASSO regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.\n\n\n\n\n\nThe last row of Figure 8.1 confirms this quite impressively, interesting enough the solution also becomes positive.\n\n\n8.1.2 RIDGE\nThe Ridge Regression solves Equation 8.2 with \\(\\lambda_1 = 0\\) and \\(\\lambda_2 &gt; 0\\), i.e. only optimizing with the \\(l_2\\) norm. The theory tells us that for for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0= np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 2]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 15, rf\"$\\lambda={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 20)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.2: Ridge regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#sec-regression-optimizers-msou",
    "href": "regression/optimizers.html#sec-regression-optimizers-msou",
    "title": "8  Optimizers",
    "section": "8.2 Model Selection/Identification and over-/underfitting",
    "text": "8.2 Model Selection/Identification and over-/underfitting\nLet us use the results we have obtain so far for a discussion on model selection.\nSo far, we have mostly explicitly proposed a model that we think will fit our data and we have seen that even it this case we can still choose multiple parameters to fin tune our selection.\nNow consider the other possibility, we have data where the model is unknown. For example, in Example 6.1 we stopped with degree 2 for our polynomial because we know about Newton’s principles, if we don’t know it, we might extend the model for a higher degree.\nOne of the leading assumptions to use in such a case is:\n\nAmong competing hypotheses, the one with the fewest assumptions should be selected, or when you have two competing theories that make exactly the same predictions, the simpler one is the more likely. - Occam’s razor\n\nThis plays an intimate role in over- and underfitting of models. To illustrate this we recall Example 6.2 with Figure 6.2 as seen below once more.\n\n\n\n\n\n\n\n\nFigure 8.3: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\nFor \\(m=1\\), a straight line, we have an underfitted model. We can not adequately capture the underlying model, at least not in the entire region.\nIf we move to \\(m=16\\) and the extreme \\(m=300\\) we see an overfitted system. The \\(m=16\\) curve follows clusters of points too close, e.g. in the region around \\(x=-2\\), this is more pronounced for \\(m=300\\) where we quite often closely follow our observations but between them we clearly overshoot.\nIn this way we can also say that an overfitted system follows the training set to closely and will not generalize good for another testing/evaluation set.\nAs a consequence model selection should always be followed by a cross-validation. Meaning we need to check if our model is any good.\nA classic method is the k-fold cross validation:\n\nTake random portions of your data and build a model. Do this \\(k\\) times and average the parameter scores (regression loadings) to produce a cross-validated model. Test the model predictions against withheld (extrapolation) data and evaluate whether the model is actually any good. - (see Brunton and Kutz 2022, 159)\n\nAs we can see there are a log of further paths to investigate but for now this concludes our excursion into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Collection of interesting reads",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#collection-of-interesting-reads",
    "href": "summary.html#collection-of-interesting-reads",
    "title": "Summary",
    "section": "",
    "text": "Semantic Versioning How to design the version of your project.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A\nPractical Introduction for Mechanical, Civil, and Aerospace\nEngineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -.\nSebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed.\nSebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber,\nEsteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population\nGrowth.” Our World in Data.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on\nIntroduction. München: No Starch Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in the form of Linear Algebra as well as some topics of statistics of sets. We will always immediately show how to use the discussed content in Python.\n\n\n\n\n\n\nNote\n\n\n\nThese notes assume that you have some basic knowledge of programming in Python and we build on this knowledge. In this sense, we use Python as a tool and only describe the inner workings if it helps us to better understand the topics at hand.\nIf this is not the case have a look at MECH-M-DUAL-1-SWD, a class on software design in the same master program and from the same authors.\nAdditionally, we can recommend the following books on Python:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming; Online Material.\nPython Cheat Sheet provided by Matthes (2023).\nMcKinney (2022): Python for data analysis 3e; Online and Print\nVasiliev (2022): Python for Data Science - A Hands-On Introduction\nInden (2023): Python lernen – kurz & gut; German\n\n\n\n\n\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -. Sebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed. Sebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on Introduction. München: No Starch Press.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "matrixdc/index.html",
    "href": "matrixdc/index.html",
    "title": "Matrix decompositions",
    "section": "",
    "text": "There are a lot of different matrix decompositions and they can be used to fulfil several tasks. We are going to look into the eigendecomposition as well as the singular value decomposition. Both of these can, for example, be used for picture compression and recognition.\nFor notation we are following again Golub and Van Loan (2013), which has plenty more to offer than we cover in these notes.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regression analysis",
    "section": "",
    "text": "In general, regression analysis can be understood as a set of tools that is used to estimate or establish a relationship between a dependent variable \\(Y\\) (also called outcome or response variable, label) and the independent variable \\(X\\) (also called regressor, predictors, covariates, explanatory variable or feature). If we add a regression function \\(f\\) and some unknown parameters \\(b\\) to the mix the problem can be written mathematically as \\[\nY = f(X, c)\n\\tag{1}\\] where \\(b\\) is found by optimizing for a good fit of \\(f\\) to the data.\nWe split up the discussion along the well known topics:\n\n6  Linear Regression\n7  Non-linear Regression\n\n7.1 Gradient Descent\n\n8  Optimizers\n\n8.2 Model Selection/Identification and over-/underfitting\n\n\nParts of this section are based on (Brunton and Kutz 2022, sec. 4).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis"
    ]
  }
]