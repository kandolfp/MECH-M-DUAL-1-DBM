[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Warning\n\n\n\nThe notes presented here are still under construction and can change without warning.\n\n\n\nPreface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.\n\n\nAcknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger for testing, checking, suggestions and general proofreading.\nThese notes are build with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basics behind modern day Data Science. We will always try to not only discuss the theory but also use Python to illustrate it and work on it programmatically.\nFor the class we will orient ourself on the first couple of chapters of Brunton and Kutz (2022), where we will highlight sections with lose similarity.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R}\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\nC = A^T, \\quad \\Rightarrow \\quad c_{ij} = a_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define the dot product or scalar product of two vectors \\(v\\) and \\(w\\) as \\[\\langle v, w\\rangle = v \\cdot w = v^T w = \\sum_i v_i w_i\\]\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nWe also have the outer product defined as: \\[\nv w^T = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor an \\({m \\times p}\\) matrix \\(A\\) and an \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition - Vector space\n\n\n\nFor a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following assumptions need to hold true to form a vector space:\nFor the vector addition:\n\nassociativity: \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity: \\[u + v = v + u,\\]\nthere exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector \\[v + 0 =  v,\\]\nthere exists an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] usually it is denoted by \\(-v\\).\n\nFor the scalar multiplication:\n\nassociativity: \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\nthe multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v\\]\ndistributivity with respect to vector addition: \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v.\\]\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n Exercise - Vector space in Python\n\n\n\n\n\nCreate some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix as vector space?\n\n\n\n\n\nHow do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition - linear map\n\n\n\nA linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a field \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w)\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix multiplication?\n\n\n\n\n\nShow that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\nDefinition - norm\n\n\n\nA norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers.\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity: \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity: \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac{1}{p}},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAn for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |a_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{sum_i sum_j |a_{ij}|^2}\\]\nthe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(\\sum_i \\sum_j |a_{ij}|^p\\right)^{\\frac1p}\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |a_{ij}|,\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#basis-of-vector-spaces",
    "href": "basics/linearalgebra.html#basis-of-vector-spaces",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\nDefinition - basis\n\n\n\nA set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\} \\in \\mathbb{R}^n\\):\n\nis called linear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}\\]\n\nThe set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\).\nThe easiest way to create one, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\n\n\n\n\n\n\n Example - Standard basis\n\n\n\n\n\n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\n# compute the coefficients\na = np.dot(x, e_1)\nb = np.dot(x, e_2)\nc = np.dot(x, e_3)\ny = a * e_1 + b * e_2 + c * e_3\nprint(f\"{y=}\")\nnp.testing.assert_allclose(x, y)\nprint(f\"{LA.norm(x-y)=}\")\n\nx=array([0.50669096, 0.70044705, 0.23254337])\ny=array([0.50669096, 0.70044705, 0.23254337])\nLA.norm(x-y)=np.float64(0.0)\n\n\nSee numpy.testing for help more ways of testing in numpy.\n\n\n\nWe call \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & 0 & 0 \\\\\n                        0 & 1 & \\dots & 0 \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        0 & 0 & \\dots & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n}\n\\] the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix. We can also think of it as the matrix that has all the standard basis vectors combined.\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\nDefinition - inverse\n\n\n\nFor matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\nThe following holds true for the inverse of matrices:\n\nthe inverse of a product is the product of the inverses \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^T = (A^{T})^{-1} \\equiv A^{-T}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nnp.testing.assert_almost_equal(A @ X, np.eye(A.shape[0]))\n# Note that the equality is hard to achieve for floats\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\nA=array([[0.7064993 , 0.0358206 , 0.992307  ],\n       [0.11226149, 0.16451945, 0.02677823],\n       [0.93768929, 0.0682723 , 0.01608129]])\nX=array([[-5.67438840e-03, -4.66259547e-01,  1.12654794e+00],\n       [-1.61764269e-01,  6.37991854e+00, -6.41932068e-01],\n       [ 1.01763208e+00,  1.01661588e-01, -7.78903030e-01]])\nA @ X=array([[ 1.00000000e+00,  1.62332375e-17,  1.73451725e-16],\n       [ 2.75323643e-18,  1.00000000e+00,  1.71146786e-17],\n       [-1.08514329e-18, -2.73462368e-18,  1.00000000e+00]])\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 8\n      6 np.testing.assert_almost_equal(A @ X, np.eye(A.shape[0]))\n      7 # Note that the equality is hard to achieve for floats\n----&gt; 8 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.6/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 8 / 9 (88.9%)\nMax absolute difference among violations: 1.73451725e-16\nMax relative difference among violations: 1.11022302e-16\n ACTUAL: array([[ 1.000000e+00,  1.623324e-17,  1.734517e-16],\n       [ 2.753236e-18,  1.000000e+00,  1.711468e-17],\n       [-1.085143e-18, -2.734624e-18,  1.000000e+00]])\n DESIRED: array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/sets.html",
    "href": "basics/sets.html",
    "title": "2  Data sets",
    "section": "",
    "text": "2.1 Basic properties of a data set\nFirst we are looking at the total net rent, i.e. the row nm.\nFor a vector \\(v \\in \\mathbb{R}^n\\) we have:\nnm_max = np.max(data['nm'])\nprint(f\"{nm_max=}\")\n\nnm_min = np.min(data['nm'])\nprint(f\"{nm_min=}\")\n\nnm_mean = np.mean(data['nm'])\n# round to 2 digits\nnm_mean_r = np.around(nm_mean, 2)\nprint(f\"{nm_mean_r=}\")\n\nnm_median = np.median(data['nm'])\nprint(f\"{nm_median=}\")\n\nnm_quartiles = np.quantile(data['nm'], [1/4, 1/2, 3/4])\nprint(f\"{nm_quartiles=}\")\n\nnm_max=np.float64(1789.55)\nnm_min=np.float64(77.31)\nnm_mean_r=np.float64(570.09)\nnm_median=np.float64(534.3)\nnm_quartiles=array([389.95, 534.3 , 700.48])\nFrom this Python snippet we know that for tenants the rent varied between 77.31 and 1789.55, with an average of 570.09 and a median of 534.3. Of course there are tricky questions that require us to dig a bit deeper into these functions, e.g. how many rooms does the most expensive flat have? The surprising answer is 3 and it was built in 1994, but how do we obtain these results?\nWe can use numpy.argwhere or a function which returns directly like numpy.argmax.\nmax_index = np.argmax(data['nm'])\nrooms = int(data['rooms'][max_index])\nyear = int(data['bj'][max_index])\nprint(f\"{rooms=}, {year=}\")\n\nrooms=3, year=1994",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#basic-properties-of-a-data-set",
    "href": "basics/sets.html#basic-properties-of-a-data-set",
    "title": "2  Data sets",
    "section": "",
    "text": "the maximal value \\[\nv^{max} = \\max_i v_i\n\\]\nthe minimal value \\[\nv^{min} = \\min_i v_i\n\\]\nthe mean of all values (often called the arithmetic mean) \\[\n\\overline{v} = \\frac1n \\sum_{i=1}^n v_i = \\frac{v_1 + v_2 + \\cdots + v_n}{n}\n\\]\nthe median, i.e. the value where half is bigger and the other half is smaller, for a sorted \\(v\\) this is \\[\n\\widetilde{v} = \\begin{cases}\n              v_{(n+1)/2}& n\\quad \\text{odd}\\\\\n              \\frac{v_{n/2} + v_{n/2+1}}{2}& n\\quad \\text{even}\n              \\end{cases}\n\\]\nmore general, we have quantiles. For a sorted \\(v\\) and \\(p\\in(0,1)\\) \\[\n\\overline{v}_p = \\begin{cases}\n               \\frac12\\left(v_{np} + v_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n              v_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n              \\end{cases}.\n\\] Some quantiles have special names, like the median for \\(p=0.5\\), the lower and upper quartile for \\(p=0.25\\) and \\(p=0.75\\) (or first, second (median) and third quartile), respectively.\n\n\n\n\n\n\n2.1.1 Visualization\n\n\n\n\n\n\nTip\n\n\n\nThere are various ways of visualizing data in Python. Two widely used packages are matplotlib and plotly.\n\n\nIt often helps to visualize the values to see differences and get an idea of their use.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nnm_sort = np.sort(data[\"nm\"])\nx = np.linspace(0, 1, len(nm_sort), endpoint=True,)\n\nplt.plot(x, nm_sort, label=\"net rent\")\nplt.axis((0, 1, np.round(nm_min/100)*100, np.round(nm_max/100)*100))\nplt.xlabel('Scaled index')\nplt.ylabel('Net rent - nm')\n\nplt.plot([0, 0.25, 0.25], [nm_quartiles[0], nm_quartiles[0], nm_min], \n         label='1st quartile')\nplt.plot([0, 0.5, 0.5], [nm_quartiles[1], nm_quartiles[1], nm_min],\n         label='2st quartile')\nplt.plot([0, 0.75, 0.75], [nm_quartiles[2], nm_quartiles[2], nm_min],\n         label='3st quartile')\nplt.plot([0, 1], [nm_mean, nm_mean],\n         label='mean')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the different measurements.\n\n\n\n\n\nWhat is shown in Figure 2.1 is often combined into a single boxplot (see Figure 2.2) that provides way more information at once.\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"Standard\"))\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"With points\", boxpoints=\"all\"))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.2: Boxplot done in plotly with whiskers following 3/2 IQR.\n\n\n\n\nThe plot contains the box which is defined by the 1st quantile \\(Q_1\\) and the 3rd quantile \\(Q_3\\), with the median as line in between these two. Furthermore, we can see the whiskers which help us identify so called outliers. By default they are defined as \\(\\pm 1.5(Q_3 - Q_1)\\), where (\\(Q_3 - Q_1\\)) is often called the interquartile range (IQR).\n\n\n\n\n\n\nNote\n\n\n\nFigure 2.2 is an interactive plot in the html version.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#spread",
    "href": "basics/sets.html#spread",
    "title": "2  Data sets",
    "section": "2.2 Spread",
    "text": "2.2 Spread\nThe spread (or dispersion, variability, scatter) are measures used in statistics to classify how data is distributed. Common examples are variance, standard deviation, and the interquartile range that we have already seen above.\n\n\n\n\n\n\nDefinition - variance\n\n\n\nFor a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the variance is defined as \\[\n\\operatorname{Var}(v) = \\frac1n \\sum_{i=1}^n (v_i - \\mu)^2, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] or directly \\[\n\\operatorname{Var}(v) = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j&gt;i} (v_i - v_j)^2.\n\\]\n\n\n\n\n\n\n\n\nDefinition - standard deviation\n\n\n\nFor a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\frac1n \\sum_{i=1}^n (v_i - \\mu)^2}, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] If we interpret \\(v\\) as a sample this is often also called uncorrected sample standard deviation.\n\n\n\n\n\n\n\n\nDefinition - interquartile range (IQR)\n\n\n\nFor a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the interquartile range is defined as the difference of the first and third quartile, i.e. \\[\nIQR = \\overline{v}_{0.75} - \\overline{v}_{0.25}.\n\\]\n\n\nWith numpy they are computed as follows\n\nnm_var = np.var(data[\"nm\"])\nprint(f\"{nm_var=}\")\n\nnm_std = np.std(data[\"nm\"])\nprint(f\"{nm_std=}\")\n\nnm_IQR = nm_quartiles[2] - nm_quartiles[0]\nprint(f\"{nm_IQR=}\")\n\nnm_var=np.float64(60208.75551600402)\nnm_std=np.float64(245.37472468859548)\nnm_IQR=np.float64(310.53000000000003)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#histogram",
    "href": "basics/sets.html#histogram",
    "title": "2  Data sets",
    "section": "2.3 Histogram",
    "text": "2.3 Histogram\nWhen exploring data it is also quite useful to draw histograms. For the net rent this makes not much sense but for rooms this is useful.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['rooms'])\nplt.xlabel('rooms')\nplt.ylabel('# of rooms')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of the number of rooms in our dataset.\n\n\n\n\n\nWhat we see in Figure 2.3 is simply the amount of occurrences of \\(1\\) to \\(6\\) in our dataset. Already we can see something rather interesting, there are flats with \\(5.5\\) rooms in our dataset.\nAnother helpful histogram is Figure 2.4 showing the amount of buildings built per year.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['bj'])\nplt.xlabel('year of building')\nplt.ylabel('# of buildings')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Histogram of buildings built per year.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#correlation",
    "href": "basics/sets.html#correlation",
    "title": "2  Data sets",
    "section": "2.4 Correlation",
    "text": "2.4 Correlation\nIn statistics, the terms correlation or dependence describe any statistical relationship between bivariate data (data that is paired) or random variables.\nFor our dataset we can, for example, check:\n\nthe living area in \\(m^2\\) - wfl vs. the net rent - nm\nthe year of construction - bj vs. if central heating - zh0 is available\nthe year of construction - bj vs. the city district - bez\n\n\n\nShow the code for the figure\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=data[\"wfl\"], y=data[\"nm\"], mode=\"markers\"), row=1, col=1)\nfig.update_xaxes(title_text=\"living area in m^2\", row=1, col=1)\nfig.update_yaxes(title_text=\"net rent\", row=1, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"zh0\"], mode=\"markers\"), row=2, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=2, col=1)\nfig.update_yaxes(title_text=\"central heating\", row=2, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"bez\"], mode=\"markers\"), row=3, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=3, col=1)\nfig.update_yaxes(title_text=\"city district\", row=3, col=1)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.5: Scatterplot to investigate correlations in the data set.\n\n\n\n\nIn the first plot of Figure 2.5 we see that the rent tends to go up with the size of the flat but there are for sure some rather cheap options in terms of space.\nThe second plot of Figure 2.5 tells us that central heating became a constant around \\(1966\\). Of course we can also guess that the older buildings with central heating were renovated, but we have no data to support this claim.\nThe third plot of Figure 2.5 does not yield an immediate correlation.\nMore formally, we can describe possible correlations using the covariance. The covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\nDefinition - covariance\n\n\n\nFor two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the covariance is defined as \\[\n\\operatorname{cov}(v, w) = \\frac1n \\langle v -\\overline{v}, w - \\overline{w}\\rangle\n\\]\n\n\nThe covariance is tricky to interpret, e.g. the unities of the two must not make sense. In the example below, we have rent per square meter, which makes some sense.\nFrom the covariance we can compute the correlation.\n\n\n\n\n\n\nDefinition - correlation\n\n\n\nFor two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the correlation is defined as \\[\n\\rho_{v,w} = \\operatorname{corr}(v, w) = \\frac{\\operatorname{cov}(v, w)}{\\sigma_v \\sigma_w}\n\\]\n\n\nIn numpy the function numpy.cov computes a matrix where the diagonal is the variance of the values and the off-diagonals are the covariances of the \\(i\\) and \\(j\\) samples. Consequently, numpy.corrcoef is a matrix as well.\n\ncov_nm_wfl = np.cov(data[\"nm\"], data[\"wfl\"])\nprint(f\"{cov_nm_wfl[0,1]=}\")\n\ncorr_nm_wfl = np.corrcoef(data[\"nm\"], data[\"wfl\"])\nprint(f\"{corr_nm_wfl[0,1]=}\")\n\ncov_nm_wfl[0,1]=np.float64(4369.1195844122)\ncorr_nm_wfl[0,1]=np.float64(0.7074626685750687)\n\n\nThe above results, particularly \\(\\rho_{\\text{nm},\\text{wfl}}=0.707\\) suggest that the higher the rent, the more space you get.\n\n\n\n\n\n\nTip\n\n\n\nCorrelation and causation are not the same thing!\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe showed some basic tests for correlation, there are more elaborate methods but they are subject to a later chapter.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/epilogue.html",
    "href": "basics/epilogue.html",
    "title": "3  Epilogue",
    "section": "",
    "text": "This sums up our basic introduction. We introduced the basic mathematical constructs to use in further sections and learned how to work with them in Python.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Epilogue</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html",
    "href": "matrixdc/eigen.html",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "We start off with the so called eigendecomposition. The main idea is to compute a decomposition (or factorization) of a square matrix into a canonical form. We will represent a matrix by its eigenvalues and eigenvectors. In order to do so we need to recall some more definitions from our favourite linear algebra book, such as Golub and Van Loan (2013).\n\n\n\n\n\n\nDefinition - Determinat\n\n\n\nIf \\(A = (a) \\in \\mathbb{R}^{1\\times1}\\) (a single number), then its determinant is given by \\(\\det(A) = a\\). Now, the determinant of \\(A \\in \\mathbb{R}^{n\\times n}\\) is defined in terms of order-\\((n-1)\\) determinants: \\[\n\\det(A) = \\sum_{j=1}^n (-1)^{j+1} a_{1j}\\det(A_1j),\n\\] where \\(A_1j\\) is an \\((n-1) \\times (n-1)\\) matrix obtained by deleting the first row and the \\(j\\)th column of \\(A\\).\n(Compare Golub and Van Loan 2013, 66–67)\n\n\nSome important properties of the determinant are:\n\nfor two matrices \\(A\\) and \\(B\\) in \\(\\mathbb{R}^{n\\times n}\\) \\[\n\\det(AB) = \\det(A)\\det(B),\n\\]\nfor the transpose of a matrix \\(A\\) \\[\n\\det(A^T) = \\det(A),\n\\]\nfor a scalar \\(\\alpha\\in\\mathbb{R}\\) and a matrix \\(A\\) \\[\n\\det(\\alpha A) = \\alpha^n\\det(A),\n\\]\nwe call a matrix \\(A\\) nonsingular if and only if \\[\n\\det(A) \\not= 0.\n\\]\n\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Collection of interesting reads",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#collection-of-interesting-reads",
    "href": "summary.html#collection-of-interesting-reads",
    "title": "Summary",
    "section": "",
    "text": "Semantic Versioning How to design the version of your project.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -.\nSebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed.\nSebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on\nIntroduction. München: No Starch Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in the form of Linear Algebra as well as some topics of statistics of sets. We will always immediately show how to use the discussed content in Python.\n\n\n\n\n\n\nNote\n\n\n\nThese notes assume that you have some basic knowledge of programming in Python and we build on this knowledge. In this sense, we use Python as a tool and only describe the inner workings if it helps us to better understand the topics at hand.\nIf this is not the case have a look at MECH-M-DUAL-1-SWD, a class on software design in the same master program and from the same authors.\nAdditionally, we can recommend the following books on Python:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming; Online Material.\nPython Cheat Sheet provided by Matthes (2023).\nMcKinney (2022): Python for data analysis 3e; Online and Print\nVasiliev (2022): Python for Data Science - A Hands-On Introduction\nInden (2023): Python lernen – kurz & gut; German\n\n\n\n\n\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -. Sebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed. Sebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on Introduction. München: No Starch Press.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "matrixdc/index.html",
    "href": "matrixdc/index.html",
    "title": "Matrix decompositions",
    "section": "",
    "text": "There are a lot of different matrix decompositions and they can be used to fulfil several tasks. We are going to look into the eigendecomposition as well as the singular value decomposition. Both of these can, for example, be used for picture compression and recognition.\nFor notation we are following again Golub and Van Loan (2013), which has plenty more to offer than we cover in these notes.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions"
    ]
  }
]