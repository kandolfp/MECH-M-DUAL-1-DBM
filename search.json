[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Warning\n\n\n\nThe notes presented here are still under construction and can change without warning.\n\n\n\nPreface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.\n\n\nAcknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger for testing, checking, suggestions and general proofreading.\nThese notes are build with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basics behind modern day Data Science. We will always try to not only discuss the theory but also use Python to illustrate it and work on it programmatically.\nFor the class we will orient ourself on the first couple of chapters of Brunton and Kutz (2022), where we will highlight sections with lose similarity.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R},\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n.\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\nC = A^T, \\quad \\Rightarrow \\quad c_{ij} = a_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define two more mappings.\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor a \\({m \\times p}\\) matrix \\(A\\) and a \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition 1.1 (Vector space) For a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following properties need to hold true to form a vector space.\nFor the vector addition we need to have\n\nassociativity \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity \\[u + v = v + u,\\]\nthere needs to exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector such that \\[v + 0 =  v,\\]\nthere needs to exist an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] and this element is usually denoted by \\(-v\\).\n\nFor the scalar multiplication we need to have\n\nassociativity \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\ndistributivity with respect to the vector addition \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v,\\]\nand there needs to exist a multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v.\\]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n Exercise - Vector space in Python\n\n\n\n\n\nCreate some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix as vector space?\n\n\n\n\n\nHow do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Linear map) A linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a field \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w),\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Dot product) The dot product or scalar product of two vectors \\(v\\) and \\(w\\) as is defined by \\[\\langle v, w\\rangle = v \\cdot w = v^T w = \\sum_i v_i w_i.\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Outer product) We also have the outer product defined as: \\[\nv w^T = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix multiplication?\n\n\n\n\n\nShow that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\n\nDefinition 1.5 (Norm) A norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac{1}{p}},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAnd for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |a_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{sum_i sum_j |a_{ij}|^2},\\]\nthe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(\\sum_i \\sum_j |a_{ij}|^p\\right)^{\\frac1p},\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |a_{ij}|.\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "href": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\n\nDefinition 1.6 (Basis) A set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\}, b_i \\in \\mathbb{R}^n\\):\n\nis called linear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}.\\]\n\nThe set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\nFor a shorter notation we often associate the matrix \\[\nB = \\left[b_1 | \\cdots | b_n\\right]\n\\] with the basis.\n\n\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\) and its associated matrix is \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & \\dots & 0\\\\\n                        0 & 1 & \\ddots & \\vdots \\\\  \n                        \\vdots & \\ddots & 1 & 0\\\\\n                        0 & \\dots & 0 & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n},\n\\] and called the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix.\nThe easiest way to create one of standard basis vectors, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\nand the identity matrix by\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n Example - Standard basis\n\n\n\n\n\n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\n# compute the coefficients\na = np.dot(x, e_1) / np.dot(e_1, e_1)\nb = np.dot(x, e_2) / np.dot(e_2, e_2)\nc = np.dot(x, e_3) / np.dot(e_3, e_3)\ny = a * e_1 + b * e_2 + c * e_3\nprint(f\"{y=}\")\nprint(f\"{np.allclose(x, y)=}\")\nprint(f\"{LA.norm(x-y)=}\")\n\nx=array([0.12738736, 0.3579606 , 0.33665708])\ny=array([0.12738736, 0.3579606 , 0.33665708])\nnp.allclose(x, y)=True\nLA.norm(x-y)=np.float64(0.0)\n\n\nSee numpy.testing for more ways of testing in numpy.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\n\nDefinition 1.7 (Matrix inverse) For matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\n\n\nThe following holds true for the inverse of matrices:\n\nthe inverse of a product is the product of the inverses \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^T = (A^{T})^{-1} \\equiv A^{-T}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nprint(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n# Note that the equality is hard to achieve for floats\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\nA=array([[0.57527147, 0.75879147, 0.37159354],\n       [0.66799428, 0.7617738 , 0.30623173],\n       [0.84113172, 0.32116651, 0.71754931]])\nX=array([[-6.51911526,  6.18270213,  0.737401  ],\n       [ 3.22477277, -1.45761373, -1.04792404],\n       [ 6.19852265, -6.5951289 ,  0.9982683 ]])\nA @ X=array([[ 1.00000000e+00,  1.21484493e-15,  4.12932919e-17],\n       [ 2.86260145e-16,  1.00000000e+00, -4.93555464e-18],\n       [ 2.89976067e-16,  3.95154174e-16,  1.00000000e+00]])\nnp.allclose(A @ X, np.eye(A.shape[0]))=True\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 8\n      6 print(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n      7 # Note that the equality is hard to achieve for floats\n----&gt; 8 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.7/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 8 / 9 (88.9%)\nMax absolute difference among violations: 1.21484493e-15\nMax relative difference among violations: 4.4408921e-16\n ACTUAL: array([[ 1.000000e+00,  1.214845e-15,  4.129329e-17],\n       [ 2.862601e-16,  1.000000e+00, -4.935555e-18],\n       [ 2.899761e-16,  3.951542e-16,  1.000000e+00]])\n DESIRED: array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n\n\n\n\n\n\nDefinition 1.8 (Change of basis) If we associate the matrices \\(B\\) and \\(C\\) with the matrix consisting of the basis vectors of two bases \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) of a vector space we can define the transformation matrix \\(T_{\\mathcal{C}}^{\\mathcal{B}}\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) as \\[\nT_{\\mathcal{C}}^{\\mathcal{B}} = C^{-1}B.\n\\]\nSo if we have a vector \\(b\\) represented in \\(\\mathcal{B}\\) we can compute its representation in \\(\\hat{b}\\) in \\(\\mathcal{C}\\) as \\[\n\\hat{b} = T_{\\mathcal{C}}^{\\mathcal{B}} b = C^{-1}B b.\n\\]\nA special form is if we have the standard basis \\(I\\) and move to a basis \\(C\\) we get \\[\n\\hat{b} = T_{C}^{I} b = C^{-1} b.\n\\]\n\n\n\n\n\n\n\n\n\n\n Example - basis change\n\n\n\n\n\nFor \\[\nB = \\left[\n    \\begin{array}{ccc} 1 & 3 & 2 \\\\\n                       0 & 1 & 1 \\\\  \n                       2 & 0 & 1 \\\\  \\end{array}\n\\right]\\quad \\text{and}\\quad\nC = \\left[\n    \\begin{array}{ccc} 1 & 0 & 1 \\\\\n                       0 & 1 & 1 \\\\  \n                       1 & 1 & 0 \\\\  \\end{array}\n\\right]\n\\] we get \\[\nT_{C}^{B} = \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right],\n\\] and for a \\(v = 2 b_1 - b_2 + 3 b_3\\) we can compute its representation in \\(C\\) as \\[\n\\hat{v} = T_{C}^{B} v\n= \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} 5 \\\\ 2 \\\\ 0 \\end{array}\n\\right],\n\\] and therefore, \\(v = 5c_1 + 2c_2 + 0 c_3\\).\n(Compare Wikipedia)\n\n\n\nThere are special basis vectors, respectively matrices that allow for easy computation of the inverse.\n\n\n\n\n\n\n\nDefinition 1.9 (Orthonormal vector) We call a set of vectors \\(\\mathcal{V}=\\{u_1, u_2, \\ldots, u_m\\}\\) orthonormal if and only if \\[\n\\forall i,j: \\langle u_i, u_j \\rangle = \\delta_{ij}\n\\] where \\(\\delta_{ij}\\) is called the Kronecker delta which is \\(1\\) if and only if \\(i=j\\) and \\(0\\) otherwise. This is true for a inner product, see Definition 1.3.\n\n\n\n\nExtending this to a matrix (and to that end a basis) as follows.\n\n\n\n\n\n\n\nDefinition 1.10 (Orthogonal matrix) We call a matrix \\(Q\\in\\mathbb{R}^{n\\times n}\\), here the real and square is important, orthogonal if its columns and rows are orthonormal vectors. This is the same as \\[\nQ^T Q = Q Q^T = I\n\\] and this implies that \\(Q^{-1} = Q^T\\).\n\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/sets.html",
    "href": "basics/sets.html",
    "title": "2  Data sets",
    "section": "",
    "text": "2.1 Basic properties of a data set\nFirst we are looking at the total net rent, i.e. the row nm.\nFor a vector \\(v \\in \\mathbb{R}^n\\) we have:\nnm_max = np.max(data['nm'])\nprint(f\"{nm_max=}\")\n\nnm_min = np.min(data['nm'])\nprint(f\"{nm_min=}\")\n\nnm_mean = np.mean(data['nm'])\n# round to 2 digits\nnm_mean_r = np.around(nm_mean, 2)\nprint(f\"{nm_mean_r=}\")\n\nnm_median = np.median(data['nm'])\nprint(f\"{nm_median=}\")\n\nnm_quartiles = np.quantile(data['nm'], [1/4, 1/2, 3/4])\nprint(f\"{nm_quartiles=}\")\n\nnm_max=np.float64(1789.55)\nnm_min=np.float64(77.31)\nnm_mean_r=np.float64(570.09)\nnm_median=np.float64(534.3)\nnm_quartiles=array([389.95, 534.3 , 700.48])\nFrom this Python snippet we know that for tenants the rent varied between 77.31 and 1789.55, with an average of 570.09 and a median of 534.3. Of course there are tricky questions that require us to dig a bit deeper into these functions, e.g. how many rooms does the most expensive flat have? The surprising answer is 3 and it was built in 1994, but how do we obtain these results?\nWe can use numpy.argwhere or a function which returns the index directly like numpy.argmax.\nmax_index = np.argmax(data['nm'])\nrooms = int(data['rooms'][max_index])\nyear = int(data['bj'][max_index])\nprint(f\"{rooms=}, {year=}\")\n\nrooms=3, year=1994",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#basic-properties-of-a-data-set",
    "href": "basics/sets.html#basic-properties-of-a-data-set",
    "title": "2  Data sets",
    "section": "",
    "text": "the maximal value, i.e. the maximum \\[\nv^{max} = \\max_i v_i,\n\\]\nthe minimal value, i.e. the minimum \\[\nv^{min} = \\min_i v_i,\n\\]\nthe mean of all values (often called the arithmetic mean) \\[\n\\overline{v} = \\frac1n \\sum_{i=1}^n v_i = \\frac{v_1 + v_2 + \\cdots + v_n}{n},\n\\]\nthe median, i.e. the value where half of all the other values are bigger and the other half is smaller, for a sorted \\(v\\) this is \\[\n\\widetilde{v} = \\begin{cases}\n              v_{(n+1)/2}& n\\quad \\text{odd}\\\\\n              \\frac{v_{n/2} + v_{n/2+1}}{2}& n\\quad \\text{even}\n              \\end{cases},\n\\]\nmore general, we have quantiles. For a sorted \\(v\\) and \\(p\\in(0,1)\\) \\[\n\\overline{v}_p = \\begin{cases}\n               \\frac12\\left(v_{np} + v_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n              v_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n              \\end{cases}.\n\\] Some quantiles have special names, like the median for \\(p=0.5\\), the lower and upper quartile for \\(p=0.25\\) and \\(p=0.75\\) (or first, second (median) and third quartile), respectively.\n\n\n\n\n\n\n2.1.1 Visualization\n\n\n\n\n\n\nTip\n\n\n\nThere are various ways of visualizing data in Python. Two widely used packages are matplotlib and plotly.\n\n\nIt often helps to visualize the values to see differences and get an idea of their use.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nnm_sort = np.sort(data[\"nm\"])\nx = np.linspace(0, 1, len(nm_sort), endpoint=True,)\n\nplt.plot(x, nm_sort, label=\"net rent\")\nplt.axis((0, 1, np.round(nm_min/100)*100, np.round(nm_max/100)*100))\nplt.xlabel('Scaled index')\nplt.ylabel('Net rent - nm')\n\nplt.plot([0, 0.25, 0.25], [nm_quartiles[0], nm_quartiles[0], nm_min], \n         label='1st quartile')\nplt.plot([0, 0.5, 0.5], [nm_quartiles[1], nm_quartiles[1], nm_min],\n         label='2st quartile')\nplt.plot([0, 0.75, 0.75], [nm_quartiles[2], nm_quartiles[2], nm_min],\n         label='3st quartile')\nplt.plot([0, 1], [nm_mean, nm_mean],\n         label='mean')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the different measurements.\n\n\n\n\n\nWhat is shown in Figure 2.1 is often combined into a single boxplot (see Figure 2.2) that provides way more information at once.\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"Standard\"))\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"With points\", boxpoints=\"all\"))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.2: Boxplot done in plotly with whiskers following 3/2 IQR.\n\n\n\n\nThe plot contains the box which is defined by the 1st quantile \\(Q_1\\) and the 3rd quantile \\(Q_3\\), with the median as line in between these two. Furthermore, we can see the whiskers which help us identify so called outliers. By default they are defined as \\(\\pm 1.5(Q_3 - Q_1)\\), where (\\(Q_3 - Q_1\\)) is often called the interquartile range (IQR).\n\n\n\n\n\n\nNote\n\n\n\nFigure 2.2 is an interactive plot in the html version.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#spread",
    "href": "basics/sets.html#spread",
    "title": "2  Data sets",
    "section": "2.2 Spread",
    "text": "2.2 Spread\nThe spread (or dispersion, variability, scatter) are measures used in statistics to classify how data is distributed. Common examples are variance, standard deviation, and the interquartile range that we have already seen above.\n\n\n\n\n\n\n\nDefinition 2.1 (Variance) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the variance is defined as \\[\n\\operatorname{Var}(v) = \\frac1n \\sum_{i=1}^n (v_i - \\mu)^2, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}\n\\] or directly \\[\n\\operatorname{Var}(v) = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j&gt;i} (v_i - v_j)^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Standard deviation) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\frac1n \\sum_{i=1}^n (v_i - \\mu)^2}, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] If we interpret \\(v\\) as a sample this is often also called uncorrected sample standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Interquartile range (IQR)) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the interquartile range is defined as the difference of the first and third quartile, i.e. \\[\nIQR = \\overline{v}_{0.75} - \\overline{v}_{0.25}.\n\\]\n\n\n\n\nWith numpy they are computed as follows\n\nnm_var = np.var(data[\"nm\"])\nprint(f\"{nm_var=}\")\n\nnm_std = np.std(data[\"nm\"])\nprint(f\"{nm_std=}\")\n\nnm_IQR = nm_quartiles[2] - nm_quartiles[0]\nprint(f\"{nm_IQR=}\")\n\nnm_var=np.float64(60208.75551600402)\nnm_std=np.float64(245.37472468859548)\nnm_IQR=np.float64(310.53000000000003)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#histogram",
    "href": "basics/sets.html#histogram",
    "title": "2  Data sets",
    "section": "2.3 Histogram",
    "text": "2.3 Histogram\nWhen exploring data it is also quite useful to draw histograms. For the net rent this makes not much sense but for rooms this is useful.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['rooms'])\nplt.xlabel('rooms')\nplt.ylabel('# of rooms')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of the number of rooms in our dataset.\n\n\n\n\n\nWhat we see in Figure 2.3 is simply the amount of occurrences of \\(1\\) to \\(6\\) in our dataset. Already we can see something rather interesting, there are flats with \\(5.5\\) rooms in our dataset.\nAnother helpful histogram is Figure 2.4 showing the amount of buildings built per year.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['bj'])\nplt.xlabel('year of building')\nplt.ylabel('# of buildings')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Histogram of buildings built per year.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#correlation",
    "href": "basics/sets.html#correlation",
    "title": "2  Data sets",
    "section": "2.4 Correlation",
    "text": "2.4 Correlation\nIn statistics, the terms correlation or dependence describe any statistical relationship between bivariate data (data that is paired) or random variables.\nFor our dataset we can, for example, check:\n\nthe living area in \\(m^2\\) - wfl vs. the net rent - nm\nthe year of construction - bj vs. if central heating - zh0 is available\nthe year of construction - bj vs. the city district - bez\n\n\n\nShow the code for the figure\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=data[\"wfl\"], y=data[\"nm\"], mode=\"markers\"), row=1, col=1)\nfig.update_xaxes(title_text=\"living area in m^2\", row=1, col=1)\nfig.update_yaxes(title_text=\"net rent\", row=1, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"zh0\"], mode=\"markers\"), row=2, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=2, col=1)\nfig.update_yaxes(title_text=\"central heating\", row=2, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"bez\"], mode=\"markers\"), row=3, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=3, col=1)\nfig.update_yaxes(title_text=\"city district\", row=3, col=1)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.5: Scatterplot to investigate correlations in the data set.\n\n\n\n\nIn the first plot of Figure 2.5 we see that the rent tends to go up with the size of the flat but there are for sure some rather cheap options in terms of space.\nThe second plot of Figure 2.5 tells us that central heating became a constant around \\(1966\\). Of course we can also guess that the older buildings with central heating were renovated, but we have no data to support this claim.\nThe third plot of Figure 2.5 does not yield an immediate correlation.\nMore formally, we can describe possible correlations using the covariance. The covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nDefinition 2.4 (Covariance) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the covariance is defined as \\[\n\\operatorname{cov}(v, w) = \\frac1n \\langle v -\\overline{v}, w - \\overline{w}\\rangle.\n\\]\n\n\n\n\nThe covariance is tricky to interpret, e.g. the unities of the two must not make sense. In the example below, we have rent per square meter, which makes some sense.\nFrom the covariance we can compute the correlation.\n\n\n\n\n\n\n\nDefinition 2.5 (Correlation) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the correlation is defined as \\[\n\\rho_{v,w} = \\operatorname{corr}(v, w) = \\frac{\\operatorname{cov}(v, w)}{\\sigma_v \\sigma_w}.\n\\]\n\n\n\n\nIn numpy the function numpy.cov computes a matrix where the diagonal is the variance of the values and the off-diagonals are the covariances of the \\(i\\) and \\(j\\) samples. Consequently, numpy.corrcoef is a matrix as well.\n\ncov_nm_wfl = np.cov(data[\"nm\"], data[\"wfl\"])\nprint(f\"{cov_nm_wfl[0, 1]=}\")\n\ncorr_nm_wfl = np.corrcoef(data[\"nm\"], data[\"wfl\"])\nprint(f\"{corr_nm_wfl[0, 1]=}\")\n\ncov_nm_wfl[0, 1]=np.float64(4369.1195844122)\ncorr_nm_wfl[0, 1]=np.float64(0.7074626685750687)\n\n\nThe above results, particularly \\(\\rho_{\\text{nm},\\text{wfl}}=0.707\\) suggest that the higher the rent, the more space you get.\n\n\n\n\n\n\nTip\n\n\n\nCorrelation and causation are not the same thing!\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe showed some basic tests for correlation, there are more elaborate methods but they are subject to a later chapter.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/epilogue.html",
    "href": "basics/epilogue.html",
    "title": "3  Epilogue",
    "section": "",
    "text": "This sums up our basic introduction. We introduced the basic mathematical constructs to use in further sections and learned how to work with them in Python.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Epilogue</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html",
    "href": "matrixdc/eigen.html",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1 Examples for the application\nTo get a better idea what the eigendecomposition can do we look into some examples.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#examples-for-the-application",
    "href": "matrixdc/eigen.html#examples-for-the-application",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1.1 Solving system of linear equations\nFor a system of linear equations \\(Ax=b\\) we get \\[\n\\begin{array}{lll}\nA x & = b  & \\iff \\\\\nV \\Lambda V^{-1} x & = b  & \\iff \\\\\n\\Lambda V^{-1} x & = V^{-1} b  & \\iff \\\\\nV^{-1} x & = \\Lambda^{-1}V^{-1} b  & \\iff \\\\\nx & = V\\Lambda^{-1}V^{-1} b  & \\iff\n\\end{array}\n\\] As \\(\\Lambda^{-1} = \\operatorname{diag}\\left(\\lambda_1^{-1}, \\ldots, \\lambda_n^{-1}\\right)\\) this is easy to compute once we have the eigenvalue decomposition.\n\n\n\n\n\n\nNote\n\n\n\nThe computation of the eigendecomposition is not cheap, therefore this is not always worth the effort and there are other ways of solving linear systems.\n\n\n\n\n4.1.2 Linear Ordinary Differential Equations\nIn this example we use the eigendecomposition to efficiently solve a system of differential equations \\[\n\\dot{x} = A x,\\quad x(0) = x_0\n\\] By changing to the basis \\(V\\) and using the notation \\(\\hat{x}=z\\) we have the equivalent formulations \\[\nz = V^{-1}x \\iff x = Vz,\n\\] and if follows \\[\n\\begin{array}{lll}\n\\dot{x} = A x & \\iff V \\dot{z} &= A V z \\\\\n              & \\iff \\dot{z} &= V^{-1} A V z \\\\\n              & \\iff \\dot{z} &= \\Lambda z\n\\end{array}.\n\\] So for an initial value \\(z_0\\) the solution in \\(t\\) is \\[\nz(t) = \\operatorname{diag}\\left(e^{t\\lambda_1}, \\ldots, e^{t\\lambda_n}\\right) z_0.\n\\]\nWe often say that it is now a decoupled differential equation.\n\n\n4.1.3 Higher Order Linear Differential Equations\nIf we have a higher order linear ODE such as \\[\nx^{(n)} + a_{n-1} x^{(n-1)} + \\cdots + a_2 \\ddot{x} + a_1 \\dot{x} + a_0 x = 0.\n\\tag{4.1}\\] we can stack the derivatives into a vector \\[\n\\begin{array}{ccc}\nx_1 & = & x\\\\\nx_2 & = & \\dot{x}\\\\\nx_3 & = & \\ddot{x}\\\\\n\\vdots & = & \\vdots \\\\\nx_{n-1} & = & x^{(n-2)} \\\\\nx_{n} & = & x^{(n-1)} \\\\\n\\end{array}\n\\quad\n\\Leftrightarrow\n\\quad\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} x \\\\ \\dot{x} \\\\ \\ddot{x} \\\\ \\vdots \\\\ x^{(n-2)} \\\\ x^{(n-1)} \\end{array}\n\\right],\n\\] and taking the derivative of this vector yields the following system \\[\n\\underbrace{\n\\frac{d}{d t}\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_{\\dot{x}}\n=\n\\underbrace{\\left[\n    \\begin{array}{cccccc} 0 & 1 & 0 & \\dots & 0 & 0\\\\\n                          0 & 0 & 1 & \\dots & 0 & 0\\\\  \n                          0 & 0 & 0 & \\dots & 0 & 0\\\\\n                          \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n                          0 & 0 & 0 & \\dots & 0 & 1\\\\\n                          -a_0 & -a_1 & -a_2 & \\dots & -a_{n-2} & -a_{n-1}\\\\\n    \\end{array}\n\\right]\n}_{A}\n\\underbrace{\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_x.\n\\]\nWe transformed it into a system of coupled 1st order ODEs \\(\\dot{x}=Ax\\) and we can solve this as seen above. More importantly, the characteristic polynomial of Equation 4.1 is equal to the characteristic polynomial of Definition 4.2 and the eigenvalues are the roots of this polynomial.\n\n\n4.1.4 Generalized eigenvalue problem\nLet us motivate this by the example of modal analysis. If we consider the free vibrations of a weakly undamped system we get the equation \\[\nM\\ddot{u} + K u = 0, \\quad u(0) = u_0,\n\\] with the mass matrix \\(M\\) and the stiffness matrix \\(K\\) and \\(u(t)\\) being the displacement. As we know, the solution of this linear differential equation has the form \\(u(t)=e^{i\\omega t}u_0\\) and thus we get \\[\n(-\\omega^2 M + K) u_0 = 0.\n\\tag{4.2}\\]\n\n\n\n\n\n\n\nDefinition 4.5 (Generalized eigenvalue problem) If \\(A, B \\in \\mathbb{C}^{n\\times n}\\), then the set of all matrices of the form \\(A-\\lambda B\\) with \\(\\lambda\\in\\mathbb{C}\\) is a pencil. The generalized eigenvalues of \\(A-\\lambda B\\) are elements of the set \\(\\lambda(A,B)\\) defined by \\[\n\\lambda(A,B) = \\{z\\in\\mathbb{C}: \\det(A-zB)=0\\}.\n\\] If \\(\\lambda \\in \\lambda(A,B)\\) and \\(0\\neq v\\in\\mathbb{C}^n\\) satisfies \\[\nA v = \\lambda B v,\n\\tag{4.3}\\] then \\(v\\) is an eigenvector of \\(A-\\lambda B\\). The problem of finding a nontrivial solution to Equation 4.3 is called the generalized eigenvalue problem.\n(Compare Golub and Van Loan 2013, chap. 7.7)\n\n\n\n\nIn our example Equation 4.2 the eigenvalues are \\(\\lambda = \\omega^2\\) and correspond to the square of the natural frequencies and the eigenvectors \\(v=u\\) correspond to the modes of vibration.\nIf \\(M\\) is invertible we can write \\[\nM^{-1}(K -\\omega^2 M ) u_0 = (M^{-1}K -\\omega^2 I ) u_0 = 0.\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases inverting the matrix is not recommended, to directly solve the generalized eigenvalue problem, see (Golub and Van Loan 2013, chap. 7.7) for details.\n\n\n\n\n4.1.5 Low-rank approximation of a square matrix\nWe can use the eigenvalue decomposition to approximate a square matrix.\nLet us sort the eigenvalues in \\(\\Lambda\\) and let us call \\(W^T=V^{-1}\\) then we can write \\[\nA = V\\Lambda W^T = \\sum_{i=1}^n\\lambda_i v_i u_i^T\n\\] where \\(v_i\\) and \\(u_i\\) correspond to the rows of the matrices. Now we can define the rank \\(r\\) approximation of \\(A\\) as \\[\nA\\approx A_r = V\\Lambda W^T = \\sum_{i=1}^r\\lambda_i v_i u_i^T.\n\\]\nTo make this a bit easier to understand the following illustration is helpful:\n\n\n\nLow Rank Approximation\n\n\nThis approximation can be used to reduce the storage demand of an image.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_cut = im_gray[1500:3001,1500:3001] / 255\n\nlam_, V_ = LA.eig(im_cut)\norder = np.argsort(np.abs(lam_))\nlam = lam_[order[::-1]]\nV = V_[:, order[::-1]]\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\nVinv = LA.inv(V)\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3,1,1)\nax_eig.plot((np.abs(lam)))\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\lambda_i|$\")\n#ax_eig.set_xlabel(\"$i$\")\nax_eig.set_title(\"absolute value of the eigenvalues\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(lam) * p))\n    A_r = np.real(V[:, 0:r] @ np.diag(lam[0:r], 0) @ Vinv[0:r, :])\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_cut, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.1: Image of MCI I and the reconstruction with approximated matrix.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#summary",
    "href": "matrixdc/eigen.html#summary",
    "title": "4  Eigendecomposition",
    "section": "4.2 Summary",
    "text": "4.2 Summary\nThe eigenvalue decomposition is an important tool but it has its limitations:\n\nthe matrices involved need to be square\neigenvalues might be complex, even if the problem at hand is real\nwe only get a diagonal matrix \\(\\Lambda\\) if all eigenvectors are linear independent\nthe computation of \\(V^{-1}\\) is non-trivial unless \\(A\\) is symmetric and \\(V\\) becomes unitary (\\(V^{-1} = V^T\\)).\n\nTherefore, we will look into a generalized decomposition called the singular value decomposition in the next section.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html",
    "href": "matrixdc/svd.html",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "5.1 Low rank approximation\nAgain, we can cut of the reconstruction at a certain point and create an approximation. More formally this is defined in the next definition.\nWe can use this for image compression.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_scale = im_gray[1500:3001, 1500:3001] / 255\n\nU, s, Vh = LA.svd(im_scale, full_matrices=False)\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3, 1, 1)\nax_eig.plot(s)\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\sigma_i|$\")\nax_eig.set_title(\"singular value\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(s) * p))\n    A_r = U[:, :r] @ np.diag(s[:r]) @ Vh[:r, :]\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_scale, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.1: Image of MCI I and the reconstruction with reduced rank matrices.\nAs the matrices \\(U\\) and \\(V\\) are orthogonal, they also define a basis of the corresponding (sub) vector spaces. As mentioned before, the SVD automatically selects these and they are optimal.\nConsequently, the matrices \\(U\\) and \\(V\\) can be understood as reflecting patterns in the image. We can think of the columns of \\(U\\) and \\(V\\) as the vertical respectively horizontal patterns of \\(A\\).\nWe can illustrate this by looking at the modes of our decomposition \\[\nM_k = U(:, k) V^T(k, :).\n\\]\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nrec = [0, 1, 2, 3, 4, 5]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(3, 3, 1))\naxs.append(fig.add_subplot(3, 3, 2))\naxs.append(fig.add_subplot(3, 3, 3))\naxs.append(fig.add_subplot(3, 3, 4))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 6))\n\nfor i, r in enumerate(rec):\n    M_r = np.outer(U[:, r], Vh[r, :])\n    axs[i].imshow(M_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r + 1\n    axs[i].set_title(f\"${r=}$\")\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Modes of the SVD decomposition of the MCI I image.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#low-rank-approximation",
    "href": "matrixdc/svd.html#low-rank-approximation",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "Definition 5.2 (Low-Rank Approximation) If \\(A \\in \\mathbb{R}^{m\\times n}\\) and has the SVD \\(A = U\\Sigma V^T\\) than \\[\nA_k = U(:, 1:k) \\Sigma(1:k, 1:k) V^T(1:k, :)\n\\] is the optimal low-rank approximation of \\(A\\) with rank \\(k\\). This is often called the truncated SVD.\n(See Golub and Van Loan 2013, Corollary 2.4.7 p. 79)\n\n\n\n\n\n\n\nTruncated SVD\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we compare this to Figure 4.1 we can see that we get a much better result for smaller \\(r\\). Let us have a look why.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe big advantage here is, that the selection is optimal. A disadvantage is that the need to store the basis separately and this increases the necessary storage. We will see in later sections about wavelets and Fourier decomposition how a common basis can be used to reduce the storage by still keeping good reconstructive properties.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#principal-component-analysis",
    "href": "matrixdc/svd.html#principal-component-analysis",
    "title": "5  Singular Value Decomposition",
    "section": "5.2 Principal Component Analysis",
    "text": "5.2 Principal Component Analysis\nOn of the most important applications of SVD is in the stable computation of the so called principal component analysis (PCA). It is a common technique in data exploration, analysis, visualization, and preprocessing.\nThe main idea of PCM is to transform the data in such a way that the main directions (principal components) capture the largest variation. In short we perform a change of the basis, see Definition 1.8.\nLet us investigate this in terms of a (artificial) data set.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: Noisy Gaussian Data, pp. 25-27).\n\n\nWe generate a noisy cloud (see Figure 5.3) that consists of \\(10000\\) points in 2D, generated from a normal distribution with zero mean and unit variance. The data is than:\n\nscaled by \\(2\\) in the first direction and by \\(\\frac12\\) in second,\nrotated by \\(\\frac\\pi3\\)\ntranslation in the direction \\(\\left[2\\ 1\\right]^T\\).\n\nThe resulting matrix \\(X\\) is a long and skinny matrix with each measurement (or experiment) stacked next to each other. This means, each column represents a new set, e.g. a time step, and each row corresponds to the same sensor.\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is a slight adaptation (for nicer presentation in these notes) of the (Brunton and Kutz 2022, Code 1.4) also see notebook on github.\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nxC = np.array([2, 1])      # Center of data (mean)\nsig = np.array([2, 0.5])   # Principal axes\n\ntheta = np.pi / 3            # Rotate cloud by pi/3\n\nR = np.array([[np.cos(theta), -np.sin(theta)],     # Rotation matrix\n              [np.sin(theta), np.cos(theta)]])\n\nnPoints = 10000            # Create 10,000 points\nX = R @ np.diag(sig) @ np.random.randn(2, nPoints) + np.diag(xC) @ np.ones((2, nPoints))\n\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(X[0, :], X[1, :], '.', color='k')\nax1.grid()\nax1.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\n## f_ch01_ex03_1b\n\nXavg = np.mean(A, axis=1)                  # Compute mean\nB = X - np.tile(Xavg, (nPoints, 1)).T       # Mean-subtracted data\n\n# Find principal components (SVD)\nU, S, VT = np.linalg.svd(B / np.sqrt(nPoints), full_matrices=False)\n\nax2 = fig.add_subplot(122)\nax2.plot(X[0, :], X[1, :], '.', color='k')   # Plot data to overlay PCA\nax2.grid()\nax2.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\ntheta = 2 * np.pi * np.arange(0, 1, 0.01)\n\n# 1-std confidence interval\nXstd = U @ np.diag(S) @ np.array([np.cos(theta), np.sin(theta)])\n\nax2.plot(Xavg[0] + Xstd[0, :], Xavg[1] + Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 2 * Xstd[0, :], Xavg[1] + 2 * Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 3 * Xstd[0, :], Xavg[1] + 3 * Xstd[1, :], '-', color='r', linewidth=3)\n\n# Plot principal components U[:,0]S[0] and U[:,1]S[1]\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,0] * S[0]]),\n         np.array([Xavg[1], Xavg[1] + U[1,0] * S[0]]), '-', color='cyan', linewidth=5)\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,1] * S[1]]),\n         np.array([Xavg[1], Xavg[1] + U[1,1] * S[1]]), '-', color='cyan', linewidth=5)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.3: Principal components of the mean-subtracted Gaussian data on the left as, as well as the first three standard deviation ellisoids and the two scaled left singular vectors.\n\n\n\n\n\n\n5.2.1 Computation\nFor the computation we follow the outline given in (Brunton and Kutz 2022, chap. 1.5). First we need to center our matrix \\(X\\) according to the mean per feature, in our case per row. \\[\n\\overline{x}_j = \\frac1n \\sum_{i=1}^n X_{ij}\n\\] and our mean matrix is the outer product with the one vector \\[\n\\overline{X} = \\left[\\begin{array}{c}1\\\\\\vdots\\\\1\\end{array}\\right] \\overline{x}\n\\] which can be used to compute the centred matrix \\(B = X - \\overline{X}\\).\nThe PCA is the eigendecomposition of the covariance matrix \\[\nC = \\frac{1}{n-1} B^T B\n\\tag{5.2}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe normalization factor of \\(n-1\\) in Equation 5.2 an not \\(n\\) is called Bassel’s correction and compensates for the bias in the estimation of the population variance.\n\n\nAs \\(C\\) is symmetric and positive semi-definite, therefore it has non-negative real eigenvalues and the matrix \\(V\\) of the eigendecomposition satisfies \\(V^{-1} = V^T\\) (i.e. it is orthogonal Definition 1.10). The principal components are the eigenvectors and the eigenvalue are the variance along these components.\nIf we instead compute the SVD of \\(B = U\\Sigma V^T\\) we get \\[\nC = \\frac{1}{n-1} B^TB = \\frac{1}{n-1} V \\Sigma V^T = \\frac{1}{n-1} V (\\Lambda^T\\Lambda) V^T\n\\] leading to a way of computing the principal components in a robust way as \\[\n\\lambda_k = \\frac{\\sigma_k^2}{n-1}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIf the sensor ranges of our matrix are very different in magnitude the correlation matrix is scaled by the row wise standard deviation of \\(B\\) similar as for the mean.\n\n\nIn our example we get \\(\\sigma_1=1.996\\approx 2\\) and \\(\\sigma_2=0.573\\approx \\frac12\\). These results recover our given parameters very well. Additionally we can see that our rotation matrix is closely matched by \\(U\\) (up to signs) from our SVD: \\[\nR_{\\frac\\pi3} = \\left[\n\\begin{array}{cc} 0.5&0.866\\\\-0.866&0.5\\end{array}\n\\right] \\quad U = \\left[\n\\begin{array}{cc}-0.513&-0.858\\\\-0.858&0.513\\end{array} \\right]\n\\]\n\n\n5.2.2 Example Eigenfaces\nWe combine SVD/PCA in a illustrative example called eigenfaces as introduced in (Brunton and Kutz 2022, Sec 1.6, pp. 28-34).\nThe idea is to apply the PCA techniques to a large set of faces to extract the dominate correlations between the images and create a face basis that can be used to represent an image in these coordinates. For example you can reconstruct a face in this space by projecting onto the eigen vectors or it can be used for face recognition as similar faces usually cluster under this projection.\nThe images are taken from the Yale Face Dataset B, in our case we use a GitHub that provides Julia Pluto notebooks for Chapter 1 to 4 of Brunton and Kutz (2022).\nOur training set, so to speak, consists of the first 36 people in the dataset. We compute the average face and subtract it from our dataset to get our matrix \\(B\\). From here a SVD provides us with our basis \\(U\\). To test our basis we use individual 37 and a portion of the image of the MCI Headquarter (to see how well it performs on objects). For this we use the projection \\[\n\\tilde{x} = U_r U_r^T x.\n\\] If we split this up, we first project onto our found patterns (encode) and than reconstruct from them (decode).\n\n\n\n\n\n\nNote\n\n\n\nWe can understand this as encoding and decoding our test image, which is the general setup of an autoencoder (a topic for another lecture).\nThe correlation coefficients \\(x_r = U_r^T x\\) might reveal patterns for different \\(x\\). In the case of faces, we can use this for face recognition, i.e. if the coefficients of \\(x_r\\) are in the same cluster as other images, they are probably from the same person.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is an adaptation of the (Brunton and Kutz 2022, Code 1.7 and 1.9).\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\ntrainingFaces = faces[:, : np.sum(nfaces[:36])]\navgFace = np.mean(trainingFaces, axis=1)\n\nB = trainingFaces - np.tile(avgFace, (trainingFaces.shape[1], 1)).T\nU, _, _ = LA.svd(B, 'econ')\n\ntestFace = faces[:, np.sum(nfaces[:36])]\ntestFaceMS = testFace - avgFace\nrec = [25, 100, 400]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(2, 4, 1))\naxs.append(fig.add_subplot(2, 4, 2))\naxs.append(fig.add_subplot(2, 4, 3))\naxs.append(fig.add_subplot(2, 4, 4))\naxs.append(fig.add_subplot(2, 4, 5))\naxs.append(fig.add_subplot(2, 4, 6))\naxs.append(fig.add_subplot(2, 4, 7))\naxs.append(fig.add_subplot(2, 4, 8))\n\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    axs[i].set_title(f\"${r=}$\")\n\naxs[3].imshow(np.reshape(testFace, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\naxs[3].set_axis_off()\naxs[3].set_title(f\"Original image\")\n\nshift = 1500\ntestFaceMS = np.reshape(im_gray[shift:shift+n, shift:shift+m].T, n*m) - avgFace\nrec = [100, 400, 1600]\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[4 + i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[4 + i].set_axis_off()\n    axs[4 + i].set_title(f\"${r=}$\")\naxs[7].imshow(im_gray[shift:shift+n, shift:shift+m], cmap=plt.get_cmap(\"gray\"))\naxs[7].set_axis_off()\naxs[7].set_title(f\"Original image\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.4: Approximate reconstruction of a test face and an object using the eigenfaces basis for different order r.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to resource limitations the above computation can not be done for each build. We try to make sure that the code matches the image but if something is different if you try it yourself we apologise for that.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#further-applications-of-the-svd",
    "href": "matrixdc/svd.html#further-applications-of-the-svd",
    "title": "5  Singular Value Decomposition",
    "section": "5.3 Further applications of the SVD",
    "text": "5.3 Further applications of the SVD\nThere are many more applications of the SVD but we want to highlight some regarding systems of linear equations, \\[\nA x = b\n\\tag{5.3}\\] where the matrix \\(A\\), as well as the vector \\(b\\) is known an \\(x\\) is unknown.\nDepending on the structure of \\(A\\) and the specific \\(b\\) we have no, one, or infinitely many solutions. For now the interesting case is where \\(A\\) is rectangular and therefore we have either an\n\nunder-determined system \\(m\\ll n\\), so more unknowns than equations,\nover-determined system \\(m\\gg n\\), so more equations than unknowns.\n\nFor the second case (more equations than unknowns) we often switch to solving the optimization problem that minimizes \\[\n\\|Ax-b\\|_2^2.\n\\tag{5.4}\\] This is called the least square solution. The least square solution will also minimize \\(\\|Ax-b\\|_2\\). For an under-determined system we might seek the solution which minimizes \\(\\|x\\|_2\\) called the minimum norm solution.\nIf we us the SVD decomposition for \\(A = U \\Sigma V^T\\) we can define the following\n\n\n\n\n\n\n\nDefinition 5.3 (Pseudo-inverse) We define the matrix \\(A^\\dagger \\in \\mathbb{R}^{m\\times n}\\) by \\(A^\\dagger = V\\Sigma^\\dagger U^T\\) where \\[\n\\Sigma^\\dagger = \\operatorname{diag}\\left(\\frac1\\sigma_1. \\frac1\\sigma_2, \\ldots, \\frac1\\sigma_r, 0, \\ldots, 0\\right) \\in \\mathbb{R}^{m\\times n}, \\quad r=\\operatorname{rank}(A).\n\\]\nThe matrix \\(A^\\dagger\\) is often called the Moore-Penrose left pseudo-inverse as it fulfils the Moore-Penrose conditions conditions. It is also the matrix to provides the minimal Frobenius norm solution to \\[\n\\min_{X \\in \\mathbb{R}^{m\\times n}}\\| A x - I_n\\|_F.\n\\]\n(Compare Golub and Van Loan 2013, 290)\n\n\n\n\nIf we only use the truncated version, i.e. where we only use non-zero singular values, we can use it to find good solutions to Equation 5.4.\n\n\n\n\n\n\n\nDefinition 5.4 (Condition number) The condition number of a matrix provides a measure how sensitive the solution of Equation 5.3 is to perturbations in \\(A\\) and \\(b\\). For a square matrix \\(A\\) the condition number is defined as \\[\n\\kappa(A) = \\|A\\| \\left\\|A^{-1}\\right\\|,\n\\] for an appropriate underlying norm. For the 2-norm \\(\\kappa_2\\) is \\[\n\\kappa_2(A) = \\|A\\|_2 \\left\\|A^{-1}\\right\\|_2 = \\frac{\\sigma_{max}}{\\sigma_{min}}.\n\\]\nTo get a better idea on what this means think of it in this way. For the perturbed linear system \\[\nA(x + \\epsilon_x) = b + \\epsilon_b,\n\\] we can outline the worst case, where \\(\\epsilon_x\\) aligns with the singular vector of the largest singular vector and \\(x\\) with the smallest singular value, i.e. \\[\nA(x + \\epsilon_x) = \\sigma_{min}x + \\sigma_{max}\\epsilon_x.\n\\] Consequently, the output signal-to-noise \\(\\|b\\|/\\|\\epsilon_b\\) is equivalent with the input signal-to-noise \\(\\|x\\|/\\|\\epsilon_x\\) and the factor between those two is \\(\\kappa_2(A)\\).\nIn this sense \\(\\kappa_2\\) can be extended for more general matrices.\n(Compare Golub and Van Loan 2013, 87; and Brunton and Kutz 2022, 18–19)\n\n\n\n\n\n5.3.1 Linear regression with SVD\nBefore we go into more details about regression in the next section we give a brief outlook in terms of how to solve such a problem with SVD.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: One-Dimensional Linear Regression, Example: Cement Heat Generation Data, pp. 19-22).\n\n\n\n5.3.1.1 Linear Regression (see Brunton and Kutz 2022, 19–21)\nFirst we just take a linear correlation that we augment with some Gaußian Noise. So our matrix \\(A\\) is simple a vector with our \\(x\\)-coordinates and \\(b\\) is the augmented image under our linear correlation. \\[\n\\left[\n    \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{array}\n\\right]x =\n\\left[\n    \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{array}\n\\right]\n\\quad\n\\Leftrightarrow\n\\quad\nU\\Sigma V^T x = b\n\\quad\n\\Leftrightarrow\n\\quad\nx = A^\\dagger b\n\\]\nFor this example \\(\\Sigma = \\|a\\|_2\\), \\(V=1\\), and \\(U=\\tfrac{a}{\\|a\\|_2^2}\\). This is basically just the projection of \\(b\\) along our basis \\(a\\) and this is \\[\nx = \\frac{a^T b}{a^T a}.\n\\]\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nk = 3\nA = np.arange(-2, 2, 0.25).reshape(-1, 1)\nb = k*A + np.random.randn(*A.shape) * 0.5\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(A, k*A, color=\"k\", label=\"Target\")\nplt.plot(A, b, 'x', color=\"r\", label=\"Noisy data\")\nplt.plot(A, A*x, '--', color=\"b\", label=\"Regression line\")\nplt.legend()\nplt.xlabel(\"$a$\")\nplt.ylabel(\"$b$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.5: Linear regression with SVD.\n\n\n\n\n\nOur reconstructed unknown \\(x=\\) 2.855 and is a reasonable good match for \\(k=\\) 3.0\n\n\n5.3.1.2 Multi-Linear Regression (see Brunton and Kutz 2022, 21–23)\nThe second example is based on the Portland Cement Data build in with MATLAB. In Python we again use the dataset provided on GitHub. The data set contains the heat generation during the hardening of 12 cement mixtures comprised of 4 basic ingredients, i.e. \\(A\\in \\mathbb{R}^{13\\times 4}\\). The aim is to determine the weights \\(x\\) that relate the proportion of the ingredients to the heat generation in the mixture.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\nimport requests\nimport io\n%config InlineBackend.figure_formats = ['svg']\n\n# Transform the content of the file into a numpy.ndarray\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_ingredients.csv\")\n# Transform the content of the file into a numpy.ndarray\nA = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_heat.csv\")\nb = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(b, color=\"k\", label=\"Target - Heat data\")\nplt.plot(A@x, '--', color=\"b\", label=\"Regression\")\nplt.legend()\nplt.xlabel(\"mixture\")\nplt.ylabel(\"Heat[cal/g]\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.6: Estimate for hardening in cement mixtures.\n\n\n\n\n\nThis concludes our investigation of matrix decompositions, we will investigate further decompositions of signals later, but for now we dive deeper into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "regression/linear.html",
    "href": "regression/linear.html",
    "title": "6  Linear Regression",
    "section": "",
    "text": "The general idea of linear regression is to approximate a point cloud by a mathematical function, this is often called curve fitting and it is closely related to optimization techniques, (compare Brunton and Kutz 2022, sec. 4.1).\nLet us assume that we want to establish a relationship between our \\(n\\) observations with \\(m\\) independent variables. In order to get the notation down correctly we should describe the variables more closely.\nWe have \\(n\\) observations (the \\(k\\) representation would be \\(y_k\\)) resulting in a vector: \\[\ny = \\left[\n    \\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{array}\n    \\right].\n\\]\nWe have \\(m\\) independent variables and consequently for each of this \\(n\\) observations this results in a matrix \\[X = \\left[\n    \\begin{array}{c} X_{1-} \\\\ X_{2-} \\\\ X_{3-} \\\\ \\vdots \\\\ X_{n-} \\end{array}\n    \\right] = \\left[X_{-1}, \\dots, X_{-m}\\right],\\] where the second representation more closely fits to our idea of the \\(m\\) independent variables.\nLet us model the relation in a linear fashion with the parameters \\(\\beta\\) as \\[\ny_k \\overset{!}{=} X_{k1} \\beta_1 + X_{k2} \\beta_2 + \\cdots + X_{km} \\beta_m, \\quad 1\\leq k \\leq n,\n\\] or in short \\[\ny = X\\beta = f(X, \\beta).\n\\]\nLet us assume for a moment we already have a realisation of \\(f\\), i.e. we know the parameters \\(\\beta\\), we get the error of the approximation as \\[\n\\epsilon = y - f(X, \\beta)\\quad \\Leftrightarrow \\quad \\epsilon_k = y_k - f(X_{k-}, \\beta).\n\\tag{6.1}\\]\nThere are various possibilities to select the metric for minimizing \\(\\epsilon\\) and therefore characterizing the quality of the fit. This is done by selecting the underlying norm. Most commonly we use the \\(1\\)-norm, the \\(2\\)-norm and the \\(\\infty\\)-norm, i.e. \\[\nE_1 (f) = \\frac1n \\sum_{k=1}^n|f(X_{k-}, \\beta) - y_k |,\n\\tag{6.2}\\] for the \\(1\\)-norm or mean absolute error, \\[\nE_2 (f) = \\sqrt{\\frac1n \\sum_{k=1}^n|f(X_{k-}, \\beta) - y_k |^2},\n\\tag{6.3}\\] for the \\(2\\)-norm or least-square error, \\[\nE_\\infty (f) = \\max_{k}|f(X_{k-}, \\beta) - y_k |\n\\tag{6.4}\\] for the \\(\\infty\\)-norm or maximum error. Of course \\(p\\)-norms work as well \\[\nE_p (f) = \\left(\\frac1n \\sum_{k=1}^n|f(X_{k-}, \\beta) - y_k |^p\\right)^{1/p}.\n\\] If we go back and we want to solve Equation 6.1 for \\(\\beta\\) we have different\nDepending on the selected norm we get different realisations. To illustrate this we use the example from (Brunton and Kutz 2022, 136–67).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.optimize\n%config InlineBackend.figure_formats = [\"svg\"]\n\n# Function definitions\ndef fit1(x0, t):\n    x, y = t\n    return np.max(np.abs(x0[0] * x + x0[1] - y))\ndef fit2(x0, t):\n    x, y = t\n    return np.sum(np.abs(x0[0] * x + x0[1] - y))\ndef fit3(x0, t):\n    x, y = t\n    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])\nz = np.array([0.2, 0.5, 0.3, 3.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2])\nt = (x, y)\nt2 = (x, z)\n\nx0 = np.array([1, 1])\n\np1 = scipy.optimize.fmin(fit1, x0, args=(t,), disp=False)\np2 = scipy.optimize.fmin(fit2, x0, args=(t,), disp=False)\np3 = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)\n\np12 = scipy.optimize.fmin(fit1, x0, args=(t2,), disp=False)\np22 = scipy.optimize.fmin(fit2, x0, args=(t2,), disp=False)\np32 = scipy.optimize.fmin(fit3, x0, args=(t2,), disp=False)\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(p1, xf)\ny2 = np.polyval(p2, xf)\ny3 = np.polyval(p3, xf)\n\ny12 = np.polyval(p12, xf)\ny22 = np.polyval(p22, xf)\ny32 = np.polyval(p32, xf)\n\n\nfig = plt.figure()\nax = fig.add_subplot(2, 1, 1)\n\nax.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nax.plot(xf, y1, label=r\"$E_\\infty$\")\nax.plot(xf, y2, \"--\", linewidth=2, label=r\"$E_1$\")\nax.plot(xf, y3, \":\", linewidth=2, label=r\"$E_2$\")\nax.set_ylim(0, 4)\nax.set_xlim(0, 11)\nax.set_title(\"Observations with no outliers.\")\nax.legend()\n\nax = fig.add_subplot(2, 1, 2)\nax.plot(x, z, \"o\", color=\"r\", label=\"observations\")\nax.plot(xf, y12, label=r\"$E_\\infty$\")\nax.plot(xf, y22, \"--\", linewidth=2, label=r\"$E_1$\")\nax.plot(xf, y32, \":\", linewidth=2, label=r\"$E_2$\")\nax.set_ylim(0, 4)\nax.set_xlim(0, 11)\nax.set_title(\"Observations with outliers.\")\n# ax.legend()\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6.1: Line fit with different norms. Top without outliers, bottom with one outlier.\n\n\n\n\n\nIf we use Equation 6.3 it is called the least square fit.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Collection of interesting reads",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#collection-of-interesting-reads",
    "href": "summary.html#collection-of-interesting-reads",
    "title": "Summary",
    "section": "",
    "text": "Semantic Versioning How to design the version of your project.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -.\nSebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed.\nSebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on\nIntroduction. München: No Starch Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in the form of Linear Algebra as well as some topics of statistics of sets. We will always immediately show how to use the discussed content in Python.\n\n\n\n\n\n\nNote\n\n\n\nThese notes assume that you have some basic knowledge of programming in Python and we build on this knowledge. In this sense, we use Python as a tool and only describe the inner workings if it helps us to better understand the topics at hand.\nIf this is not the case have a look at MECH-M-DUAL-1-SWD, a class on software design in the same master program and from the same authors.\nAdditionally, we can recommend the following books on Python:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming; Online Material.\nPython Cheat Sheet provided by Matthes (2023).\nMcKinney (2022): Python for data analysis 3e; Online and Print\nVasiliev (2022): Python for Data Science - A Hands-On Introduction\nInden (2023): Python lernen – kurz & gut; German\n\n\n\n\n\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -. Sebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed. Sebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on Introduction. München: No Starch Press.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "matrixdc/index.html",
    "href": "matrixdc/index.html",
    "title": "Matrix decompositions",
    "section": "",
    "text": "There are a lot of different matrix decompositions and they can be used to fulfil several tasks. We are going to look into the eigendecomposition as well as the singular value decomposition. Both of these can, for example, be used for picture compression and recognition.\nFor notation we are following again Golub and Van Loan (2013), which has plenty more to offer than we cover in these notes.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regression analysis",
    "section": "",
    "text": "In general, regression analysis can be understood as a set of tools that is used to estimate or establish a relationship between a dependent variable \\(Y\\) (also called outcome or response variable, label) and the independent variable \\(X\\) (also called regressor, predictors, covariates, explanatory variable or feature). If we add a regression function \\(f\\) and some unknown parameters \\(\\beta\\) to the mix the problem can be written mathematically as \\[\nY = f(X, \\beta)\n\\] {#qe-curvefitting} where \\(\\beta\\) is found by optimizing for a good fit of \\(f\\) to the data.\nWe split up the discussion along the well known topics:\n\nlinear regression\nnon-linear regression\nmultilinear regression\ngradient decent\nover and under fitting\n\nParts of this section are based on (Brunton and Kutz 2022, sec. 4).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis"
    ]
  }
]