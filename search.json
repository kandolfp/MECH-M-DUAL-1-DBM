[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Warning\n\n\n\nThe notes presented here are still under construction and can change without warning.\n\n\n\nPreface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.\n\n\nAcknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger and Matthias Panny for testing, checking, suggestions and general proofreading.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basics behind modern day Data Science. We will always try to not only discuss the theory but also use Python to illustrate it and work on it programmatically.\nFor the class we will orient ourself on the first couple of chapters of Brunton and Kutz (2022), where we will highlight sections with lose similarity.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R},\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n.\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\nC = A^{\\mathsf{T}}, \\quad \\Rightarrow \\quad c_{ij} = a_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define two more mappings.\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor a \\({m \\times p}\\) matrix \\(A\\) and a \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition 1.1 (Vector space) For a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following properties need to hold true to form a vector space.\nFor the vector addition we need to have\n\nassociativity \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity \\[u + v = v + u,\\]\nthere needs to exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector such that \\[v + 0 =  v,\\]\nthere needs to exist an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] and this element is usually denoted by \\(-v\\).\n\nFor the scalar multiplication we need to have\n\nassociativity \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\ndistributivity with respect to the vector addition \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v,\\]\nand there needs to exist a multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v.\\]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n Exercise - Vector space in Python\n\n\n\n\n\nCreate some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix as vector space?\n\n\n\n\n\nHow do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Linear map) A linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a field \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w),\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Dot product) The dot product, inner product, or scalar product of two vectors \\(v\\) and \\(w\\) as is defined by \\[\\langle v, w\\rangle = v \\cdot w = v^{\\mathsf{T}} w = \\sum_i v_i w_i.\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Outer product) We also have the outer product defined as: \\[\nv w^{\\mathsf{T}} = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix multiplication?\n\n\n\n\n\nShow that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\n\nDefinition 1.5 (Norm) A norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac{1}{p}},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAnd for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |a_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{\\sum_i \\sum_j |a_{ij}|^2},\\]\nthe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(\\sum_i \\sum_j |a_{ij}|^p\\right)^{\\frac1p},\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |a_{ij}|.\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "href": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\n\nDefinition 1.6 (Basis) A set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\}, b_i \\in \\mathbb{R}^n\\):\n\nis called linear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}.\\]\n\nThe set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\nFor a shorter notation we often associate the matrix \\[\nB = \\left[b_1 | \\cdots | b_n\\right]\n\\] with the basis.\n\n\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\) and its associated matrix is \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & \\dots & 0\\\\\n                        0 & 1 & \\ddots & \\vdots \\\\  \n                        \\vdots & \\ddots & 1 & 0\\\\\n                        0 & \\dots & 0 & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n},\n\\] and called the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix.\nThe easiest way to create one of standard basis vectors, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\nand the identity matrix by\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n Example - Standard basis\n\n\n\n\n\n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\n# compute the coefficients\na = np.dot(x, e_1) / np.dot(e_1, e_1)\nb = np.dot(x, e_2) / np.dot(e_2, e_2)\nc = np.dot(x, e_3) / np.dot(e_3, e_3)\ny = a * e_1 + b * e_2 + c * e_3\nprint(f\"{y=}\")\nprint(f\"{np.allclose(x, y)=}\")\nprint(f\"{LA.norm(x-y)=}\")\n\nx=array([0.39547419, 0.08215934, 0.65898437])\ny=array([0.39547419, 0.08215934, 0.65898437])\nnp.allclose(x, y)=True\nLA.norm(x-y)=np.float64(0.0)\n\n\nSee numpy.testing for more ways of testing in numpy.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\n\nDefinition 1.7 (Matrix inverse) For matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\n\n\nThe following holds true for the inverse of matrices:\n\nthe inverse of a product is the product of the inverses \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^{\\mathsf{T}} = (A^{mathsf{T}})^{-1} \\equiv A^{-mathsf{T}}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nprint(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n# Note that the equality is hard to achieve for floats\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\nA=array([[0.53812641, 0.80834419, 0.54236917],\n       [0.60734177, 0.46851799, 0.57865028],\n       [0.87267671, 0.18204671, 0.77005635]])\nX=array([[ 43.94203332, -90.09391785,  36.75064406],\n       [  6.41423529, -10.136462  ,   3.09923179],\n       [-51.31427028, 104.49647447, -41.0822354 ]])\nA @ X=array([[ 1.00000000e+00,  4.34551385e-15, -4.39727176e-15],\n       [ 2.65012915e-15,  1.00000000e+00, -1.73718639e-15],\n       [-2.36940866e-15,  8.58353835e-15,  1.00000000e+00]])\nnp.allclose(A @ X, np.eye(A.shape[0]))=True\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 8\n      6 print(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n      7 # Note that the equality is hard to achieve for floats\n----&gt; 8 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.7/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 9 / 9 (100%)\nMax absolute difference among violations: 8.58353835e-15\nMax relative difference among violations: 3.33066907e-15\n ACTUAL: array([[ 1.000000e+00,  4.345514e-15, -4.397272e-15],\n       [ 2.650129e-15,  1.000000e+00, -1.737186e-15],\n       [-2.369409e-15,  8.583538e-15,  1.000000e+00]])\n DESIRED: array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n\n\n\n\n\n\nDefinition 1.8 (Change of basis) If we associate the matrices \\(B\\) and \\(C\\) with the matrix consisting of the basis vectors of two bases \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) of a vector space we can define the transformation matrix \\(T_{\\mathcal{C}}^{\\mathcal{B}}\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) as \\[\nT_{\\mathcal{C}}^{\\mathcal{B}} = C^{-1}B.\n\\]\nSo if we have a vector \\(b\\) represented in \\(\\mathcal{B}\\) we can compute its representation in \\(\\hat{b}\\) in \\(\\mathcal{C}\\) as \\[\n\\hat{b} = T_{\\mathcal{C}}^{\\mathcal{B}} b = C^{-1}B b.\n\\]\nA special form is if we have the standard basis \\(I\\) and move to a basis \\(C\\) we get \\[\n\\hat{b} = T_{C}^{I} b = C^{-1} b.\n\\]\n\n\n\n\n\n\n\n\n\n\n Example - basis change\n\n\n\n\n\nFor \\[\nB = \\left[\n    \\begin{array}{ccc} 1 & 3 & 2 \\\\\n                       0 & 1 & 1 \\\\  \n                       2 & 0 & 1 \\\\  \\end{array}\n\\right]\\quad \\text{and}\\quad\nC = \\left[\n    \\begin{array}{ccc} 1 & 0 & 1 \\\\\n                       0 & 1 & 1 \\\\  \n                       1 & 1 & 0 \\\\  \\end{array}\n\\right]\n\\] we get \\[\nT_{C}^{B} = \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right],\n\\] and for a \\(v = 2 b_1 - b_2 + 3 b_3\\) we can compute its representation in \\(C\\) as \\[\n\\hat{v} = T_{C}^{B} v\n= \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} 5 \\\\ 2 \\\\ 0 \\end{array}\n\\right],\n\\] and therefore, \\(v = 5c_1 + 2c_2 + 0 c_3\\).\n(Compare Wikipedia)\n\n\n\nThere are special basis vectors, respectively matrices that allow for easy computation of the inverse.\n\n\n\n\n\n\n\nDefinition 1.9 (Orthonormal vector) We call a set of vectors \\(\\mathcal{V}=\\{u_1, u_2, \\ldots, u_m\\}\\) orthonormal if and only if \\[\n\\forall i,j: \\langle u_i, u_j \\rangle = \\delta_{ij}\n\\] where \\(\\delta_{ij}\\) is called the Kronecker delta which is \\(1\\) if and only if \\(i=j\\) and \\(0\\) otherwise. This is true for a inner product, see Definition 1.3.\n\n\n\n\nExtending this to a matrix (and to that end a basis) as follows.\n\n\n\n\n\n\n\nDefinition 1.10 (Orthogonal matrix) We call a matrix \\(Q\\in\\mathbb{R}^{n\\times n}\\), here the real and square is important, orthogonal if its columns and rows are orthonormal vectors. This is the same as \\[\nQ^{\\mathsf{T}} Q = Q Q^{\\mathsf{T}} = I\n\\] and this implies that \\(Q^{-1} = Q^{\\mathsf{T}}\\).\n\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/sets.html",
    "href": "basics/sets.html",
    "title": "2  Data sets",
    "section": "",
    "text": "2.1 Basic properties of a data set\nFirst we are looking at the total net rent, i.e. the row nm.\nFor a vector \\(v \\in \\mathbb{R}^n\\) we have:\nnm_max = np.max(data['nm'])\nprint(f\"{nm_max=}\")\n\nnm_min = np.min(data['nm'])\nprint(f\"{nm_min=}\")\n\nnm_mean = np.mean(data['nm'])\n# round to 2 digits\nnm_mean_r = np.around(nm_mean, 2)\nprint(f\"{nm_mean_r=}\")\n\nnm_median = np.median(data['nm'])\nprint(f\"{nm_median=}\")\n\nnm_quartiles = np.quantile(data['nm'], [1/4, 1/2, 3/4])\nprint(f\"{nm_quartiles=}\")\n\nnm_max=np.float64(1789.55)\nnm_min=np.float64(77.31)\nnm_mean_r=np.float64(570.09)\nnm_median=np.float64(534.3)\nnm_quartiles=array([389.95, 534.3 , 700.48])\nFrom this Python snippet we know that for tenants the rent varied between 77.31 and 1789.55, with an average of 570.09 and a median of 534.3. Of course there are tricky questions that require us to dig a bit deeper into these functions, e.g. how many rooms does the most expensive flat have? The surprising answer is 3 and it was built in 1994, but how do we obtain these results?\nWe can use numpy.argwhere or a function which returns the index directly like numpy.argmax.\nmax_index = np.argmax(data['nm'])\nrooms = int(data['rooms'][max_index])\nyear = int(data['bj'][max_index])\nprint(f\"{rooms=}, {year=}\")\n\nrooms=3, year=1994",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#basic-properties-of-a-data-set",
    "href": "basics/sets.html#basic-properties-of-a-data-set",
    "title": "2  Data sets",
    "section": "",
    "text": "the maximal value, i.e. the maximum \\[\nv^{max} = \\max_i v_i,\n\\]\nthe minimal value, i.e. the minimum \\[\nv^{min} = \\min_i v_i,\n\\]\nthe mean of all values (often called the arithmetic mean) \\[\n\\overline{v} = \\frac1n \\sum_{i=1}^n v_i = \\frac{v_1 + v_2 + \\cdots + v_n}{n},\n\\]\nthe median, i.e. the value where half of all the other values are bigger and the other half is smaller, for a sorted \\(v\\) this is \\[\n\\widetilde{v} = \\begin{cases}\n              v_{(n+1)/2}& n\\quad \\text{odd}\\\\\n              \\frac{v_{n/2} + v_{n/2+1}}{2}& n\\quad \\text{even}\n              \\end{cases},\n\\]\nmore general, we have quantiles. For a sorted \\(v\\) and \\(p\\in(0,1)\\) \\[\n\\overline{v}_p = \\begin{cases}\n               \\frac12\\left(v_{np} + v_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n              v_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n              \\end{cases}.\n\\] Some quantiles have special names, like the median for \\(p=0.5\\), the lower and upper quartile for \\(p=0.25\\) and \\(p=0.75\\) (or first, second (median) and third quartile), respectively.\n\n\n\n\n\n\n2.1.1 Visualization\n\n\n\n\n\n\nTip\n\n\n\nThere are various ways of visualizing data in Python. Two widely used packages are matplotlib and plotly.\n\n\nIt often helps to visualize the values to see differences and get an idea of their use.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nnm_sort = np.sort(data[\"nm\"])\nx = np.linspace(0, 1, len(nm_sort), endpoint=True,)\n\nplt.plot(x, nm_sort, label=\"net rent\")\nplt.axis((0, 1, np.round(nm_min/100)*100, np.round(nm_max/100)*100))\nplt.xlabel('Scaled index')\nplt.ylabel('Net rent - nm')\n\nplt.plot([0, 0.25, 0.25], [nm_quartiles[0], nm_quartiles[0], nm_min], \n         label='1st quartile')\nplt.plot([0, 0.5, 0.5], [nm_quartiles[1], nm_quartiles[1], nm_min],\n         label='2st quartile')\nplt.plot([0, 0.75, 0.75], [nm_quartiles[2], nm_quartiles[2], nm_min],\n         label='3st quartile')\nplt.plot([0, 1], [nm_mean, nm_mean],\n         label='mean')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the different measurements.\n\n\n\n\n\nWhat is shown in Figure 2.1 is often combined into a single boxplot (see Figure 2.2) that provides way more information at once.\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"Standard\"))\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"With points\", boxpoints=\"all\"))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.2: Boxplot done in plotly with whiskers following 3/2 IQR.\n\n\n\n\nThe plot contains the box which is defined by the 1st quantile \\(Q_1\\) and the 3rd quantile \\(Q_3\\), with the median as line in between these two. Furthermore, we can see the whiskers which help us identify so called outliers. By default they are defined as \\(\\pm 1.5(Q_3 - Q_1)\\), where (\\(Q_3 - Q_1\\)) is often called the interquartile range (IQR).\n\n\n\n\n\n\nNote\n\n\n\nFigure 2.2 is an interactive plot in the html version.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#spread",
    "href": "basics/sets.html#spread",
    "title": "2  Data sets",
    "section": "2.2 Spread",
    "text": "2.2 Spread\nThe spread (or dispersion, variability, scatter) are measures used in statistics to classify how data is distributed. Common examples are variance, standard deviation, and the interquartile range that we have already seen above.\n\n\n\n\n\n\n\nDefinition 2.1 (Variance) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the variance is defined as \\[\n\\operatorname{Var}(v) = \\frac1n \\sum_{i=1}^n (v_i - \\mu)^2, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}\n\\] or directly \\[\n\\operatorname{Var}(v) = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j&gt;i} (v_i - v_j)^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Standard deviation) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\frac1n \\sum_{i=1}^n (v_i - \\mu)^2}, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] If we interpret \\(v\\) as a sample this is often also called uncorrected sample standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Interquartile range (IQR)) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the interquartile range is defined as the difference of the first and third quartile, i.e. \\[\nIQR = \\overline{v}_{0.75} - \\overline{v}_{0.25}.\n\\]\n\n\n\n\nWith numpy they are computed as follows\n\nnm_var = np.var(data[\"nm\"])\nprint(f\"{nm_var=}\")\n\nnm_std = np.std(data[\"nm\"])\nprint(f\"{nm_std=}\")\n\nnm_IQR = nm_quartiles[2] - nm_quartiles[0]\nprint(f\"{nm_IQR=}\")\n\nnm_var=np.float64(60208.75551600402)\nnm_std=np.float64(245.37472468859548)\nnm_IQR=np.float64(310.53000000000003)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#histogram",
    "href": "basics/sets.html#histogram",
    "title": "2  Data sets",
    "section": "2.3 Histogram",
    "text": "2.3 Histogram\nWhen exploring data it is also quite useful to draw histograms. For the net rent this makes not much sense but for rooms this is useful.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['rooms'])\nplt.xlabel('rooms')\nplt.ylabel('# of rooms')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of the number of rooms in our dataset.\n\n\n\n\n\nWhat we see in Figure 2.3 is simply the amount of occurrences of \\(1\\) to \\(6\\) in our dataset. Already we can see something rather interesting, there are flats with \\(5.5\\) rooms in our dataset.\nAnother helpful histogram is Figure 2.4 showing the amount of buildings built per year.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['bj'])\nplt.xlabel('year of building')\nplt.ylabel('# of buildings')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Histogram of buildings built per year.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#correlation",
    "href": "basics/sets.html#correlation",
    "title": "2  Data sets",
    "section": "2.4 Correlation",
    "text": "2.4 Correlation\nIn statistics, the terms correlation or dependence describe any statistical relationship between bivariate data (data that is paired) or random variables.\nFor our dataset we can, for example, check:\n\nthe living area in \\(m^2\\) - wfl vs. the net rent - nm\nthe year of construction - bj vs. if central heating - zh0 is available\nthe year of construction - bj vs. the city district - bez\n\n\n\nShow the code for the figure\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=data[\"wfl\"], y=data[\"nm\"], mode=\"markers\"), row=1, col=1)\nfig.update_xaxes(title_text=\"living area in m^2\", row=1, col=1)\nfig.update_yaxes(title_text=\"net rent\", row=1, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"zh0\"], mode=\"markers\"), row=2, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=2, col=1)\nfig.update_yaxes(title_text=\"central heating\", row=2, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"bez\"], mode=\"markers\"), row=3, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=3, col=1)\nfig.update_yaxes(title_text=\"city district\", row=3, col=1)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.5: Scatterplot to investigate correlations in the data set.\n\n\n\n\nIn the first plot of Figure 2.5 we see that the rent tends to go up with the size of the flat but there are for sure some rather cheap options in terms of space.\nThe second plot of Figure 2.5 tells us that central heating became a constant around \\(1966\\). Of course we can also guess that the older buildings with central heating were renovated, but we have no data to support this claim.\nThe third plot of Figure 2.5 does not yield an immediate correlation.\nMore formally, we can describe possible correlations using the covariance. The covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nDefinition 2.4 (Covariance) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the covariance is defined as \\[\n\\operatorname{cov}(v, w) = \\frac1n \\langle v -\\overline{v}, w - \\overline{w}\\rangle.\n\\]\n\n\n\n\nThe covariance is tricky to interpret, e.g. the unities of the two must not make sense. In the example below, we have rent per square meter, which makes some sense.\nFrom the covariance we can compute the correlation.\n\n\n\n\n\n\n\nDefinition 2.5 (Correlation) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the correlation is defined as \\[\n\\rho_{v,w} = \\operatorname{corr}(v, w) = \\frac{\\operatorname{cov}(v, w)}{\\sigma_v \\sigma_w},\n\\] where \\(\\sigma_v\\) and \\(\\sigma_w\\) are the standard deviation of these vectors, see Definition 2.2.\n\n\n\n\nIn numpy the function numpy.cov computes a matrix where the diagonal is the variance of the values and the off-diagonals are the covariances of the \\(i\\) and \\(j\\) samples. Consequently, numpy.corrcoef is a matrix as well.\n\ncov_nm_wfl = np.cov(data[\"nm\"], data[\"wfl\"])\nprint(f\"{cov_nm_wfl[0, 1]=}\")\n\ncorr_nm_wfl = np.corrcoef(data[\"nm\"], data[\"wfl\"])\nprint(f\"{corr_nm_wfl[0, 1]=}\")\n\ncov_nm_wfl[0, 1]=np.float64(4369.1195844122)\ncorr_nm_wfl[0, 1]=np.float64(0.7074626685750687)\n\n\nThe above results, particularly \\(\\rho_{\\text{nm},\\text{wfl}}=0.707\\) suggest that the higher the rent, the more space you get.\n\n\n\n\n\n\nTip\n\n\n\nCorrelation and causation are not the same thing!\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe showed some basic tests for correlation, there are more elaborate methods but they are subject to a later chapter.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/epilogue.html",
    "href": "basics/epilogue.html",
    "title": "3  Epilogue",
    "section": "",
    "text": "This sums up our basic introduction. We introduced the basic mathematical constructs to use in further sections and learned how to work with them in Python.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Epilogue</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html",
    "href": "matrixdc/eigen.html",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1 Examples for the application\nTo get a better idea what the eigendecomposition can do we look into some examples.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#examples-for-the-application",
    "href": "matrixdc/eigen.html#examples-for-the-application",
    "title": "4  Eigendecomposition",
    "section": "",
    "text": "4.1.1 Solving system of linear equations\nFor a system of linear equations \\(Ax=b\\) we get \\[\n\\begin{array}{lll}\nA x & = b  & \\iff \\\\\nV \\Lambda V^{-1} x & = b  & \\iff \\\\\n\\Lambda V^{-1} x & = V^{-1} b  & \\iff \\\\\nV^{-1} x & = \\Lambda^{-1}V^{-1} b  & \\iff \\\\\nx & = V\\Lambda^{-1}V^{-1} b  & \\iff\n\\end{array}\n\\] As \\(\\Lambda^{-1} = \\operatorname{diag}\\left(\\lambda_1^{-1}, \\ldots, \\lambda_n^{-1}\\right)\\) this is easy to compute once we have the eigenvalue decomposition.\n\n\n\n\n\n\nNote\n\n\n\nThe computation of the eigendecomposition is not cheap, therefore this is not always worth the effort and there are other ways of solving linear systems.\n\n\n\n\n4.1.2 Linear Ordinary Differential Equations\nIn this example we use the eigendecomposition to efficiently solve a system of differential equations \\[\n\\dot{x} = A x,\\quad x(0) = x_0\n\\] By changing to the basis \\(V\\) and using the notation \\(\\hat{x}=z\\) we have the equivalent formulations \\[\nz = V^{-1}x \\iff x = Vz,\n\\] and if follows \\[\n\\begin{array}{lll}\n\\dot{x} = A x & \\iff V \\dot{z} &= A V z \\\\\n              & \\iff \\dot{z} &= V^{-1} A V z \\\\\n              & \\iff \\dot{z} &= \\Lambda z\n\\end{array}.\n\\] So for an initial value \\(z_0\\) the solution in \\(t\\) is \\[\nz(t) = \\operatorname{diag}\\left(e^{t\\lambda_1}, \\ldots, e^{t\\lambda_n}\\right) z_0.\n\\]\nWe often say that it is now a decoupled differential equation.\n\n\n4.1.3 Higher Order Linear Differential Equations\nIf we have a higher order linear ODE such as \\[\nx^{(n)} + a_{n-1} x^{(n-1)} + \\cdots + a_2 \\ddot{x} + a_1 \\dot{x} + a_0 x = 0.\n\\tag{4.2}\\] we can stack the derivatives into a vector \\[\n\\begin{array}{ccc}\nx_1 & = & x\\\\\nx_2 & = & \\dot{x}\\\\\nx_3 & = & \\ddot{x}\\\\\n\\vdots & = & \\vdots \\\\\nx_{n-1} & = & x^{(n-2)} \\\\\nx_{n} & = & x^{(n-1)} \\\\\n\\end{array}\n\\quad\n\\Leftrightarrow\n\\quad\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} x \\\\ \\dot{x} \\\\ \\ddot{x} \\\\ \\vdots \\\\ x^{(n-2)} \\\\ x^{(n-1)} \\end{array}\n\\right],\n\\] and taking the derivative of this vector yields the following system \\[\n\\underbrace{\n\\frac{d}{d t}\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_{\\dot{x}}\n=\n\\underbrace{\\left[\n    \\begin{array}{cccccc} 0 & 1 & 0 & \\dots & 0 & 0\\\\\n                          0 & 0 & 1 & \\dots & 0 & 0\\\\  \n                          0 & 0 & 0 & \\dots & 0 & 0\\\\\n                          \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n                          0 & 0 & 0 & \\dots & 0 & 1\\\\\n                          -a_0 & -a_1 & -a_2 & \\dots & -a_{n-2} & -a_{n-1}\\\\\n    \\end{array}\n\\right]\n}_{A}\n\\underbrace{\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_x.\n\\]\nWe transformed it into a system of coupled 1st order ODEs \\(\\dot{x}=Ax\\) and we can solve this as seen above. More importantly, the characteristic polynomial of Equation 4.2 is equal to the characteristic polynomial of Definition 4.2 and the eigenvalues are the roots of this polynomial.\n\n\n4.1.4 Generalized eigenvalue problem\nLet us motivate this by the example of modal analysis. If we consider the free vibrations of a undamped system we get the equation \\[\nM\\ddot{u} + K u = 0, \\quad u(0) = u_0,\n\\] with the mass matrix \\(M\\) and the stiffness matrix \\(K\\) and \\(u(t)\\) being the displacement. As we know, the solution of this linear differential equation has the form \\(u(t)=e^{i\\omega t}u_0\\) and thus we get \\[\n(-\\omega^2 M + K) u_0 = 0.\n\\tag{4.3}\\]\n\n\n\n\n\n\n\nDefinition 4.5 (Generalized eigenvalue problem) If \\(A, B \\in \\mathbb{C}^{n\\times n}\\), then the set of all matrices of the form \\(A-\\lambda B\\) with \\(\\lambda\\in\\mathbb{C}\\) is a pencil. The generalized eigenvalues of \\(A-\\lambda B\\) are elements of the set \\(\\lambda(A,B)\\) defined by \\[\n\\lambda(A,B) = \\{z\\in\\mathbb{C}: \\det(A-zB)=0\\}.\n\\] If \\(\\lambda \\in \\lambda(A,B)\\) and \\(0\\neq v\\in\\mathbb{C}^n\\) satisfies \\[\nA v = \\lambda B v,\n\\tag{4.4}\\] then \\(v\\) is an eigenvector of \\(A-\\lambda B\\). The problem of finding a nontrivial solution to Equation 4.4 is called the generalized eigenvalue problem.\n(Compare Golub and Van Loan 2013, chap. 7.7)\n\n\n\n\nIn our example Equation 4.3 the eigenvalues are \\(\\lambda = \\omega^2\\) and correspond to the square of the natural frequencies and the eigenvectors \\(v=u\\) correspond to the modes of vibration.\nIf \\(M\\) is invertible we can write \\[\nM^{-1}(K -\\omega^2 M ) u_0 = (M^{-1}K -\\omega^2 I ) u_0 = 0.\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases inverting the matrix is not recommended, to directly solve the generalized eigenvalue problem, see (Golub and Van Loan 2013, chap. 7.7) for details.\n\n\nLet us walk through this with an example from (Downey and Micheli 2024, sec. 8.3).\n\n\n\n\n\n\n\nExample 4.1 (Two Story Building)  \n\n\n\n\n\n\nFigure 4.1: Two story frame where the floors have different dynamic properties.\n\n\n\nIn Figure 4.1 we can see a two story building consisting of a two column frame with a floor. The first floor columns are fixed at the base and have a height \\(h\\), the second floor is fixed to the first and has the same height. The columns of each frame are modelled as beams with flexural rigidity \\(EI\\) and \\(2 EI\\), respectively Where \\(E\\) is called Young’s modulus and \\(I\\) the second moment of area. The mass is centred at the floor level.\nThis allows us to model such a building as a system with two degrees of freedom \\(x_1\\) and \\(x_2\\) as the displacement indicated in blue. The dashed blue line would be such a displacement for the frame.\nThe resulting equations of motion become \\[\n\\begin{array}{r}\nm_1 \\ddot{x_1} + k_1 x_1 + k_2 (x_2 - x_1) = 0,\\\\\nm_2 \\ddot{x_1} + k_2 (x_2 - x_1) = 0,\n\\end{array}\n\\] resulting in the matrices \\[\nM = \\left[\n\\begin{array}{cc}\nm_1 & 0 \\\\\n0 & m_2\n\\end{array}\n\\right], \\quad\nK = \\left[\\begin{array}{cc}\nk_1 + k_2 & -k_2 \\\\\n-k_2 & k_2\n\\end{array}\n\\right].\n\\]\nFor the derivation of the stiffness coefficients we refer to (Downey and Micheli 2024, 188) and recall the result here as follows \\[\nk_1 = \\frac{48 EI}{h^3}, \\quad k_2 = \\frac{24 EI}{h^3}.\n\\] This results in the stiffness matrix \\[\nK = \\underbrace{\\frac{24 EI}{h^3}}_{=k}\n\\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right].\n\\]\nNow we can manually compute \\[\n\\det(-\\omega^2 M + K) = 0 \\Leftrightarrow 2m^2\\omega^4 - 5 m k \\omega^2 + 2 k^2 = 0,\n\\] and solving this equation for \\(\\omega^2\\) results in \\[\n\\omega_1^2 = \\frac{k}{2m}, \\quad \\omega_2^2 = \\frac{2k}{m}.\n\\]\nTo compute the eigenvectors \\(v_1\\) and \\(v_2\\) we use Equation 4.1, i.e. for \\(v_1\\) this reads as \\[\n-\\frac{k}{2m} \\left[\n\\begin{array}{cc}\n2m & 0 \\\\\n0 & m\n\\end{array}\n\\right]\n+\nk \\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n=\n\\left[\\begin{array}{cc}\n2 k & -k \\\\\n-k & \\frac{k}{2}\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n\\overset{!}{=}\n\\left[\\begin{array}{c}\n0\\\\\n0\n\\end{array}\n\\right],\n\\] and results in the relation \\(2v_{11} = v_{21}\\). We can select a solution as \\[\nv_1 = \\left[\\begin{array}{c}\n\\frac12\\\\\n1\n\\end{array}\n\\right].\n\\] For \\(v_2\\) we proceed similarly and derive a solution as: \\[\nv_2 = \\left[\\begin{array}{c}\n-1\\\\\n1\n\\end{array}\n\\right].\n\\]\nThe eigenvectors illustrate how the displacement functions and are not just some theoretical value. The following figure visualizes the two modes.\n\n\n\n\n\n\n\n\n\n\n\n(a) First mode\n\n\n\n\n\n\n\n\n\n\n\n(b) Second mode\n\n\n\n\n\n\n\nFigure 4.2: Model of a two story building with the shape of the modes according to the modal analysis.\n\n\n\nTwo wrap up the example our overall temporal response consists of the time invariant part defined by our eigenvectors \\(v_1\\), and \\(v_2\\), as well as the time dependent part with our eigenfrequencies \\(\\omega_1\\) and \\(\\omega_2\\) as well as the constants \\(A_1\\), \\(A_2\\), \\(\\phi_1\\), \\(\\phi_2\\) depending on the initial condition (see Downey and Micheli 2024, chap. 5). \\[\n\\left[\\begin{array}{c}\nx_1(t)\\\\\nx_2(t)\n\\end{array}\n\\right]\n=\n\\left[\nv_1, v_2\n\\right]\n\\left[\\begin{array}{c}\nA_1 \\sin(\\omega_1 t + \\phi_1)\\\\\nA_2 \\sin(\\omega_2 t + \\phi_2)\n\\end{array}\n\\right]\n\\]\n(Compare Downey and Micheli 2024, chap. 8.3, pp. 189-191)\n\n\n\n\n\n\n4.1.5 Low-rank approximation of a square matrix\nWe can use the eigenvalue decomposition to approximate a square matrix.\nLet us sort the eigenvalues in \\(\\Lambda\\) and let us call \\(U^{\\mathsf{T}}=V^{-1}\\) then we can write \\[\nA = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^n\\lambda_i v_i u_i^{\\mathsf{T}}\n\\] where \\(v_i\\) and \\(u_i\\) correspond to the rows of the matrices. Now we can define the rank \\(r\\) approximation of \\(A\\) as \\[\nA\\approx A_r = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^r\\lambda_i v_i u_i^{\\mathsf{T}}.\n\\]\nTo make this a bit easier to understand the following illustration is helpful:\n\n\n\nLow Rank Approximation\n\n\nThis approximation can be used to reduce the storage demand of an image.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_cut = im_gray[1500:3001,1500:3001] / 255\n\nlam_, V_ = LA.eig(im_cut)\norder = np.argsort(np.abs(lam_))\nlam = lam_[order[::-1]]\nV = V_[:, order[::-1]]\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\nVinv = LA.inv(V)\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3,1,1)\nax_eig.plot((np.abs(lam)))\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\lambda_i|$\")\n#ax_eig.set_xlabel(\"$i$\")\nax_eig.set_title(\"absolute value of the eigenvalues\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(lam) * p))\n    A_r = np.real(V[:, 0:r] @ np.diag(lam[0:r], 0) @ Vinv[0:r, :])\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_cut, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.3: Image of MCI I and the reconstruction with approximated matrix.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#summary",
    "href": "matrixdc/eigen.html#summary",
    "title": "4  Eigendecomposition",
    "section": "4.2 Summary",
    "text": "4.2 Summary\nThe eigenvalue decomposition is an important tool but it has its limitations:\n\nthe matrices involved need to be square\neigenvalues might be complex, even if the problem at hand is real\nwe only get a diagonal matrix \\(\\Lambda\\) if all eigenvectors are linear independent\nthe computation of \\(V^{-1}\\) is non-trivial unless \\(A\\) is symmetric and \\(V\\) becomes unitary (\\(V^{-1} = V^{\\mathsf{T}}\\)).\n\nTherefore, we will look into a generalized decomposition called the singular value decomposition in the next section.\n\n\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A Practical Introduction for Mechanical, Civil, and Aerospace Engineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html",
    "href": "matrixdc/svd.html",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "5.1 Low rank approximation\nAgain, we can cut of the reconstruction at a certain point and create an approximation. More formally this is defined in the next definition.\nWe can use this for image compression.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_scale = im_gray[1500:3001, 1500:3001] / 255\n\nU, s, Vh = LA.svd(im_scale, full_matrices=False)\n\nrec = [1/1000, 10/100, 25/100, 50/100, 1]\n\nfig = plt.figure()\nax_eig = fig.add_subplot(3, 1, 1)\nax_eig.plot(s)\nax_eig.set_yscale(\"log\")\nax_eig.set_ylabel(r\"$|\\sigma_i|$\")\nax_eig.set_title(\"singular value\")\nax_eig.set_aspect(\"auto\", \"box\")\n\naxs = [] \naxs.append(fig.add_subplot(3, 3, 9))\naxs.append(fig.add_subplot(3, 3, 8))\naxs.append(fig.add_subplot(3, 3, 7))\naxs.append(fig.add_subplot(3, 3, 6))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 4))\n\nfor i, p in enumerate(rec):\n    r = int(np.ceil(len(s) * p))\n    A_r = U[:, :r] @ np.diag(s[:r]) @ Vh[:r, :]\n    axs[i].imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r - 1\n    axs[i].set_title(f\"${r=}$\")\n\naxs[5].imshow(im_scale, cmap=plt.get_cmap(\"gray\"))\naxs[5].set_axis_off()\naxs[5].set_title(f\"Original image\")\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.1: Image of MCI I and the reconstruction with reduced rank matrices.\nAs the matrices \\(U\\) and \\(V\\) are orthogonal, they also define a basis of the corresponding (sub) vector spaces. As mentioned before, the SVD automatically selects these and they are optimal.\nConsequently, the matrices \\(U\\) and \\(V\\) can be understood as reflecting patterns in the image. We can think of the columns of \\(U\\) and \\(V\\) as the vertical respectively horizontal patterns of \\(A\\).\nWe can illustrate this by looking at the modes of our decomposition \\[\nM_k = U(:, k) V^{\\mathsf{T}}(k, :).\n\\]\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nrec = [0, 1, 2, 3, 4, 5]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(3, 3, 1))\naxs.append(fig.add_subplot(3, 3, 2))\naxs.append(fig.add_subplot(3, 3, 3))\naxs.append(fig.add_subplot(3, 3, 4))\naxs.append(fig.add_subplot(3, 3, 5))\naxs.append(fig.add_subplot(3, 3, 6))\n\nfor i, r in enumerate(rec):\n    M_r = np.outer(U[:, r], Vh[r, :])\n    axs[i].imshow(M_r, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    r = r + 1\n    axs[i].set_title(f\"${r=}$\")\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.35, wspace=0.001)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Modes of the SVD decomposition of the MCI I image.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#low-rank-approximation",
    "href": "matrixdc/svd.html#low-rank-approximation",
    "title": "5  Singular Value Decomposition",
    "section": "",
    "text": "Definition 5.2 (Low-Rank Approximation) If \\(A \\in \\mathbb{R}^{m\\times n}\\) and has the SVD \\(A = U\\Sigma V^{\\mathsf{T}}\\) than \\[\nA_k = U(:, 1:k)\\, \\Sigma(1:k, 1:k)\\, V^{\\mathsf{T}}(1:k, :)\n\\] is the optimal low-rank approximation of \\(A\\) with rank \\(k\\). This is often called the truncated SVD.\n(See Golub and Van Loan 2013, Corollary 2.4.7 p. 79)\n\n\n\n\n\n\n\nTruncated SVD\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we compare this to Figure 4.3 we can see that we get a much better result for smaller \\(r\\). Let us have a look why.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe big advantage here is, that the selection is optimal. A disadvantage is that the need to store the basis separately and this increases the necessary storage. We will see in later sections about wavelets and Fourier decomposition how a common basis can be used to reduce the storage by still keeping good reconstructive properties.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#sec-matrixdc-pca",
    "href": "matrixdc/svd.html#sec-matrixdc-pca",
    "title": "5  Singular Value Decomposition",
    "section": "5.2 Principal Component Analysis",
    "text": "5.2 Principal Component Analysis\nOn of the most important applications of SVD is in the stable computation of the so called principal component analysis (PCA). It is a common technique in data exploration, analysis, visualization, and preprocessing.\nThe main idea of PCM is to transform the data in such a way that the main directions (principal components) capture the largest variation. In short we perform a change of the basis, see Definition 1.8.\nLet us investigate this in terms of a (artificial) data set.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: Noisy Gaussian Data, pp. 25-27).\n\n\nWe generate a noisy cloud (see Figure 5.3) that consists of \\(10000\\) points in 2D, generated from a normal distribution with zero mean and unit variance. The data is than:\n\nscaled by \\(2\\) in the first direction and by \\(\\frac12\\) in second,\nrotated by \\(\\frac\\pi3\\)\ntranslation in the direction \\(\\left[2\\ 1\\right]^{\\mathsf{T}}\\).\n\nThe resulting matrix \\(X\\) is a long and skinny matrix with each measurement (or experiment) stacked next to each other. This means, each column represents a new set, e.g. a time step, and each row corresponds to the same sensor.\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is a slight adaptation (for nicer presentation in these notes) of the (Brunton and Kutz 2022, Code 1.4) also see notebook on github.\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nxC = np.array([2, 1])      # Center of data (mean)\nsig = np.array([2, 0.5])   # Principal axes\n\ntheta = np.pi / 3            # Rotate cloud by pi/3\n\nR = np.array([[np.cos(theta), -np.sin(theta)],     # Rotation matrix\n              [np.sin(theta), np.cos(theta)]])\n\nnPoints = 10000            # Create 10,000 points\nX = R @ np.diag(sig) @ np.random.randn(2, nPoints) + np.diag(xC) @ np.ones((2, nPoints))\n\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(X[0, :], X[1, :], '.', color='k')\nax1.grid()\nax1.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\n## f_ch01_ex03_1b\n\nXavg = np.mean(X, axis=1)                  # Compute mean\nB = X - np.tile(Xavg, (nPoints, 1)).T       # Mean-subtracted data\n\n# Find principal components (SVD)\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nS = S / np.sqrt(nPoints - 1)\n\nax2 = fig.add_subplot(122)\nax2.plot(X[0, :], X[1, :], '.', color='k')   # Plot data to overlay PCA\nax2.grid()\nax2.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\ntheta = 2 * np.pi * np.arange(0, 1, 0.01)\n\n# 1-std confidence interval\nXstd = U @ np.diag(S) @ np.array([np.cos(theta), np.sin(theta)])\n\nax2.plot(Xavg[0] + Xstd[0, :], Xavg[1] + Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 2 * Xstd[0, :], Xavg[1] + 2 * Xstd[1, :], \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 3 * Xstd[0, :], Xavg[1] + 3 * Xstd[1, :], '-', color='r', linewidth=3)\n\n# Plot principal components U[:,0]S[0] and U[:,1]S[1]\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,0] * S[0]]),\n         np.array([Xavg[1], Xavg[1] + U[1,0] * S[0]]), '-', color='cyan', linewidth=5)\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,1] * S[1]]),\n         np.array([Xavg[1], Xavg[1] + U[1,1] * S[1]]), '-', color='cyan', linewidth=5)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.3: Principal components of the mean-subtracted Gaussian data on the left as, as well as the first three standard deviation ellisoids and the two scaled left singular vectors.\n\n\n\n\n\n\n5.2.1 Computation\nFor the computation we follow the outline given in (Brunton and Kutz 2022, chap. 1.5). First we need to center our matrix \\(X\\) according to the mean per feature, in our case per row. \\[\n\\overline{x}_j = \\frac1n \\sum_{i=1}^n X_{ij}\n\\] and our mean matrix is the outer product with the one vector \\[\n\\overline{X} = \\left[\\begin{array}{c}1\\\\\\vdots\\\\1\\end{array}\\right] \\overline{x}\n\\] which can be used to compute the centred matrix \\(B = X - \\overline{X}\\).\nThe PCA is the eigendecomposition of the covariance matrix \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}} B\n\\tag{5.2}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe normalization factor of \\(n-1\\) in Equation 5.2 an not \\(n\\) is called Bassel’s correction and compensates for the bias in the estimation of the population variance.\n\n\nAs \\(C\\) is symmetric and positive semi-definite, therefore it has non-negative real eigenvalues and the matrix \\(V\\) of the eigendecomposition satisfies \\(V^{-1} = V^{\\mathsf{T}}\\) (i.e. it is orthogonal Definition 1.10). The principal components are the eigenvectors and the eigenvalue are the variance along these components.\nIf we instead compute the SVD of \\(B = U\\Sigma V^{\\mathsf{T}}\\) we get \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}}B = \\frac{1}{n-1} V \\Sigma V^{\\mathsf{T}} = \\frac{1}{n-1} V (\\Lambda^{\\mathsf{T}}\\Lambda) V^{\\mathsf{T}}\n\\] leading to a way of computing the principal components in a robust way as \\[\n\\lambda_k = \\frac{\\sigma_k^2}{n-1}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIf the sensor ranges of our matrix are very different in magnitude the correlation matrix is scaled by the row wise standard deviation of \\(B\\) similar as for the mean.\n\n\nIn our example we get our scaled \\(\\sigma_1=1.988\\approx 2\\) and \\(\\sigma_2=0.496\\approx \\frac12\\). These results recover our given parameters very well. Additionally we can see that our rotation matrix is closely matched by \\(U\\) (up to signs) from our SVD: \\[\nR_{\\frac\\pi3} = \\left[\n\\begin{array}{cc} 0.5&0.866\\\\-0.866&0.5\\end{array}\n\\right], \\quad U = \\left[\n\\begin{array}{cc}-0.501&-0.865\\\\-0.865&0.501\\end{array} \\right]\n\\]\n\n\n5.2.2 Example Eigenfaces\nWe combine SVD/PCA in a illustrative example called eigenfaces as introduced in (Brunton and Kutz 2022, Sec 1.6, pp. 28-34).\nThe idea is to apply the PCA techniques to a large set of faces to extract the dominate correlations between the images and create a face basis that can be used to represent an image in these coordinates. For example you can reconstruct a face in this space by projecting onto the eigen vectors or it can be used for face recognition as similar faces usually cluster under this projection.\nThe images are taken from the Yale Face Dataset B, in our case we use a GitHub that provides Julia Pluto notebooks for Chapter 1 to 4 of Brunton and Kutz (2022).\nOur training set, so to speak, consists of the first 36 people in the dataset. We compute the average face and subtract it from our dataset to get our matrix \\(B\\). From here a SVD provides us with our basis \\(U\\). To test our basis we use individual 37 and a portion of the image of the MCI Headquarter (to see how well it performs on objects). For this we use the projection \\[\n\\tilde{x} = U_r U_r^{\\mathsf{T}} x.\n\\] If we split this up, we first project onto our found patterns (encode) and than reconstruct from them (decode).\n\n\n\n\n\n\nNote\n\n\n\nWe can understand this as encoding and decoding our test image, which is the general setup of an autoencoder (a topic for another lecture).\nThe correlation coefficients \\(x_r = U_r^{\\mathsf{T}} x\\) might reveal patterns for different \\(x\\). In the case of faces, we can use this for face recognition, i.e. if the coefficients of \\(x_r\\) are in the same cluster as other images, they are probably from the same person.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is an adaptation of the (Brunton and Kutz 2022, Code 1.7 and 1.9).\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\ntrainingFaces = faces[:, : np.sum(nfaces[:36])]\navgFace = np.mean(trainingFaces, axis=1)\n\nB = trainingFaces - np.tile(avgFace, (trainingFaces.shape[1], 1)).T\nU, _, _ = LA.svd(B, 'econ')\n\ntestFace = faces[:, np.sum(nfaces[:36])]\ntestFaceMS = testFace - avgFace\nrec = [25, 100, 400]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(2, 4, 1))\naxs.append(fig.add_subplot(2, 4, 2))\naxs.append(fig.add_subplot(2, 4, 3))\naxs.append(fig.add_subplot(2, 4, 4))\naxs.append(fig.add_subplot(2, 4, 5))\naxs.append(fig.add_subplot(2, 4, 6))\naxs.append(fig.add_subplot(2, 4, 7))\naxs.append(fig.add_subplot(2, 4, 8))\n\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    axs[i].set_title(f\"${r=}$\")\n\naxs[3].imshow(np.reshape(testFace, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\naxs[3].set_axis_off()\naxs[3].set_title(f\"Original image\")\n\nshift = 1500\ntestFaceMS = np.reshape(im_gray[shift:shift+n, shift:shift+m].T, n*m) - avgFace\nrec = [100, 400, 1600]\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[4 + i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[4 + i].set_axis_off()\n    axs[4 + i].set_title(f\"${r=}$\")\naxs[7].imshow(im_gray[shift:shift+n, shift:shift+m], cmap=plt.get_cmap(\"gray\"))\naxs[7].set_axis_off()\naxs[7].set_title(f\"Original image\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.4: Approximate reconstruction of a test face and an object using the eigenfaces basis for different order r.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to resource limitations the above computation can not be done for each build. We try to make sure that the code matches the image but if something is different if you try it yourself we apologise for that.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#further-applications-of-the-svd",
    "href": "matrixdc/svd.html#further-applications-of-the-svd",
    "title": "5  Singular Value Decomposition",
    "section": "5.3 Further applications of the SVD",
    "text": "5.3 Further applications of the SVD\nThere are many more applications of the SVD but we want to highlight some regarding systems of linear equations, \\[\nA x = b\n\\tag{5.3}\\] where the matrix \\(A\\), as well as the vector \\(b\\) is known an \\(x\\) is unknown.\nDepending on the structure of \\(A\\) and the specific \\(b\\) we have no, one, or infinitely many solutions. For now the interesting case is where \\(A\\) is rectangular and therefore we have either an\n\nunder-determined system \\(m\\ll n\\), so more unknowns than equations,\nover-determined system \\(m\\gg n\\), so more equations than unknowns.\n\nFor the second case (more equations than unknowns) we often switch to solving the optimization problem that minimizes \\[\n\\|Ax-b\\|_2^2.\n\\tag{5.4}\\] This is called the least square solution. The least square solution will also minimize \\(\\|Ax-b\\|_2\\). For an under-determined system we might seek the solution which minimizes \\(\\|x\\|_2\\) called the minimum norm solution.\nIf we us the SVD decomposition for \\(A = U \\Sigma V^{\\mathsf{T}}\\) we can define the following\n\n\n\n\n\n\n\nDefinition 5.3 (Pseudo-inverse) We define the matrix \\(A^\\dagger \\in \\mathbb{R}^{m\\times n}\\) by \\(A^\\dagger = V\\Sigma^\\dagger U^{\\mathsf{T}}\\) where \\[\n\\Sigma^\\dagger = \\operatorname{diag}\\left(\\frac{1}{\\sigma_1}. \\frac{1}{\\sigma_2}, \\ldots, \\frac{1}{\\sigma_r}, 0, \\ldots, 0\\right) \\in \\mathbb{R}^{m\\times n}, \\quad r=\\operatorname{rank}(A).\n\\]\nThe matrix \\(A^\\dagger\\) is often called the Moore-Penrose left pseudo-inverse as it fulfils the Moore-Penrose conditions conditions. It is also the matrix to provides the minimal Frobenius norm solution to \\[\n\\min_{X \\in \\mathbb{R}^{m\\times n}}\\| A X - I_n\\|_F.\n\\]\n(Compare Golub and Van Loan 2013, 290)\n\n\n\n\nIf we only use the truncated version, i.e. where we only use non-zero singular values, we can use it to find good solutions to Equation 5.4.\n\nIn numpy it can be computed by numpy.linalg.pinv.\n\n\n\n\n\n\n\n\nDefinition 5.4 (Condition number) The condition number of a matrix provides a measure how sensitive the solution of Equation 5.3 is to perturbations in \\(A\\) and \\(b\\). For a square matrix \\(A\\) the condition number is defined as \\[\n\\kappa(A) = \\|A\\| \\left\\|A^{-1}\\right\\|,\n\\] for an appropriate underlying norm. For the 2-norm \\(\\kappa_2\\) is \\[\n\\kappa_2(A) = \\|A\\|_2 \\left\\|A^{-1}\\right\\|_2 = \\frac{\\sigma_{max}}{\\sigma_{min}}.\n\\]\nTo get a better idea on what this means think of it in this way. For the perturbed linear system \\[\nA(x + \\epsilon_x) = b + \\epsilon_b,\n\\] we can outline the worst case, where \\(\\epsilon_x\\) aligns with the singular vector of the largest singular vector and \\(x\\) with the smallest singular value, i.e. \\[\nA(x + \\epsilon_x) = \\sigma_{min}x + \\sigma_{max}\\epsilon_x.\n\\] Consequently, the output signal-to-noise \\(\\|b\\|/\\|\\epsilon_b\\) is equivalent with the input signal-to-noise \\(\\|x\\|/\\|\\epsilon_x\\) and the factor between those two is \\(\\kappa_2(A)\\).\nIn this sense \\(\\kappa_2\\) can be extended for more general matrices.\n(Compare Golub and Van Loan 2013, 87; and Brunton and Kutz 2022, 18–19)\n\n\n\n\n\n5.3.1 Linear regression with SVD\nBefore we go into more details about regression in the next section we give a brief outlook in terms of how to solve such a problem with SVD.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: One-Dimensional Linear Regression, Example: Cement Heat Generation Data, pp. 19-22).\n\n\n\n5.3.1.1 Linear Regression (see Brunton and Kutz 2022, 19–21)\nFirst we just take a linear correlation that we augment with some Gaußian Noise. So our matrix \\(A\\) is simple a vector with our \\(x\\)-coordinates and \\(b\\) is the augmented image under our linear correlation. \\[\n\\left[\n    \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{array}\n\\right]x =\n\\left[\n    \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{array}\n\\right]\n\\quad\n\\Leftrightarrow\n\\quad\nU\\Sigma V^{\\mathsf{T}} x = b\n\\quad\n\\Leftrightarrow\n\\quad\nx = A^\\dagger b\n\\]\nFor this example \\(\\Sigma = \\|a\\|_2\\), \\(V=1\\), and \\(U=\\tfrac{a}{\\|a\\|_2^2}\\). This is basically just the projection of \\(b\\) along our basis \\(a\\) and this is \\[\nx = \\frac{a^{\\mathsf{T}} b}{a^{\\mathsf{T}} a}.\n\\]\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nk = 3\nA = np.arange(-2, 2, 0.25).reshape(-1, 1)\nb = k*A + np.random.randn(*A.shape) * 0.5\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(A, k*A, color=\"k\", label=\"Target\")\nplt.plot(A, b, 'x', color=\"r\", label=\"Noisy data\")\nplt.plot(A, A*x, '--', color=\"b\", label=\"Regression line\")\nplt.legend()\nplt.xlabel(\"$a$\")\nplt.ylabel(\"$b$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.5: Linear regression with SVD.\n\n\n\n\n\nOur reconstructed unknown \\(x=\\) 2.855 and is a reasonable good match for \\(k=\\) 3.0\n\n\n5.3.1.2 Multi-Linear Regression (see Brunton and Kutz 2022, 21–23)\nThe second example is based on the Portland Cement Data build in with MATLAB. In Python we again use the dataset provided on GitHub. The data set contains the heat generation during the hardening of 12 cement mixtures comprised of 4 basic ingredients, i.e. \\(A\\in \\mathbb{R}^{13\\times 4}\\). The aim is to determine the weights \\(x\\) that relate the proportion of the ingredients to the heat generation in the mixture.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\nimport requests\nimport io\n%config InlineBackend.figure_formats = ['svg']\n\n# Transform the content of the file into a numpy.ndarray\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_ingredients.csv\")\n# Transform the content of the file into a numpy.ndarray\nA = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/hald_heat.csv\")\nb = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(b, color=\"k\", label=\"Target - Heat data\")\nplt.plot(A@x, '--', color=\"b\", label=\"Regression\")\nplt.legend()\nplt.xlabel(\"mixture\")\nplt.ylabel(\"Heat[cal/g]\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.6: Estimate for hardening in cement mixtures.\n\n\n\n\n\nThis concludes our investigation of matrix decompositions, we will investigate further decompositions of signals later, but for now we dive deeper into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "regression/linear.html",
    "href": "regression/linear.html",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1 Ordinary Least Square\nIt is worth looking into the least square solution \\[\nE_2 (f) = \\sqrt{\\frac1n \\sum_{k=1}^n|\\underbrace{f(X_{k-}, c) - y_k}_{\\mathbf{e}_k} |^2},\n\\] more closely. We can interpret it as the optimization problem \\[\nc = \\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2\n\\] and with some linear algebra we get \\[\n\\begin{array}{ccl}\nc &= &\\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} \\langle y -Xv, y -Xv\\rangle,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} (y -Xv)^{\\mathsf{T}} (y -Xv),\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} y^{\\mathsf{T}}y - y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v.\\\\\n\\end{array}\n\\] In order to find a solution we compute the derivative with respect to \\(v\\) set it to \\(0\\) and simplify, i.e. \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\,v} y^{\\mathsf{T}}y- y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v = - 2X^{\\mathsf{T}}y + 2 X^{\\mathsf{T}}Xv\n\\tag{6.5}\\] and \\[\nv = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y \\equiv X^\\dagger y.\n\\] We recall, that \\(X^\\dagger\\) is called the Moore-Penrose pseudo-inverse, see Definition 5.3.\nSee Figure 6.1 for the result when using the pseudo-inverse.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#sec-regr-linear-ols",
    "href": "regression/linear.html#sec-regr-linear-ols",
    "title": "6  Linear Regression",
    "section": "",
    "text": "6.1.1 Alternative computation\nThe pseudo-inverse provides us with the optimal solution but for large systems the computation can be inefficient, or more precisely, there are more efficient ways to get the same results.\nFollowing (Brunton and Kutz 2022, 137–38) we can find an alternative for the above example in Figure 6.1.\nWe want to fit the data points \\((x_i, y_i)\\) with the function \\(f(x) = c_2 x + c_1\\) resulting in the error \\[\nE_2(f) = \\sqrt{\\frac1n \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2}.\n\\] A solution that minimizes the above equation also minimizes \\[\nE_2 = \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2\n\\] and we find the solution by partial differentiation \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_1} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) = 0,\n\\] \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_2} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) x_k = 0,\n\\] and this results in the system \\[\n\\left[\n\\begin{array}{cc}\nn & \\sum_k x_k \\\\\n\\sum_k x_k & \\sum_k x_k^2\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\nc_1 \\\\ c_2\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\n\\sum_k y_k\\\\\\sum_k x_k y_k\n\\end{array}\n\\right].\n\\tag{6.6}\\] This ansatz can be extended to polynomials of degree \\(k\\), where the result is always a \\((k+1) \\times (k+1)\\) matrix.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#polynomial-regression",
    "href": "regression/linear.html#polynomial-regression",
    "title": "6  Linear Regression",
    "section": "6.2 Polynomial Regression",
    "text": "6.2 Polynomial Regression\nPolynomial regression, despite its name, is linear regression with a special function \\(f\\) where the relation is polynomial in \\(x= \\left[a_1, \\dots, x_m\\right]^{\\mathsf{T}}\\) \\[\ny_k = x_k^0 + x_k^1 c_1 + x_k^2 c_2 + \\cdots + x_k^m c_m, \\quad 1\\leq k \\leq n,\n\\] With the matrix form \\[\nX = \\left[\n\\begin{array}{cccc}\n1 & x_1    &x_1^2  & \\cdots &x_1^m  \\\\\n1 & x_2    &x_2^2  & \\cdots &x_2^m  \\\\\n1 & \\vdots &\\vdots & \\ddots &\\vdots \\\\\n1 & x_n    &x_n^2  & \\cdots &x_n^m\n\\end{array}\n\\right]\n\\tag{6.7}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe matrix Equation 6.7 is called the Vandermonde matrix.\n\n\nThis can be solved in the same ways as the before with \\(X^\\dagger\\) or the direct system, but it should not as Equation 6.7 is badly conditioned. There are other methods like divided differences, Lagrange interpolation for this task.\n\n\n\n\n\n\n\nExample 6.1 (Parameter estimation of a falling object) Just because we deal with linear regression this does not means that the model needs to be linear too. As long as we are linear in the parameters \\(c\\) we can apply our findings, even for non linear independent variables.\nTo illustrate this, let us consider an object falling without aerodynamic drag, described by the differential equation \\[\nm \\ddot{y}(t) = -m g,\n\\] for the gravitational constant \\(g\\). Integration with respect to \\(t\\) results in \\[\ny(t) = y(0) + v(0) t - \\frac{g}{2} t^2.\n\\] So we get \\[\nX{k-} =\n\\left[\n    \\begin{array}{ccc}\n    1 & t_k & -\\frac{1}{2} t_k^2 \\\\\n    \\end{array}\n\\right],\n\\quad \\text{and} \\quad\ny = \\left[\n\\begin{array}{c}\ny^{(0)}\\\\ v^{(0)} \\\\ g\n\\end{array}\n\\right]\n\\] or in the long form, for \\(t_{k+1}-t_k = 0.1\\) \\[\nX = \\left[\n\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0.1 & -0.005 \\\\\n1 & 0.2 & -0.020 \\\\\n1 & 0.3 & -0.045 \\\\\n1 & 0.4 & -0.080 \\\\\n\\vdots & \\vdots & \\vdots\n\\end{array}\n\\right]\n\\] and we can, for example, estimate our unknowns \\(y^{(0)}\\), \\(v^{(0)}\\), and \\(g\\) by \\[\nc = X^\\dagger y.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 6.2 (Polynomial regression) In the following example we generate an artificial sample of \\(n=100\\) points resulting in the samples \\[\ny_k = \\frac12 x_k^2 + x_k + 2 + \\epsilon_k\n\\] where \\(\\epsilon_k\\) is a random number that simulates the error. We perform the interpolation with \\(X^\\dagger\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore', np.exceptions.RankWarning)\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 100\nx = 6 * np.random.rand(m) - 3\ny = 1/2 * x ** 2 + x + 2 + np.random.randn(m)\n\nX1 = np.vander(x, 2)\nX2 = np.vander(x, 3)\nX3 = np.vander(x, 16)\n\np1 = np.linalg.pinv(X1) @ y\np2 = np.linalg.pinv(X2) @ y\np3 = np.linalg.pinv(X3) @ y\np4 = np.polyfit(x, y, 300)\n\nxf = np.arange(-3, 3, 0.1)\ny1 = np.polyval(p1, xf)\ny2 = np.polyval(p2, xf)\ny3 = np.polyval(p3, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\", linewidth=3)\nplt.plot(xf, y1, label=r\"$m=1$\")\nplt.plot(xf, y2, label=r\"$m=2$\")\nplt.plot(xf, y3, label=r\"$m=16$\")\nplt.plot(xf, y4, label=r\"$m=300$\")\n\nplt.ylim(0, 10)\nplt.xlim(-3, 3)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.legend(loc=\"upper left\")\n#plt.gca().set_aspect(0.25)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6.2: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe condition of the Vandermonde matrix increases rapidly:\n\n\n\ndegree\n\\(m=2\\)\n\\(m=3\\)\n\\(m=5\\)\n\\(m=10\\)\n\\(m=15\\)\n\\(m=20\\)\n\n\n\n\n\\(\\kappa_2\\)\n6.07e0\n1.63e1\n1.89e2\n154e5\n1.24e8\n1.41e11\n\n\n\nThe result of the \\(m=300\\) is unstable and we can not compute it via \\(X^\\dagger\\).\n\n\nAs can be seen in Figure 6.2 we do not necessarily get a good result if we use a higher degree polynomial. This is especially true if we extrapolate and not interpolate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#data-linearization",
    "href": "regression/linear.html#data-linearization",
    "title": "6  Linear Regression",
    "section": "6.3 Data Linearization",
    "text": "6.3 Data Linearization\nQuite often it is possible to linearize our model at hand. For example if we want to fit for \\[\nf(x, c) = c_2 \\exp(c_1 x),\n\\tag{6.8}\\]\nand use the same derivation as for Equation 6.6 we end up with the corresponding system as \\[\nc_2 \\sum_k x_k \\exp(2 c_1 x_k) - \\sum_k x_k y_k \\exp(c_1 x_k) =0,\n\\] \\[\nc_2 \\sum_k \\exp(2 c_1 x_k) - \\sum_k y_k \\exp(c_1 x_k) =0.\\\\\n\\]\nThis non-linear system can not be solved in a straight forward fashion but we can avoid it by linearization with the simple transformation \\[\n\\begin{array}{ccl}\n\\hat{y} &=& \\ln(y), \\\\\n\\hat{x} &=& x, \\\\\nc_3 &=& \\ln c_2,\n\\end{array}\n\\] and taking the natural logarithm of both sides of Equation 6.8 and simplifying \\[\n\\ln y = \\ln(c_2 \\exp(c_1 x)) = \\ln(c_2) + c_1 x.\n\\] Now all that is left to apply \\(\\ln\\) to the data \\(y\\) and solve the linear problem. In order to apply it to the original function the parameters transform needs to be reversed.\n\n\n\n\n\n\n\nExample 6.3 (World population) We take a look at the population growth were the data is kindly provided by Ritchie et al. (2023). Have a look at there excellent work on ourworldindata.org.\n\nShow the code for the figure\nfrom owid.catalog import charts\nimport numpy as np\nimport scipy.optimize\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndf = charts.get_data(\"https://ourworldindata.org/grapher/population?country=~OWID_WRL\")\ndata = df[df[\"entities\"] == \"World\"]\nx = data[\"years\"].to_numpy()\ny = data[\"population\"].to_numpy()\nylog = np.log(y)\n\ndef fit3(x0, t):\n    x, y = t\n    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))\n\nstart = [-np.inf, 0, 1700, 1900, 1980]\n\nyest = []\n\nfor s in start:\n    filter = x &gt;= s\n    t = (x[filter], ylog[filter])\n    x0 = np.array([1, 1])\n    b = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)\n    yest.append(np.exp(b[1]) * np.exp(b[0] * x))\n\nfig = go.Figure()\nfig2 = go.Figure()\nfig.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"], y=data[\"population\"], name=\"data\"))\nfig2.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"], y=data[\"population\"], name=\"data\"))\n\nfor i, ye in enumerate(yest):\n    fig.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n    fig2.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n\n\nfig.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig.update_yaxes(title_text=\"population\")\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig.show()\n\nfig2.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig2.update_yaxes(title_text=\"population\", type=\"log\", range=[8.5,10])\nfig2.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig2.show()\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) World population with regression lines normal scale.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) World population with regression lines logarithmic scale.\n\n\n\n\n\n\n\nFigure 6.3: Line fit with different norms. Top without outliers, bottom with one outlier.\n\n\n\n\n\n\n\nNext, we are going to look into actual non-linear regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber, Esteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population Growth.” Our World in Data.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html",
    "href": "regression/nonlinear.html",
    "title": "7  Non-linear Regression",
    "section": "",
    "text": "7.1 Gradient Descent\nFor a higher dimensional system or function \\(f\\) the gradient must be zero\n\\[\n\\nabla f(x) = 0\n\\] to know that we are in an extrema. Since we can have saddle points this is not the sole criteria but a necessary one. Gradient descent, as the name suggest uses the gradient as direction in an iterative algorithm to find a minimum.\nThe idea is basically, if you are lost on a mountain in the fog and you can not see the path, the fastest and a reliable way that only uses local information is to follow the steepest slope down.\nWe express this algorithm in terms of the iterations \\(x^k\\) for guesses of the minimum with the updates \\[\nx^{(k+1)} = x^{(k)} - \\delta\\, \\nabla f(x^{(k)})\n\\] where the parameter \\(\\delta\\) defines how far along the gradient descent curve we move. This formula is an update for a Newton method where we use the derivative as the update function. This leaves us with the problem to find an algorithm to determine \\(\\delta\\).\nAgain, we can view this as an optimization problem for a new function \\[\nF(\\delta) = f(x^{(k+1)}(\\delta))\n\\] and \\[\n\\partial_\\delta F = -\\nabla f(x^{(k+1)})\\nabla f(x^{(k)}) = 0.\n\\tag{7.1}\\]\nNow the interpretation of Equation 7.1 is that we want that the gradient of the current step is orthogonal to the gradient of the next step.\nIn order to make it clearer we follow the example given in (Brunton and Kutz 2022, sec. 4.2,pp. 141-144).\nIn order to get a better idea on how this is working for curve fitting we apply the gradient descent method to our curve fitting from Section 6.1.\nIn Equation 6.5 we computed the gradient and instead of computing \\(X^\\dagger\\) with high cost we get the low cost iterative solver:\n\\[\nc^{(k+1)} = c^{(k)} - \\delta (2 X^{\\mathsf{T}} X c^{(k)} - 2 X^{\\mathsf{T}} y)\n\\]\nAs \\(\\delta\\) is tricky to compute we go ahead and introduce we do not update it but prescribe it. This will not grant us the optimal convergence (if there is convergence) but if we choose it right we still get convergence.\nSo lets try it with our example from Figure 6.1.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef gd(c, delta, X, y, n, stop=1e-10):\n    diff = 1\n    for _ in range(1, n):\n        cnew = update(c, delta, X, y)\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = gd(c, delta, X, y, 50)\nc_20 = gd(c_10, delta, X, y, 50)\nc_30 = gd(c_20, delta, X, y, 200)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"$n=50$\")\nplt.plot(xf, y2, label=r\"$n=100$\")\nplt.plot(xf, y3, label=r\"$n=300$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.2: Line fit with gradient descent for different number of iterations and learning rate 2e-3.\nThe above algorithm uses the entire set \\(X\\) for the computation. For a large enough set \\(X\\) this is quite cost intense, even if it is still cheaper than computing \\(X^\\dagger\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "href": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "title": "7  Non-linear Regression",
    "section": "",
    "text": "Warning\n\n\n\nA function does not necessarily experience gravity in the same way as we do, so please do not try this in real live, i.e. cliffs tend to be hard to walk down.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.1 (Gradient descent) For the function \\[\nf(x) = x_1^2 + 3 x_2^2\n\\] we can compute the gradient as \\[\n\\nabla f (x)= \\left[ \\begin{array}{c} \\partial_{x_1} f(x)\\\\ \\partial_{x_2} f(x) \\end{array} \\right] = \\left[ \\begin{array}{c} 2 x_1 \\\\ 6 x_2 \\end{array} \\right]\n\\] Resulting in \\[\nx^{k+1} = x^{(k)} - \\delta \\, \\nabla f(x^{(k)}) =\n\\left[ \\begin{array}{c} (1 - 2 \\delta) x^{(k)}_1 \\\\ (1 - 6 \\delta)x^{(k)}_2 \\end{array} \\right].\n\\] Consequently \\[\nF(\\delta) = (1-2\\delta)^2 x_1^2 + (1-6\\delta)^2 x_2^2,\n\\] \\[\n\\partial_\\delta F = -2^2(1-2\\delta)x_1^2 - 6^2(1-6\\delta)x_2^2,\n\\] and \\[\n\\partial_\\delta F(\\delta) = 0 \\Leftrightarrow \\delta = \\frac{x_1^2 + 9 x_2^2}{2 x_1^2 + 54 x_2^2}.\n\\]\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\nimport numpy as np\n\nx_ = np.linspace(-3, 3, 20)\ny_ = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_, y_)\n\nf = lambda x, y: np.pow(x, 2) + 3 * np.pow(y, 2)\ngrad_f = lambda x: x * np.array([2, 6]).reshape(x.shape)\ndelta = lambda x: (x[0]**2 + 9 * x[1]**2)/(2 * x[0]**2 + 54 * x[1]**2)\n\nZ = f(X, Y)\n\nfig = go.Figure()\nfig.add_trace(go.Surface(z=Z, x=X, y=Y, colorscale='greys', name=\"Function\"))\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\", project_z=True))\nfig.update_scenes(xaxis_title_text=r\"x_1\",  \n                  yaxis_title_text=r\"x_2\",  \n                  zaxis_title_text=r\"f(x)\")\nx = np.array([3, 2]).reshape((1, 2))\nz = np.array(f(x[0, 0], x[0, 1]))\ndiff = 1\n\nwhile diff &gt; 1e-10:\n    x_new = x[-1, :] - delta(x[-1, :]) * grad_f(x[-1, :])\n    z = np.hstack((z, f(x_new[0], x_new[1])))\n    diff = np.linalg.norm(z[-1] - z[-2])\n    x = np.vstack((x, x_new))\n\nfig.add_scatter3d(x=x[:, 0], y=x[:, 1], z=z, line_color='red', name=\"Descent path\")\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 7.1: Gradient descent applied for the function \\(f(x) = x_1^2 + 3x_2^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can not compute the gradient analytically there are numerical methods to help do the computation.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn machine learning the parameter \\(\\delta\\) is often called the learning rate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#stochastic-gradient-descent",
    "href": "regression/nonlinear.html#stochastic-gradient-descent",
    "title": "7  Non-linear Regression",
    "section": "7.2 Stochastic Gradient Descent",
    "text": "7.2 Stochastic Gradient Descent\nIn order to reduce cost we can randomly select some points of our training set and only train with those. Obviously the computation of the gradient becomes much faster. We call this method Stochastic Gradient descent (SGD).\nIn Figure 7.3 we see the convergence for randomly selecting 1, 3, and 6 indices of our possible 10.\nThe downside of the SGD algorithm is that the algorithm does not settle down for a long time and will jump. In the other side it might get less stuck in local minima.\nOne possibility to try to get the strength of both is to use SDG to get a good guess for your initial value and SD for the fine tuning.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef sgd(c, delta, X, y, n, indices=-1, stop=1e-10):\n    if indices == -1:\n        indices = X.shape[0]\n    diff = 1\n    for _ in range(1, n):\n        I = np.random.choice(X.shape[0], size=indices, replace=False)\n        I.sort()\n        cnew = update(c, delta, X[I, :], y[I])\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, 1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = sgd(c, delta, X, y, 200, 1)\nc_20 = sgd(c, delta, X, y, 200, 3)\nc_30 = sgd(c, delta, X, y, 200, 5)\nc_ft = gd(c_20, delta, X, y, 150, -1)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\nyft = np.polyval(c_ft, xf)\ny4 = np.polyval(p4, xf)\n\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"#I=$1$\")\nplt.plot(xf, y2, label=r\"#I=$3$\")\n#plt.plot(xf, y3, label=r\"#I=$5$\")\nplt.plot(xf, yft, label=r\"#I=$3$ GD $n=100$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\n\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.3: Line fit with stochastic gradient descent with 1 or 3 samples and 200 iterations as well as the 3 sample version as initial guess for GD with 100 iterations.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#categorical-variables",
    "href": "regression/nonlinear.html#categorical-variables",
    "title": "7  Non-linear Regression",
    "section": "7.3 Categorical Variables",
    "text": "7.3 Categorical Variables\nEven with our excursion to non-linear regression we still had somewhat regular data to work with. This is not always the case. Sometimes there are trends in the data, like per month, or day. The inclusion of categorical variables can help to control for trends in the data.\nWe can integrate such variables to the regressor by adding columns to the matrix \\(X\\) for each of the categories. Note, they can be interpreted as to correspond to the offset (the constant \\(1\\)) so this column can be omitted and each category gets a separate offset.\nWe can see this in action in the following example. We investigate the unemployment data in Austria. There is a strong seasonality Figure 7.4 (b) in the data. This is largely due to the fact that the Austrian job market has a large touristic sector with its season and the construction industry employs less people during summer.\nFor the regression Figure 7.4 (b) we can see that this captures the seasonal change quite well.\nThe data is taken from Arbeitsmarktdaten online.\n\n\n\n\n\n\n\n\n\n\n\n(a) Regression with categorical variables per month.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Seasonality of the unemployment average over the years.\n\n\n\n\n\n\n\nFigure 7.4: Unemployment data from Austria for the years 2010 to 2017.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is also quite a difference between man and woman that could be categorized separately.\n\n\nWe wrap up this section about regression by talking more abstract about the regression of linear systems and some general thoughts about the selection of the model and consequences.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html",
    "href": "regression/optimizers.html",
    "title": "8  Optimizers",
    "section": "",
    "text": "8.1 Over-Determined Systems\nWe speak of an over-determined system if we have more rows than columns, i.e. \\(A\\) is tall and skinny and in general there is no solution to Equation 8.1 but rather we minimize the error according to a norm, see Section 6.1. If we further impose a restriction on \\(x\\) we can select a more specific solution.\nThe generalized form is \\[\nx = \\underset{v}{\\operatorname{argmin}} \\|Av - b\\|_2 + \\lambda_1 \\|v\\|_1 + \\lambda_2\\|v\\|_2\n\\tag{8.2}\\] where the parameters \\(\\lambda_1\\) and \\(\\lambda_2\\) are called the penalization coefficients, with respect to the norm. Selecting these coefficients is the first step towards model selection.\nLet us have a look at this in action for solving a random system with different parameters \\(\\lambda_1\\) and setting \\(\\lambda_2\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#over-determined-systems",
    "href": "regression/optimizers.html#over-determined-systems",
    "title": "8  Optimizers",
    "section": "",
    "text": "8.1.1 LASSO\nThe least absolute shrinkage and selection operator LASSO solves Equation 8.2 with \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2=0\\), i.e. only optimizing with the \\(\\ell_1\\) norm. The theory tells us that for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0 = np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 1]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda_1={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 50, rf\"$\\lambda_1={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 70)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.1: LASSO regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.\n\n\n\n\n\nThe last row of Figure 8.1 confirms this quite impressively, interesting enough the solution also becomes positive.\n\n\n8.1.2 RIDGE\nThe Ridge Regression solves Equation 8.2 with \\(\\lambda_1 = 0\\) and \\(\\lambda_2 &gt; 0\\), i.e. only optimizing with the \\(\\ell_2\\) norm. The theory tells us that for for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0 = np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 2]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda_2={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 15, rf\"$\\lambda_2={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 20)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.2: Ridge regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.1 (Implement the above optimization yourself.) Fill out the missing parts:\n\n\nCode fragment for implementation.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = # Random matrix with m rows and 100 columns \nb = np.random.rand(m)\nx0 = # Optimal 2 norm solution without penalization as initial start\n\noptimize = # function to optimize\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=, x0=x0)    # use your correct arguments here\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 15, rf\"$\\lambda={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 20)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#sec-regression-optimizers-msou",
    "href": "regression/optimizers.html#sec-regression-optimizers-msou",
    "title": "8  Optimizers",
    "section": "8.2 Model Selection/Identification and over-/underfitting",
    "text": "8.2 Model Selection/Identification and over-/underfitting\nLet us use the results we have obtain so far for a discussion on model selection.\nSo far, we have mostly explicitly proposed a model that we think will fit our data and we have seen that even it this case we can still choose multiple parameters to fin tune our selection.\nNow consider the other possibility, we have data where the model is unknown. For example, in Example 6.1 we stopped with degree 2 for our polynomial because we know about Newton’s principles, if we don’t know it, we might extend the model for a higher degree.\nOne of the leading assumptions to use in such a case is:\n\nAmong competing hypotheses, the one with the fewest assumptions should be selected, or when you have two competing theories that make exactly the same predictions, the simpler one is the more likely. - Occam’s razor\n\nThis plays an intimate role in over- and underfitting of models. To illustrate this we recall Example 6.2 with Figure 6.2 as seen below once more.\n\n\n\n\n\n\n\n\nFigure 8.3: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\nFor \\(m=1\\), a straight line, we have an underfitted model. We can not adequately capture the underlying model, at least not in the entire region.\nIf we move to \\(m=16\\) and the extreme \\(m=300\\) we see an overfitted system. The \\(m=16\\) curve follows clusters of points too close, e.g. in the region around \\(x=-2\\), this is more pronounced for \\(m=300\\) where we quite often closely follow our observations but between them we clearly overshoot.\nIn this way we can also say that an overfitted system follows the training set to closely and will not generalize good for another testing/evaluation set.\nAs a consequence model selection should always be followed by a cross-validation. Meaning we need to check if our model is any good.\nA classic method is the k-fold cross validation:\n\nTake random portions of your data and build a model. Do this \\(k\\) times and average the parameter scores (regression loadings) to produce a cross-validated model. Test the model predictions against withheld (extrapolation) data and evaluate whether the model is actually any good. - (see Brunton and Kutz 2022, 159)\n\nAs we can see, there are a lot of further paths to investigate but for now this concludes our excursion into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html",
    "href": "signal/fourier.html",
    "title": "9  Fourier Transform",
    "section": "",
    "text": "9.1 Fourier Series\nIn Fourier analysis the first result is stated for a periodic and piecewise smooth function \\(f(t)\\).\nWith the help of Euler’s formula: \\[\n\\mathrm{e}^{\\mathrm{i} k t} = \\cos(k t) + \\mathrm{i} \\sin(k t)\n\\tag{9.2}\\] we can rewrite Equation 9.1 as \\[\nf(t) = \\sum_{k=-\\infty}^\\infty c_k \\mathrm{e}^{\\omega\\mathrm{i} k t}\n\\] with \\[\nc_k = \\frac{1}{L} \\int_0^L f(t) \\mathrm{e}^{-\\omega\\mathrm{i} k t}\\, \\mathrm{d}t.\n\\] and for \\(n=1, 2, 3, \\ldots\\) \\[\nc_0 = \\tfrac12 a_0, \\quad c_n = \\tfrac12 (a_n - \\mathrm{i} b_n), \\quad c_{-n} = \\tfrac12 (a_n + \\mathrm{i} b_n).\n\\]",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fourier-series",
    "href": "signal/fourier.html#fourier-series",
    "title": "9  Fourier Transform",
    "section": "",
    "text": "Definition 9.4 (Fourier Series) For a \\(L\\)-periodic function \\(f(t)\\) we can write \\[\nf(t) = \\frac{a_0}{2} + \\sum_{k=1}^\\infty \\left(a_k \\cos\\left(\\omega k t \\right)+ b_k \\sin\\left(\\omega k t\\right)\\right),\n\\tag{9.1}\\] for \\[\n\\begin{align}\na_k = \\frac{2}{L}\\int_0^L f(t) \\cos\\left(\\omega k t \\right)\\, \\mathrm{d}t,\\\\\nb_k = \\frac{2}{L}\\int_0^L f(t) \\sin\\left(\\omega k t \\right)\\, \\mathrm{d}t.\n\\end{align}\n\\] where we can view the last two equations as the projection onto the orthogonal basis \\(\\{\\cos(k t), \\sin(k t)\\}_{k=0}^\\infty\\), i.e. \\[\n\\begin{align}\na_k = \\frac{1}{\\|\\cos\\left(\\omega k t\\right)\\|_2^2} \\langle f(t), \\cos\\left(\\omega k t\\right)\\rangle, \\\\\nb_k = \\frac{1}{\\|\\sin\\left(\\omega k t\\right)\\|_2^2} \\langle f(t), \\sin\\left(\\omega k t\\right)\\rangle.\n\\end{align}\n\\]\nIf we perform a partial reconstruction by truncating the series at \\(M\\) we get \\[\n\\hat{f}_M(t) = \\frac{a_0}{2} + \\sum_{k=1}^M \\left(a_k \\cos\\left(\\omega k t \\right)+ b_k \\sin\\left(\\omega k t \\right)\\right).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(f(t)\\) is real valued than \\(c_k = \\overline{c}_{-k}\\).\n\n\n\n\n\n\n\n\n\nExample 9.1 (Fourier Series of Hat functions) We test the Fourier Series with two different hat functions. The first represents a triangle with constant slope up and down, the second a rectangle with infinite slope in the corners.\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n# Parameters\nL = 2 * np.pi\nM = 7\nM2 = 50\nN = 100\n# Hat functions\nfun = lambda t, L: 0 if abs(t) &gt; L / 4 else (1 - np.sign(t) * t * 4 / L)\nfun2 = lambda t, L: 0 if abs(t) &gt; L / 4 else 1\n\n# t\n1t = np.linspace(-L/2, L/2, N, endpoint=False)\ndt = t[1] - t[0]\nw = np.pi * 2 / L\n\nf = np.fromiter(map(lambda t: fun(t, L), t), t.dtype)\nf2 = np.fromiter(map(lambda t: fun2(t, L), t), t.dtype)\n\n# Necessary functions\nscalarproduct = lambda f, g, dt: dt * np.vecdot(f, g)\na_coeff = lambda n, f: 2 / L * scalarproduct(f, np.cos(w * n * t), dt)\nb_coeff = lambda n, f: 2 / L * scalarproduct(f, np.sin(w * n * t), dt)\n\n# f_hat_0\nf_hat = np.zeros((M + 1, N))\nf_hat[0, :] = 1/2 * a_coeff(0, f)\nf2_hat = np.zeros((M2 + 1, N))\nf2_hat[0, :] = 1/2 * a_coeff(0, f2)\n\n# Computation of the approximation\na = np.zeros(M)\nb = np.zeros(M)\nfor i in range(M):\n    a[i] = a_coeff(i + 1, f)\n    b[i] = b_coeff(i + 1, f)\n    f_hat[i + 1, :] = f_hat[i, :] + \\\n        a[i] * np.cos(w * (i + 1) * t) + \\\n        b[i] * np.sin(w * (i + 1) * t)\n\nfor i in range(M2):\n    f2_hat[i + 1, :] = f2_hat[i, :] + \\\n        a_coeff(i + 1, f2) * np.cos(w * (i + 1) * t) + \\\n        b_coeff(i + 1, f2) * np.sin(w * (i + 1) * t)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f, label=r\"$f$\")\nplt.plot(t, f_hat[-1, :], label=r\"$\\hat{f_7}$\")\nplt.xticks([])\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, f_hat[0, :], label=rf\"$a_{0}$\")\nfor i in range(M):\n    plt.plot(t, a[i] * np.cos(w * (i+1) * t), label=rf\"$a_{i+1}\\cos({i+1}\\omega t)$\")\nplt.legend(ncol=np.ceil((M + 1) / 2), bbox_to_anchor=(1, -0.1))\nplt.xticks([])\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, f2, label=r\"$f$\")\nplt.plot(t, f2_hat[7, :], label=r\"$\\hat{f}_7$\")\nplt.plot(t, f2_hat[20, :], label=r\"$\\hat{f}_{20}$\")\nplt.plot(t, f2_hat[50, :], label=r\"$\\hat{f}_{50}$\")\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n1\n\nNot including the endpoint is important, as this is part of the periodicity of the function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sawtooth function and the reconstruction with 7 nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Nodes of the reconstruction\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Step function and the reconstruction with various nodes\n\n\n\n\n\n\n\nFigure 9.1: Fourier transform of a two hat functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Self implementation Example 9.1\n\n\n\n\n\nImplement the code yourself by filling out the missing sections:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nL = 2 * np.pi   # Interval\nM = 7           # Nodes for the first function\nM2 = 50         # Nodes for the second function\nN = 101         # Interpolation points\n# Hat functions\nfun = lambda x, L: # tooth\nfun2 = lambda x, L: # step\n\n# x and y for the functions\nt = # points for the evaluation ()\ndt = t[1] - t[0]\nw = np.pi * 2 / L\n\nf = np.fromiter(map(lambda t: fun(t, L), t), t.dtype)\nf2 = np.fromiter(map(lambda t: fun2(t, L), t), t.dtype)\n\n# Necessary functions\nscalarproduct = lambda f, g, dt: # see definition in the notes\na_coeff = lambda n, f: # see definition in the notes\nb_coeff = lambda n, f: # see definition in the notes\n\n# f_hat_0\nf_hat = np.zeros((M + 1, N))\nf_hat[0, :] = # a_0 for f\nf2_hat = np.zeros((M2 + 1, N))\nf2_hat[0, :] = # a_0 for f2\n\n# Computation of the approximation\na = np.zeros(M)\nb = np.zeros(M)\nfor i in range(M):\n    a[i] = a_coeff(i + 1, f)\n    b[i] = b_coeff(i + 1, f)\n    f_hat[i + 1, :] = f_hat[i, :] + \\\n        a[i] * np.cos(w * (i + 1) * t) + \\\n        b[i] * np.sin(w * (i + 1) * t)\n\nfor i in range(M2):\n    f2_hat[i + 1, :] = f2_hat[i, :] + \\\n        a_coeff(i + 1, f2) * np.cos(w * (i + 1) * t) + \\\n        b_coeff(i + 1, f2) * np.sin(w * (i + 1) * t)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f, label=r\"$f$\")\nplt.plot(t, f_hat[-1, :], label=r\"$\\hat{f_7}$\")\nplt.xticks([])\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, f_hat[0, :], label=rf\"$a_{0}$\")\nfor i in range(M):\n    plt.plot(t, a[i] * np.cos(w * (i+1) * t), label=rf\"$a_{i+1}\\cos({i+1}\\omega t)$\")\nplt.legend(ncol=np.ceil((M + 1)/2), bbox_to_anchor=(1, -0.1))\nplt.xticks([])\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, f2, label=r\"$f$\")\nplt.plot(t, f2_hat[7, :], label=r\"$\\hat{f}_7$\")\nplt.plot(t, f2_hat[20, :], label=r\"$\\hat{f}_{20}$\")\nplt.plot(t, f2_hat[50, :], label=r\"$\\hat{f}_{50}$\")\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe phenomenon that the truncated Fourier series oscillates in Figure 9.1 (c) due to the discontinuity of the function is called the Gibbs phenomenon.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fourier-transform",
    "href": "signal/fourier.html#fourier-transform",
    "title": "9  Fourier Transform",
    "section": "9.2 Fourier Transform",
    "text": "9.2 Fourier Transform\nThe Fourier Series is defined for \\(L\\)-periodic functions. The Fourier transform extends this to functions with the domain extended to \\(\\pm\\infty\\).\nLet us start of with the series representation we already know: \\[\nf(t) = \\sum_{k=-\\infty}^\\infty c_k \\mathrm{e}^{\\mathrm{i} \\omega k t}\n\\] with the coefficients \\[\nc_k = \\frac{1}{2L} \\int_{-L}^{L} f(t) \\mathrm{e}^{-\\mathrm{i} \\omega_k t}\\, \\mathrm{d}t,\n\\] with \\(\\omega_k=\\frac{k\\pi}{L} = k\\Delta \\omega\\).\nIf we now perform the transition for \\(L \\to \\infty\\) resulting in \\(\\Delta\\omega \\to 0\\) and basically moving from discrete frequencies to a continuous set of frequencies. This results in \\[\nf(t) = \\lim_{\\Delta \\omega \\to 0} \\sum_{k=-\\infty}^\\infty \\frac{\\Delta \\omega}{2\\pi}\n\\int_{-\\tfrac{\\pi}{\\Delta \\omega}}^{\\tfrac{\\pi}{\\Delta \\omega}} f(\\xi) \\mathrm{e}^{-\\mathrm{i} k \\Delta \\omega \\xi}\\, \\mathrm{d}\\xi\\,\\, \\mathrm{e}^{\\Delta\\omega\\mathrm{i} k t}\n\\] which is a Riemann integral and the kernel becomes the Fourier Transform of our function.\n\n\n\n\n\n\n\nDefinition 9.5 (Fourier Transform) A function \\(f\\,:\\, \\mathbb{R} \\to \\mathbb{R}\\) is called fourier transposable if \\[\n\\hat{f}(\\omega) = \\mathcal{F}\\{f(t)\\} = \\int_{-\\infty}^{\\infty} f(t)\\mathrm{e}^{-\\mathrm{i} \\omega t}\\, \\mathrm{d}t\n\\] exists for all \\(\\omega\\in\\mathbb{R}\\). In this case we call \\(\\hat{f}(\\omega) \\equiv \\mathcal{F}\\{f(t)\\}\\) the Fourier transform of \\(f(t)\\).\nThe inverse Fourier transform is defined as \\[\n\\mathcal{F}^{-1}\\{\\hat{f}(\\omega)\\} = \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} \\hat{f}(\\omega)\\mathrm{e}^{\\mathrm{i} \\omega x}\\, \\mathrm{d}\\omega\n\\]\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pair \\((f, \\hat{f})\\) is often called the Fourier transform pair.\nThe two integrals converge, as long as both functions are Lebesgue integrable, i.e.  \\[\\int_{-\\infty}^\\infty|f(t)|\\, \\mathrm{d}t \\le \\infty,\\] or \\(f, \\hat{f} \\in L^1[(-\\infty, \\infty)]\\).\n\n\nAs could be expected, the Fourier transform has properties that lead to computational advantages.\nFor tow functions \\(f, g \\in L^1[(-\\infty, \\infty)]\\) and \\(\\alpha, \\beta\\in\\mathbb{C}\\) the following properties hold:\n\nLinearity \\[\n\\mathcal{F}\\{\\alpha f(t) + \\beta g(t)\\} =\n\\alpha \\mathcal{F}\\{f(t)\\} + \\beta \\mathcal{F}\\{g(t)\\} =\n\\alpha \\hat{f}(\\omega)+ \\beta \\hat{g}(\\omega),\n\\] and \\[\n\\mathcal{F}^{-1}\\{\\alpha \\hat{f}(\\omega) + \\beta \\hat{g}(\\omega)\\} =\n\\alpha \\mathcal{F}^{-1}\\{\\hat{f}(\\omega)\\} + \\beta \\mathcal{F}^{-1}\\{\\hat{g}(\\omega)\\} =\n\\alpha f(t) + \\beta g(t).\n\\]\nConjugation \\[\n\\mathcal{F}\\{\\overline{f(t)}\\} = \\overline{\\hat{f}(-\\omega)}.\n\\]\nScaling, for \\(\\alpha \\neq 0\\) \\[\n\\mathcal{F}\\{f(\\alpha t)\\} = \\frac{1}{|\\alpha|}\\hat{f}\\left(\\frac{\\omega}{\\alpha}\\right).\n\\]\nDrift in time, for \\(a\\in\\mathbb{R}\\) \\[\n\\mathcal{F}\\{f(t - a)\\} = \\mathrm{e}^{-\\mathrm{i}\\omega a}\\hat{f}(\\omega).\n\\]\nDrift in frequency, for \\(a\\in\\mathbb{R}\\)\n\n\\[\n\\mathrm{e}^{\\mathrm{i} a t} \\mathcal{F}\\{f(t)\\} = \\hat{f}(\\omega - a).\n\\]\n\nIf \\(f\\) is even or odd, than \\(\\hat{f}\\) is even or odd, respectively.\nDerivative in time \\[\n\\mathcal{F}\\{\\partial_t f(t)\\} = \\mathrm{i} \\omega \\hat{f}(\\omega)\n\\] We are going to prove this by going through the lines \\[\n\\begin{align}\n\\mathcal{F}\\left\\{\\frac{d}{d\\,t}f(t)\\right\\} &= \\int_{-\\infty}^\\infty f'(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\left[f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^\\infty -\\mathrm{i} \\omega f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\mathrm{i} \\omega \\int_{-\\infty}^\\infty f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\mathrm{i} \\omega \\mathcal{F}\\{f(t)\\}\n\\end{align}\n\\] For higher derivatives we get \\[\n\\mathcal{F}\\{\\partial_t^n f(t)\\} = \\mathrm{i}^n \\omega^n \\hat{f}(\\omega)\n\\]\nDerivative in frequency \\[\n\\mathcal{F}\\{t^n f(t)\\} = \\mathrm{i}^n \\partial_\\omega^n\\hat{f}(\\omega)\n\\]\nThe convolution of two functions is defined as \\[\n(f \\ast g)(t) = \\int_{-\\infty}^{\\infty}f(t - \\xi) g(\\xi)\\, \\mathrm{d}\\xi,\n\\] and for the Fourier transform \\[\n\\mathcal{F}\\{(f \\ast g)(t)\\} = \\hat{f} \\cdot \\hat{g}.\n\\]\nParseval’s Theorem \\[\n\\|f\\|_2^2 = \\int_{-\\infty}^{\\infty}|f(t)|^2\\, \\mathrm{d}t = \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty}|\\hat{f}(\\omega)|^2\\, \\mathrm{d}\\omega\n\\] stating that the Fourier Transform preserves the 2-norm up to a scaling factor.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#discrete-fourier-transform",
    "href": "signal/fourier.html#discrete-fourier-transform",
    "title": "9  Fourier Transform",
    "section": "9.3 Discrete Fourier Transform",
    "text": "9.3 Discrete Fourier Transform\nThe Discrete Fourier Transform (DFT) is a way of approximating the Fourier transform on discrete vectors of data and it essentially a discretized version of the Fourier transform by sampling the function and numerical integration.\n\n\n\n\n\n\n\nDefinition 9.6 (Discrete-Fourier Transform) For equally spaced values \\(t_k = k\\Delta t\\), for \\(k\\in\\mathbb{Z}\\) and \\(\\Delta t&gt;0\\) and the discrete values of the function evaluations \\(f_k=f(t_k)\\). If the function is periodic with \\(L=N\\Delta t\\) than the discrete Fourier transform is given as \\[\n\\hat{f}_k = \\sum_{j=0}^{N-1}f_j\\, \\mathrm{e}^{-\\mathrm{i} j k\\tfrac{2 \\pi}{N}},\n\\tag{9.3}\\] and its inverse (iDFT) as \\[\nf_k = \\frac{1}{N}\\sum_{j=0}^{N-1}\\hat{f}_j\\, \\mathrm{e}^{\\mathrm{i} j k\\tfrac{2 \\pi}{N}}.\n\\]\n\n\n\n\nAs we can see, the DFT is a linear operator and therefore it can be written as a matrix vector product \\[\n\\left[\n    \\begin{array}{c} \\hat{f}_1 \\\\ \\hat{f}_2 \\\\ \\hat{f}_3 \\\\ \\vdots \\\\ \\hat{f}_N \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{ccccc} 1 & 1 & 1 & \\dots & 1 \\\\\n                         1 & \\omega_N & \\omega_N^2 & \\dots & \\omega_N^{N-1} \\\\  \n                         1 & \\omega_N^2 & \\omega_N^4 & \\dots & \\omega_N^{2(N-1)} \\\\\n                         \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                         1 & \\omega_N^{N-1} & \\omega_N^{2(N-1)} & \\dots & \\omega_N^{(N-1)^2 } \\\\\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} f_1 \\\\ f_2 \\\\ f_3 \\\\ \\vdots \\\\ f_N \\end{array}\n\\right]\n\\tag{9.4}\\] with \\(\\omega_N = \\exp({-\\mathrm{i} \\tfrac{2 \\pi}{N}})\\).\n\n\n\n\n\n\nNote\n\n\n\nThe matrix of the DFT is a unitary Vandermonde matrix.\n\n\nAs we can transfer the properties of the Fourier transform to the DFT we get the nice properties for sampled signals.\nThe downside of the DFT is that it does not scale well for large \\(N\\) as the matrix-vector multiplication is \\(\\mathcal{O}(N^2)\\) and becomes slow.\n\n\nShow code for the computation of the DFT matrix.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nN = 256\nw = np.exp(-1j * 2 * np.pi / N )\n\nJ, K = np.meshgrid(np.arange(N), np.arange(N))\nDFT = np.power(w, J*K)",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fast-fourier-transform",
    "href": "signal/fourier.html#fast-fourier-transform",
    "title": "9  Fourier Transform",
    "section": "9.4 Fast Fourier Transform",
    "text": "9.4 Fast Fourier Transform\nIn 1965, James W. Cooley (IBM) and John W. Tukey (Princeton) developed the so called fast Fourier transform (FFT) that scales with \\(\\mathcal{O}(N \\log(N))\\). which becomes almost linear for large enough \\(N\\), see Cooley and Tukey (1965).\n\n\n\n\n\n\nNote\n\n\n\nTo give an idea of what this change means and why this algorithm was a game changer. Audio is most of the time sampled with 44.1kHz, i.e. 44 100 samples per second. For a 10s audio clip the vector \\(f\\) will have the length \\(N = 4.41 \\times 10^5\\). The DFT computation (without generating the matrix) results ins approximately \\(2\\times 10^{11}\\) multiplications. The FFT on the other hand requires \\(6\\times 10^6\\) leading to a speed-up of about \\(30 000\\).\nHow this influenced our world we know from the use in our daily communication networks.\n(Compare Brunton and Kutz 2022, 65–66)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe should note that Cooley and Tukey where not the first to propose a FFT but the provided the formulation used today. Gauss already formulated the FFT 150 years earlier in 1805 for orbital approximations. Apparently, he did the necessary computations in his head and needed a fast algorithm so he developed the FFT. Gauss being Gauss did not see this as something important and it did not get published until 1866 in his compiled notes, Gauß (1866).\n\n\nThe main idea of the FFT is to exploit symmetries in the Fourier transform and to relate the \\(N\\)-dimensional DFT to a lower dimensional DFT by reordering the coefficients.\n\n\n\n\n\n\n\nDefinition 9.7 (Fast-Fourier Transform) If we assume that \\(N = 2^n\\), i.e. a power of \\(2\\), in particular \\(N=2M\\), and \\(F_N\\) denotes the matrix of Equation 9.4 for dimension \\(N\\) and we have \\(\\hat{f} = F_N f\\) and \\(f = \\tfrac1N \\overline{F_N} \\hat{f}\\). By splitting \\(f\\) in the even and odd indices as \\[e = [f_0, f_2, \\ldots, f_{N-2}]^{\\mathrm{T}}\\in \\mathbb{C}^{M}\\] and \\[o = [f_1, f_3, \\ldots, f_{N-1}]^{\\mathrm{T}}\\in \\mathbb{C}^{M}\\] and for Equation 9.4 we get \\[\n\\begin{align}\n\\hat{f}_k &= \\sum_{j=0}^{N-1}f_j\\, \\omega^{j k} = \\sum_{j=0}^{M-1}f_{2j}\\, \\omega^{(2j) k} + \\sum_{j=0}^{M-1}f_{2j+1}\\, \\omega^{(2j+1) k} \\\\\n&= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} + \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k}.\n\\end{align}\n\\] If we further split \\(\\hat{f}\\) in an upper and lower part \\[u = [\\hat{f}_0, \\hat{f}_2, \\ldots, \\hat{f}_{M-1}]^{\\mathrm{T}}\\in \\mathbb{C}^{M}\\] and \\[l = [\\hat{f}_{M}, \\hat{f}_{M+1}, \\ldots, \\hat{f}_{N-1}]^{\\mathrm{T}}\\in \\mathbb{C}^{M}\\] and with the property \\(\\omega^{k+M} = \\omega^k \\omega^M = - \\omega^k\\) we get\n\\[\n\\begin{align}\nu_k &= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} + \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k},\\\\\nl_k &= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} - \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k}.\n\\end{align}\n\\] This results in the more visual matrix representation \\[\n\\hat{f}= F_N f =\n\\left[\n    \\begin{array}{cc}\n    I_M & D_M \\\\\n    I_M & -D_M\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{cc}\n    F_M & 0 \\\\\n    0 & F_M\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{cc}\n    f_{even}\\\\\n    f_{odd}\n    \\end{array}\n\\right],\n\\] for \\(I_M\\) being the identity matrix in dimension \\(M\\) and \\[\nD_M =\n\\left[\n    \\begin{array}{ccccc} 1 & 0 & 0 & \\dots & 0 \\\\\n                         0 & \\omega & 0 & \\dots & 0 \\\\  \n                         0 & 0 & \\omega^2 & \\dots & 0 \\\\\n                         \\vdots & \\vdots & \\ddots & \\ddots &\\vdots\\\\\n                         0 & 0 & 0 & \\dots & \\omega^{(M-1)} \\\\\n    \\end{array}\n\\right]\n\\] Now repeat this \\(n\\) times.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(N\\) is not a power of \\(2\\) padding is used to make the size fit by extending the vector with zeros.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe original FFT paper (Cooley and Tukey 1965) uses bit flipping and similar techniques to boost performance even more. It can even be implemented to allow for in place computation to save storage.\nIMAGE\n(Compare Meyberg and Vachenauer 1992, 331)\n\n\n\n9.4.1 Examples for the FFT in action\nIn order to give an idea how FFT works in an application we follow the examples given in (Brunton and Kutz 2022, 66–76).\n\n\n\n\n\n\n\nExample 9.2 (FFT for de-noising) For a signal consisting of two main frequencies \\(f_1 = 50\\) and \\(f_2=120\\) we construct a signal \\[\nf(t) = \\sin(2\\pi f_1 t) + \\sin(2\\pi f_2 t)\n\\] and add some Gaussian white noise np.random.randn.\nWe compute the FFT from the two signals and their power spectral density (PSD), i.e.  \\[\nPSD(\\hat{f})=\\frac1N \\|\\hat{f}\\|^2.\n\\] We use the PSD to take all frequencies with a \\(PSD &lt; 100\\) out of our reconstruction as a filter. This removes noise from the signal.\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)\n\nf_clean = fun(t)\nf_noise = fun(t) + 2.5 * np.random.randn(len(t))              # Add some noise\n\nfhat_noise = np.fft.fft(f_noise)\nfhat_clean = np.fft.fft(f_clean)\n\nPSD_noise = np.abs(fhat_noise)**2 / N\nPSD_clean = np.abs(fhat_clean)**2 / N\n\nfreq = (1 / (dt * N)) * np.arange(N)\nL = np.arange(1, np.floor(N/4), dtype='int')\n\n# Apply filter in spectral space\nfilter = PSD_noise &gt; 100\nPSDclean = PSD_noise * filter\nfhat_filtered = filter * fhat_noise\nf_filtered = np.fft.ifft(fhat_filtered)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_noise, \":\", label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.figure(1)\nplt.plot(freq[L], PSD_noise[L], label=r\"Noisy\")\nplt.plot(freq[L], PSD_clean[L], label=r\"Clean\")\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"PSD\")\nplt.xlim(0, int(freq[L[-1] + 1]))\nplt.legend(loc=1)\nplt.gca().set_aspect(1)\n\nplt.figure(3)\nplt.plot(t, np.real(f_filtered), label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original clean signal and noisy signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaled square norm of of the Fourier coefficients (PSD), only parts are shown.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Original signal and de-noised signal.\n\n\n\n\n\n\n\nFigure 9.2: Signal noise filter with FFT.\n\n\n\nAs can be seen in the Figure 9.2 (c), the reconstruction is not exact. This is due to the fact that the reconstructed frequencies are not matched exactly plus we have some multiples that show up as well. In particular:\n\n\n\n\n\n\n\n\n\nFrequency\nPSD\n\n\n\n\n0\n52.0\n145.259\n\n\n1\n120.0\n230.273\n\n\n2\n3976.0\n230.273\n\n\n3\n4044.0\n145.259\n\n\n\n\n\n\n\nNote: For Figure 9.2 (c) we discarded the imaginary part of the reconstruction.\n\n\n\n\n\n\n\n\n\n\n Exercise - Self implementation Example 9.2\n\n\n\n\n\nImplement the code yourself by filling out the missing sections:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = # specify the sample steps for t\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: # define the function as described in the example\n\nf_clean = fun(t)\nf_noise = fun(t) + 2.5 * np.random.randn(len(t))\n\nfhat_noise = # transform the noisy signal with fft\nfhat_clean = # transform the clean signal with fft\n\nPSD_noise = # Compute the PSD for the noisy signal \nPSD_clean = # Compute the PSD for the clean signal \n\nfreq = (1 / (dt * N)) * np.arange(N)\nL = np.arange(1, np.floor(N/4), dtype='int')\n\n# Apply filter in spectral space\nfilter = # create the filter for the PSD values grater 100 \nPSDclean = # set everything not part of the filter to zero\nfhat_filtered = # apply the filter to the transformed function\nf_filtered = # apply the inverse fft\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_noise, \":\", label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.figure(1)\nplt.plot(freq[L], PSD_noise[L], label=r\"Noisy\")\nplt.plot(freq[L], PSD_clean[L], label=r\"Clean\")\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"PSD\")\nplt.xlim(0, int(freq[L[-1] + 1]))\nplt.legend(loc=1)\nplt.gca().set_aspect(1)\n\nplt.figure(3)\nplt.plot(t, np.real(f_filtered), label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n Exercise - DFT vs. FFT\n\n\n\n\n\nImplement Example 9.2 with DFT and FFT such that you can evaluate the runtime and create a plot showing the different runtime as well as check if the two produce the same result\n\n\n\nFor the Fourier transform we stated that the multiplication with \\(\\mathcal{F}\\{\\partial f\\}=\\mathrm{i}\\omega\\mathcal{F}\\{f\\}\\), similarly we can derive a formula for the numerical derivative of a sampled function by multiplying each entry of the transformed vector by \\(\\mathrm{i}\\kappa\\) for \\(\\kappa=\\tfrac{2\\pi k}{N}\\). \\(\\kappa\\) is called the discrete wavenumber associated with the component \\(k\\).\nLet us explore this with the example stated in (Brunton and Kutz 2022, 68–69).\n\n\n\n\n\n\n\nExample 9.3 (Spectral derivative) We compute the so called spectral derivative for the function \\[\n\\begin{align}\nf(t) &= \\cos(t) \\exp\\left(-\\frac{t^2}{25}\\right) \\\\\n\\partial_t f(t) &= -\\sin(t) \\exp\\left(-\\frac{t^2}{25}\\right) - \\frac{2}{25}t f(t)\n\\end{align}\n\\]\nIn order to porovide something to compare our results to we also compute the forward Euler finite-differences for the derivative \\[\n\\partial_t f(t_k) \\approx \\frac{f(t_{k+1}) - f(t_k)}{t_{k+1} - t_k}.\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n# Parameters\nN = 128\na, b = -15, 15\nL = b - a\n\nfun = lambda t: np.cos(t) * np.exp(-np.power(t, 2) / 25)\ndfun = lambda t: -(np.sin(t) * np.exp(-np.power(t, 2) / 25) + (2 / 25) * t * fun(t))\n\ndef fD(N, fun, dfun, a, b):\n    t = np.linspace(a, b, N, endpoint=False)\n    dt = t[1] - t[0]\n    f = fun(t)\n    df_DD = np.diff(f) / dt\n    df_DD = np.append(df_DD, (f[-1] - f[0]) / dt)\n    return df_DD, np.linalg.norm(df_DD - dfun(t)) / np.linalg.norm(df_DD)\n\ndef spD(N, fun, dfun, a, b):\n    t = np.linspace(a, b, N, endpoint=False)\n    f = fun(t)\n    fhat = np.fft.fft(f)\n    kappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n    df_hat = kappa * fhat * (1j)\n    df_r = np.fft.ifft(df_hat).real\n    return df_r, np.linalg.norm(df_r - dfun(t)) / np.linalg.norm(df_r)\n\n\n# Finite differences\ndf_fD, e = fD(N, fun, dfun, a, b)\n# Spectral derivative\ndf_spD, e = spD(N, fun, dfun, a, b)\n\n# Figures\nt = np.linspace(a, b, N, endpoint=False)\nplt.figure(0)\nplt.plot(t, dfun(t), label=\"Exact\")\nplt.plot(t, df_fD, \"-.\", label=\"Finite Differences\")\nplt.plot(t, df_spD, \"--\", label=\"Spectral\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\partial_t f$\")\nplt.legend(loc=1)\nplt.gca().set_aspect(5)\n\nplt.figure(1)\nn = 19\nM = range(3, n)\ne_spD = np.ones(len(M))\ne_fD = np.ones(len(M))\nfor i, j in enumerate(M):\n    _, e_fD[i] = fD(2**j, fun, dfun, a, b)\n    _, e_spD[i] = spD(2**j, fun, dfun, a, b)\n\nplt.loglog(np.pow(2, M), e_fD, label=\"Finite differences\")\nplt.loglog(np.pow(2, M), e_spD, label=\"Spectral derivative\")\nplt.grid()\nplt.xlabel(\"N\")\nplt.ylabel(\"Relative Error\")\nplt.legend(loc=1)\nplt.gca().set_aspect(2.5e-1)\n\nfun_saw = lambda t, L: 0 if abs(t) &gt; L / 4 else (1 - np.sign(t) * t * 4 / L)\na, b = -np.pi, np.pi\nL = b - a\nfun2 = lambda t: np.fromiter(map(lambda t: fun_saw(t, L), t), t.dtype)\nN = 2**10\n\nt = np.linspace(a, b, N, endpoint=False)\n\nplt.figure(2)\ndf_spD, _ = spD(N, fun2, fun2, a, b)\ndf_fD, _ = fD(N, fun2, fun2, a, b)\nplt.plot(t, fun2(t), label=r\"$f$\")\nplt.plot(t, df_fD, \"-.\", label=\"Finite derivative\")\nplt.plot(t, df_spD, \"--\", label=\"Spectral derivative\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$f, \\partial_t f$\")\nplt.xlim(-2, 2)\nplt.legend(loc=1)\nplt.gca().set_aspect(0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Computation of the derivative with different methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Accuracy of the methods for computing the derivative\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Gibbs phenomenon for the spectral derivative for discontinuous functions\n\n\n\n\n\n\n\nFigure 9.3: Computing the derivative of a function\n\n\n\nAs can be seen in Figure 9.3 (b) we can reduce the error of both methods by increasing \\(N\\). Nevertheless, the spectral method is more accurate and converges faster.\n\n\n\n\nIn Section 4.1.2 we have already seen how the eigendecomposition can be used to compute the solution of ordinary differential equations by changing the basis and transforming the equation into a basis that can be handled easy. The same is true with the Fourier transformation.\nAs mentioned before, Fourier introduced it to solve the heat equation and we will do the same.\n\n\n\n\n\n\nCaution\n\n\n\nAbove we have always transformed the time component of a function into the frequency domain via FFT. In the next example we have the time evolution of a signal and therefore the function depends on time and space. We apply the FFT to the space component to compute the derivative in frequency domain w.r.t. the transformed variable.\n\n\n\n\n\n\n\n\n\nExample 9.4 (Heat Equation) The heat equation in 1D is given by \\[\n\\dot{u}(t, x) = \\alpha^2 \\partial_x^2 u(\\tau, x),\n\\] for \\(u(t, x)\\) as the temperature distribution in space (\\(x\\)) and time (\\(t\\)). By applying the Fourier transform we get \\(\\mathcal{F}\\{u(t,x)\\}=\\hat{u}(t, \\omega)\\) and \\[\n\\dot{\\hat{u}}(t, \\omega) = - \\alpha^2\\omega^2\\hat{u}(t, \\omega).\n\\] This transformed the partial differential equation into an ordinary differential equation and we can solve it for each fixed frequency \\(\\omega\\) as \\[\n\\hat{u}(t, \\omega) = \\mathrm{e}^{-\\alpha^2\\omega^2 t} \\hat{u}(0, \\omega).\n\\] where \\(\\hat{u}(0, \\omega)\\) is nothing else than the Fourier transform of the initial temperature distribution at time \\(t=0\\). To compute the inverse Fourier transform we can make use of the convolution property introduced above.\n\\[\n\\begin{align}\nu(t, x) &= \\mathcal{F}^{-1}\\{\\hat{u}(t, \\omega)\\} = \\mathcal{F}^{-1}\\{\\mathrm{e}^{-\\alpha^2\\omega^2 t}\\} \\ast u(0, x) \\\\\n&= \\frac{1}{2 \\alpha \\sqrt{\\pi t}} \\mathrm{e}^{-\\tfrac{x^2}{4\\alpha^2t}} \\ast u(0, x).\n\\end{align}\n\\]\nFor the numerical computation it is more convenient to stay in the frequency domain and use \\(\\kappa\\) as before.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nalpha = 1\na, b = -50, 50\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = np.zeros_like(x)\nu0[np.abs(x) &lt; 10] = 1\nu0hat = np.fft.fft(u0)\n\n# Simulate in Fourier frequency domain\ndt = 0.001\nT = np.arange(0, 10, dt)\n\nfun = lambda t, x: - alpha**2 * (np.power(kappa, 2)) * x\neuler = lambda y, dt, t, fun: y + dt * fun(t, y)\n\nuhat = np.zeros((len(T), len(u0hat)), dtype=\"complex\")\nuhat[0, :] = u0hat\nfor i, t in enumerate(T[1:]):\n    uhat[i + 1, :] = euler(uhat[i, :], dt, t, fun)\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k, :])\n\nu = u.real    \n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\n#ax.view_init(elev=45, azim=-20, roll=0)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:int(1 / dt), :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u[0:-1:100]), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of the heat equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of the transport equation.\n\n\n\n\n\n\n\nFigure 9.4: Simulation of the heat equation in 1D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Self implementation Example 9.4\n\n\n\n\n\nImplement the code yourself by filling out the missing sections:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nalpha = 1\na, b = -50, 50\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = np.zeros_like(x)\nu0[np.abs(x) &lt; 10] = 1\nu0hat = np.fft.fft(u0)\n\n# Simulate in Fourier frequency domain\ndt = 0.001\nT = np.arange(0, 10, dt)\n\nfun = lambda t, x: #right hand side for the euler function\neuler = lambda y, dt, t, fun: # euler step\n\nuhat = np.zeros((len(T), len(u0hat)), dtype=\"complex\")\nuhat[0, :] = u0hat\nfor i, t in enumerate(T[1:]):\n    uhat[i + 1, :] = euler(uhat[i, :], dt, t, fun)\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k,:])\n\nu = u.real    \n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:int(1 / dt), :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u[0:-1:100]), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\nNext, we look at the transport equation.\n\n\n\n\n\n\n\nExample 9.5 (Transport or advection) The transport equation in 1D is given by \\[\n\\dot{u} = - c \\partial_x u.\n\\] It is called the transport equation, as the initial value simply propagates in time to the right hand side of the domain with speed \\(c\\), i.e. \\(u(t, x) = u(0, x -c t)\\). The solution is simply computed by exchanging the right hand side from the above code.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nc = 2\na, b = -20, 20\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = 1/np.cosh(x)\nu0hat = np.fft.fft(u0)\n\n# SciPy's odeint function doesn't play well with complex numbers, so we recast \n# the state u0hat from an N-element complex vector to a 2N-element real vector\nu0hat_ri = np.concatenate((u0hat.real, u0hat.imag))\n\n# Simulate in Fourier frequency domain\ndt = 0.025\nT = np.arange(0, 600*dt, dt)\n\ndef rhsWave(uhat_ri,t,kappa,c):\n    uhat = uhat_ri[:N] + (1j) * uhat_ri[N:]\n    d_uhat = -c * (1j ) *kappa * uhat\n    d_uhat_ri = np.concatenate((d_uhat.real, d_uhat.imag)).astype('float64')\n    return d_uhat_ri\n\nuhat_ri = odeint(rhsWave, u0hat_ri, T, args=(kappa, c))\n\nuhat = uhat_ri[:, :N] + (1j) * uhat_ri[:, N:]\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k, :])\n\nu = u.real    \n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\nu_plot = u[0:-1:60, :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u), aspect=1.5)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of the transport equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of the transport equation.\n\n\n\n\n\n\n\nFigure 9.5: Computing the derivative of a function\n\n\n\n\n\n\n\nTo increase complexity from these simple equations we move to the nonlinear Burger’s equation which is a combination of the two above and an example that produces shock waves in fluids.\n\n\n\n\n\n\n\nExample 9.6 (Burger’s equation) Burger’s equation in 1D is given by \\[\n\\dot{u} + u \\partial_x u = \\nu \\partial_x^2 u.\n\\] The equation consists of a nonlinear convection part \\(u \\partial_t u\\) as well as diffusion. The convection part is designed in such a way that larger amplitudes travel faster and therefore causes a shock wave to form.\nRegarding the solution of this equation, the interesting part is that we need to map back and forth between the Fourier space and time space to apply the nonlinearity.\nThis example is also very useful to test the capabilities of your solver as by decreasing the diffusion factor further and further we can approach an infinitely steep shock and in theory it can break like an ocean wave.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnu = 0.001\na, b = -10, 10\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = 1/np.cosh(x)\n\n# Simulate in Fourier frequency domain\ndt = 0.025\nT = np.arange(0,100*dt,dt)\n\ndef rhsBurgers(u, t, kappa, nu):\n    uhat = np.fft.fft(u)\n    d_uhat = (1j) * kappa * uhat\n    dd_uhat = -np.power(kappa, 2) * uhat\n    d_u = np.fft.ifft(d_uhat)\n    dd_u = np.fft.ifft(dd_uhat)\n    du_dt = -u * d_u + nu * dd_u\n    return du_dt.real\n\nu = odeint(rhsBurgers, u0, T, args=(kappa, nu))\n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:10, :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of Burger’s equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of Burger’s equation.\n\n\n\n\n\n\n\nFigure 9.6: Time integration of the Burger’s equation\n\n\n\n\n\n\n\nThis concludes our example with the FFT.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#gabor-transform",
    "href": "signal/fourier.html#gabor-transform",
    "title": "9  Fourier Transform",
    "section": "9.5 Gabor Transform",
    "text": "9.5 Gabor Transform\nThe Fourier transform is computed as soon as all the values of a signal are sampled. It provides spectral information over the entire signal and not when in time certain frequencies occur, i.e. no temporal information. The Fourier transform can only characterize periodic and stationary signals. The time component is used during the integration and no longer present in the transformed signal.\nIf both is needed in order to generate a spectrogram (plot frequency versus time), the Gabor transform brings remedy. Technically it is the Fourier transform of the signal masked by a sliding window \\[\n\\mathcal{G}\\{f\\}(t, \\omega) = G_f(t, \\omega) = \\int_{-\\infty}^\\infty f(\\tau) \\mathrm{e}^{-\\mathrm{i}\\omega\\tau}\\overline{g}(\\tau - t)\\, \\mathrm{d}\\tau.\n\\] for the so called kernel \\(g(t)\\) a Gaussian bell curve is often used \\[\ng(t - \\tau) =  \\mathrm{e}^{-\\tfrac{(t-\\tau)^2}{a^2}}.\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)\n\ngauss = lambda x, a: np.exp(-np.pow(x, 2) / a**2)\n\nf_clean = fun(t) + np.random.randn(len(t))  \ngauss = gauss(t - 0.125, 0.025) * 4.8\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_clean, label=\"Signal\")\nplt.plot(t, gauss, label=r\"$g(t-\\tau)$\")\n#plt.xlabel(\"Time [s]\")\n#plt.ylabel(r\"$f$\")\nplt.xticks([0.1, 0.125, 0.15], [r\"$\\tau - a$\", r\"$\\tau$\", r\"$\\tau + a$\"])\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.yticks([])\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9.7: Signal with Gaussian kernel\n\n\n\n\n\nFigure 9.7 illustrates the sliding time window with the spread as \\(a\\) and the center \\(\\tau\\). The inverse is given as \\[\nf(t) = \\mathcal{G}^{-1}\\{G_f(t, \\omega)\\} = \\frac{1}{2 \\pi \\|g\\|^2} = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty G_f(\\tau, \\omega) \\mathrm{e}^{\\mathrm{i}\\omega\\tau}\\overline{g}(t - \\tau)\\, \\mathrm{d}\\omega\\, \\mathrm{d}t.\n\\]\nAs with the Fourier transform the application of the Gabor transform is usually done in its discrete form.\n\n\n\n\n\n\n\nDefinition 9.8 (Discrete Gabor Transform) For discrete time \\(\\tau = k \\Delta t\\) and frequencies \\(\\nu = j \\Delta \\omega\\) the discretized kernel function has the form \\[\ng_{j,k} = \\mathrm{e}^{{\\mathrm{i}2 \\pi j\\Delta\\omega t}} g(t- k\\Delta t)\n\\] and the discrete Gabor transform \\[\n\\mathcal{G}\\{f\\}_{j,k} = \\langle f, g_{j,k} \\rangle = \\int_{-\\infty}^\\infty f(\\tau)\\overline{g}_{j,k}(\\tau)\\, \\mathrm{d}\\tau.\n\\]\n\n\n\n\nThe following example is taken from (Brunton and Kutz 2022, 77–78).\n\n\n\n\n\n\n\nExample 9.7 (Gabor transform for a quadratic chirp) As example we use an oscillating cosine function where the frequency of the oscillation increases as a quadratic function in time: \\[\nf(t) = \\cos(2\\pi\\, t\\, h(t)), \\quad \\text{with}\\quad h(t) = h_0 + (h_1 - h_0)\\frac{t}{3 t_1^2},\n\\] where the frequency shifts from \\(h_0\\) to \\(h_1\\) between \\(t=0\\) and \\(t=t_1\\).\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.9).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 2048\na, b = 0, 2\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nh0 = 50\nh1 = 250\nt1 = 2\nx = np.cos(2 * np.pi * t * (h0 + (h1 - h0) * np.power(t,2) / (3 * t1**2)))\n\nplt.figure(0)\nN = len(x)\nfreq = (1 / (dt * N)) * np.arange(N)\nPSD_noise = np.abs(np.fft.fft(x))**2 / N\nplt.plot(freq[:N//2], PSD_noise[:N//2])\nplt.xlabel(\"Frequency [Hz]\")\nplt.gca().set_aspect(10)\n\nplt.figure(1)\nplt.specgram(x, NFFT=128, Fs=1/dt, noverlap=120, cmap='jet')\nplt.colorbar()\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Frequency [Hz]\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Power spectral density of the signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Spectrogram.\n\n\n\n\n\n\n\nFigure 9.8: Quadratic chirp signal\n\n\n\nWe can see a clear peak at 50Hz but no information of time is given, where else in the spectrogram we see how the frequency progresses in time.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the analysis of time-frequency we see Heisenberg’s uncertainty principle at work. We can not attain high resolution simultaneously in both time and frequency domain. In the time domain we have perfect resolution but no information about the frequencies, in the Fourier domain the time information is not present.\nThe spectrogram resolves both but with reduced resolution. This effect manifests differently at different locations of the chirp signal. At the left where the frequency is almost constant, the band becomes narrower, whereas at the right it becomes wider due to the increased blur in horizontal direction. The product of the uncertainties of these quantities has a minimal value. In this context time and frequency domain respectively represent the extremes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is also the Laplace transform that can be understood as a generalized Fourier transform. This transform is also heavily used in the transformation of PDEs to simpler ODEs or ODEs into algebraic equations.\nNevertheless, we do not cover this transform here, for the interested reader we recommend (Brunton and Kutz 2022, sec. 2.5) as it introduces the Laplace transform with the tools we covered here.\n\n\nThe Fourier transform always assumes a harmonic nature in the signal it is applied to. Therefore, it is best suited for e.g. music, rotating machines or vibrations. For other signals, especially with discontinuous parts (Gibbs phenomenon) there are more adequate bases functions. We look at these in the next section.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nCooley, James W., and John W. Tukey. 1965. “An Algorithm for the Machine Calculation of Complex Fourier Series.” Mathematics of Computation 19 (90): 297–301. https://doi.org/10.1090/s0025-5718-1965-0178586-1.\n\n\nGauß, Carl Friedrich. 1866. “Theoria Interpolationis Methodo Nova Tractata.” Göttingen: Königliche Gesellschaft der Wissenschaften.\n\n\nMeyberg, Kurt, and Peter Vachenauer. 1992. Höhere Mathematik 2. Springer-Lehrbuch. New York, NY: Springer.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/wavelet.html",
    "href": "signal/wavelet.html",
    "title": "10  Wavelet transform",
    "section": "",
    "text": "With Wavelets we extend the concept of the Fourier analysis to general orthogonal bases. This extensions is done in such a way that we can do a multi-resolution decomposition and thus partially overcome the uncertainty principal discussed before.\nWavelets are both, local and orthogonal. The whole family of a wavelet are generated by scaling and translating a mother wavelet \\(\\psi(t)\\) as \\[\n\\psi_{a,b}(t) = \\frac{1}{\\sqrt{a}} \\psi\\left(\\frac{t-b}{a}\\right),\n\\] where the parameters \\(a\\) and \\(b\\) are responsible for scaling and translating, respectively.\nThe simplest example is the so called Haar wavelet.\n\n\n\n\n\n\n\nExample 10.1 (The Haar wavelet) The mother wavelet is defined as the step function \\[\n\\psi(t) =\n\\begin{cases}\n\\begin{array}{rl}\n1, & \\text{for}\\, 0 \\leq t \\le \\tfrac12\\\\\n-1, & \\text{for}\\, \\tfrac12 \\leq t \\le 1\\\\\n0, & \\text{else}\n\\end{array}\n\\end{cases}\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 1000\na, b = -0.25, 1.25\nt = np.linspace(a, b, N, endpoint=False)\nmother = lambda t: np.astype((t &gt;= 0) & (t &lt; 1/2), float) - np.astype((t &gt;= 1/2) & (t &lt; 1), float)\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a)\n\nplt.figure(0)\nplt.plot(t, psi(t, 1, 0))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{1,0}$\")\nplt.gca().set_aspect(0.125)\n\nplt.figure(1)\nplt.plot(t, psi(t, 1/2, 0))\nplt.xlabel(\"x\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, 0}$\")\nplt.gca().set_aspect(0.125)\n\nplt.figure(2)\nplt.plot(t, psi(t, 1/2, 1/2))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, \\frac{1}{2}}$\")\nplt.gca().set_aspect(0.125)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scaling 1 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaling 1/2 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scaling 1/2 and translation 1/2.\n\n\n\n\n\n\n\nFigure 10.1: Haar wavelets for the first two levels of multi resolution.\n\n\n\nNote that the Haar wavelets are orthogonal and provide a hierarchical basis for a signal.\n\n\n\n\n\n\n\n\n\n\n\nExample 10.2 (The Maxican hat wavelet) The mother wavelet is defined as the is the negative normalized second derivative of a Gaussian function, \\[\n\\psi(t) = (1 - t)^2\\, \\mathrm{e}^{-\\tfrac{t}{2}}\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 1000\na, b = -5, 5\nt = np.linspace(a, b, N, endpoint=False)\nmother = lambda t, d: (1 - np.pow(t, 2)) * np.exp(-1/2 * np.pow(t, 2))\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a, 2)\n\nplt.figure(0)\nplt.plot(t, psi(t, 1, 0))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{1,0}$\")\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, psi(t, 1/2, 0))\nplt.xlabel(\"x\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, 0}$\")\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, psi(t, 1/2, 1/2))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, \\frac{1}{2}}$\")\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scaling 1 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaling 1/2 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scaling 1/2 and translation 1/2.\n\n\n\n\n\n\n\nFigure 10.2: Mexican hat wavelets for the first two levels of multi resolution.\n\n\n\n\n\n\n\nIf we have a wavelet \\(\\psi\\), we can generate a new wavelet by convolution \\[\n\\psi \\ast \\phi\n\\], for a bounded and integrable function \\(\\phi\\).\n\n\n\n\n\n\n\nDefinition 10.1 (Continuous Wavelet Transform) The continuous wavelet transform is given by \\[\n\\mathcal{W}_\\psi\\{f\\}(a, b) = \\langle f, \\psi_{a, b} \\rangle = \\int_{-\\infty}^\\infty f(t)\\overline{\\psi}_{a,b}\\, \\mathrm{d}t,\n\\] this is only true for a bounded wavelet \\(\\psi\\) \\[\nC_\\psi = \\int_{-\\infty}^\\infty \\frac{|\\hat{\\psi}(\\tau)|^2}{|\\tau|}\\, \\mathrm{d}\\tau,\n\\] i.e. a wavelet with \\(C_\\psi &lt; \\infty\\). In this case also the inverse transform exists and is defined as \\[\nf(t) = \\frac{1}{C_\\psi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty\\mathcal{W}_\\psi\\{f\\}(a, b)\\psi_{a, b}(t)\\frac{1}{a^2} \\, \\mathrm{d}a\\, \\mathrm{d}b,\n\\]\n\n\n\n\nFrom the continuous wavelet transform we go on to the discrete wavelet transform as similar for the Fourier transforms we have seen we will hardly ever have the entire signal at hand for the transformation.\n\n\n\n\n\n\n\nDefinition 10.2 (Discrete Wavelet Transform) The discrete wavelet transform is given by \\[\n\\mathcal{W}_\\psi\\{f\\}(j, k) = \\langle f, \\psi_{j, k} \\rangle = \\int_{-\\infty}^\\infty f(t)\\overline{\\psi}_{j, k}\\, \\mathrm{d}t,\n\\] where \\(\\psi_{j, k}\\) is a discrete family of wavelets \\[\n\\psi_{j, k}(t) = \\frac{1}{a^j}\\psi\\left(\\frac{t - k b}{a^j}\\right).\n\\] The inverse is than given by \\[\nf(t) = \\sum_{j,k = -\\infty}^\\infty \\mathcal{W}_\\psi\\{f\\}(j, k) \\psi_{j, k}(t).\n\\] Which is nothing else than expressing the function in the wavelet family. If this family of wavelets is orthogonal (as e.g. the Haar wavelet) it is possible to expand the function \\(f\\) uniquely as they form a basis.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere also exists a fast wavelet transform that reduces the computational complexity from \\(\\mathcal{O}(N\\log N)\\) to \\(\\mathcal{O}(N)\\) by cleverly reusing parts of the inner product computation.\n\n\n\n\n\n\n\n\n\nExample 10.3 (Signal analysis with the Haar wavelet) To start and get an idea how the analysis works we use an instructive example. We have a piecewise constant function as \\(v = [3, 1, 0, 4, 0, 6, 9, 9]^{\\mathrm{T}}\\) \\[\nf(x) = \\sum_{i=1}^{8} v_i \\chi_{[i-1, i)},\n\\] where \\(\\chi_{[c, d)}\\) is the indicator function that is 1 on the interval \\([c, d)\\) and zero everywhere else.\nNow let us proceed through the levels of the transform to see what is happening, we select \\(a=2\\) and \\(b=1\\):\n\\[\n\\mathcal{W}\\{f\\}(1, :) =\n\\left(\n                        \\begin{array}{c c c c c c c c}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0\\\\\n                            0&0&0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\\\\\n                            \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0\\\\\n                            0&0&0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}\\\\\n                        \\end{array}\n                    \\right)\n                    \\left[\n                        \\begin{array}{c}\n                            3\\\\\n                            1\\\\\n                            0\\\\\n                            4\\\\\n                            0\\\\\n                            6\\\\\n                            9\\\\\n                            9\n                        \\end{array}\n                    \\right]=\\left[\n                            \\begin{array}{c}\n                                2\\sqrt{2}\\\\\n                                2\\sqrt{2}\\\\\n                                3\\sqrt{2}\\\\\n                                9\\sqrt{2}\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\\\\\n                            \\end{array}\n                        \\right]\n\\]\nWe than apply the next level just for the first half \\[\n\\mathcal{W}\\{f\\}(2, :) =\n\\left(\n                        \\begin{array}{c c c c c c c c}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0\\\\\n                            \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&1&0&0&0\\\\\n                            0&0&0&0&0&1&0&0\\\\\n                            0&0&0&0&0&0&1&0\\\\\n                            0&0&0&0&0&0&0&1\\\\\n                        \\end{array}\n                    \\right)\n                    \\left[\n                            \\begin{array}{c}\n                                2\\sqrt{2}\\\\\n                                2\\sqrt{2}\\\\\n                                3\\sqrt{2}\\\\\n                                9\\sqrt{2}\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\\\\\n                            \\end{array}\n                        \\right]=\\left[\n                            \\begin{array}{c}\n                                4\\\\\n                                12\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\\\\\n                            \\end{array}\n                        \\right]\n\\] and our final step \\[\n\\mathcal{W}\\{f\\}(3, :) =\n\\left(\n                        \\begin{array}{c c c c c c c c}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                                \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                                0&0&1&0&0&0&0&0\\\\\n                                0&0&0&1&0&0&0&0\\\\\n                                0&0&0&0&1&0&0&0\\\\\n                                0&0&0&0&0&1&0&0\\\\\n                                0&0&0&0&0&0&1&0\\\\\n                                0&0&0&0&0&0&0&1\\\\\n                        \\end{array}\n                    \\right)\n                    \\left[\n                            \\begin{array}{c}\n                                4\\\\\n                                12\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\\\\\n                            \\end{array}\n                        \\right]=\\left[\n                            \\begin{array}{c}\n                                8\\sqrt{2}\\\\\n                                -4\\sqrt{2}\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\\\\\n                            \\end{array}\n                        \\right]\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pywt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\nN = 1000\na, b = 0, 8\nt = np.linspace(a, b, N, endpoint=False)\nv = np.array([3, 1, 0, 4, 0, 6, 9, 9])\nchi = lambda t, a, b: np.astype((t &gt;= a) & (t &lt; b), float)\nmother = lambda t: chi(t, 0, 1/2) - chi(1/2, 1)\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a, 2)\n\ndef f(t, v, spread=1):\n    y = np.zeros(t.shape)\n    for i, x in enumerate(v):\n        y += x * chi(t, i * spread, (i + 1) * spread)\n    return y\n\nX = np.zeros((4, len(v)))\nfor i in range(0, 4):\n    x = pywt.wavedec(v, wavelet=\"Haar\", level=i)\n    X[i, :] = np.concatenate(x) \n\nplt.figure(0)\nplt.plot(t, f(t, v), label=\"Signal\")\n#plt.plot(t, f(t, X[0, :], 1))\nplt.plot(t, f(t, X[1, 0:5], 2), label=\"Level 1\")\nplt.plot(t, f(t, X[2, 0:3], 4), label=\"Level 2\")\nplt.plot(t, f(t, X[3, 0:1], 8), label=\"Level 3\")\nplt.xlabel(\"t\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Scaling 1 and translation 0.\n\n\n\n\n\n\n\nMexican hat wavelets for the first two levels of multi resolution.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wavelet transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html",
    "href": "signal/twodim.html",
    "title": "11  Two-Dimensional Transform",
    "section": "",
    "text": "11.1 Fourier\nIf we apply FFT to a matrix \\(X\\in\\mathrm{R}^{m\\times n}\\) we can simply apply the 1D version to every row and than to every column of the resulting matrix. The other way round will produce the same final result.\nThis is shown in the code below but note we should use the more efficient np.fft.fft2.\nOf course we can use this to compress the image by removing small values from the transform.\nWe can also use the FFT for de-noising and filtering of signals. It is rather simple to just eliminate certain frequency bands in the frequency domain.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html#fourier",
    "href": "signal/twodim.html#fourier",
    "title": "11  Two-Dimensional Transform",
    "section": "",
    "text": "Show the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\n\nCshift = np.zeros_like(A, dtype='complex')\nC = np.zeros_like(A, dtype='complex')\nfor j in range(A.shape[0]):\n    C[j, :] = np.fft.fft(A[j, :])\n    Cshift[j,:] = np.fft.fftshift(np.copy(C[j, :]))\n\nRshift = np.zeros_like(A, dtype='complex')\nR = np.zeros_like(A, dtype='complex')\nD = np.zeros_like(C)\nfor j in range(A.shape[1]):\n    R[:, j] = np.fft.fft(A[:, j])\n    Rshift[:, j] = np.fft.fftshift(np.copy(R[:, j]))\n    D[:, j] = np.fft.fft(C[:, j])\n\nmyplot(A)\nmyplot(np.log(np.abs(Cshift)))\nmyplot(np.log(np.abs(Rshift)))\nmyplot(np.fft.fftshift(np.log(np.abs(D))))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Image after applying FFT on each individual row\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Image after applying FFT on each individual column\n\n\n\n\n\n\n\n\n\n\n\n(d) Row and column wise FFT (order does not matter)\n\n\n\n\n\n\n\nFigure 11.1: Image of MCI I and the row/column wise FFT.\n\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_fft = np.fft.fft2(A)\nA_fft_sort = np.sort(np.abs(A_fft.reshape(-1)))\nmyplot(A)\n\nfor c in (0.05, 0.01, 0.002):\n    thresh = A_fft_sort[int(np.floor((1 - c) * len(A_fft_sort)))]\n    A_fft_th = A_fft * (np.abs(A_fft) &gt; thresh)\n    A_th = np.fft.ifft2(A_fft_th).real\n    myplot(A_th)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) 5% of FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 1% of FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n(d) 0.2% of FFT coefficients\n\n\n\n\n\n\n\nFigure 11.2: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.\n\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_noise = A + (200 * np.random.randn(*A.shape)).astype('uint8')\nA_noise_fft = np.fft.fft2(A_noise)\nA_noise_fft_shift = np.fft.fftshift(A_noise_fft)\nF = np.log(np.abs(A_noise_fft_shift) + 1)\n\nmyplot(A_noise)\nmyplot(F)\n\nnx, ny = A.shape\nX, Y = np.meshgrid(np.arange(-ny/2 + 1, ny / 2 + 1), np.arange(-nx / 2 + 1, nx / 2 + 1))\nR2 = np.power(X, 2) + np.power(Y, 2)\nind = R2 &lt; 150**2\nA_noise_fft_shift_filter = A_noise_fft_shift * ind\nF_filter = np.log(np.abs(A_noise_fft_shift_filter) + 1)\n\nA_filter = np.fft.ifft2(np.fft.fftshift(A_noise_fft_shift_filter)).real\nmyplot(A_filter)\nmyplot(F_filter)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Noisy image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Noisy FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Filtered/De-noised image\n\n\n\n\n\n\n\n\n\n\n\n(d) Filtered/De-noised FFT coefficients\n\n\n\n\n\n\n\nFigure 11.3: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html#wavelet",
    "href": "signal/twodim.html#wavelet",
    "title": "11  Two-Dimensional Transform",
    "section": "11.2 Wavelet",
    "text": "11.2 Wavelet\nSimilar to the FFT also the Wavelet transform is used in much the same situations.\nBefore we go on and apply the wavelet transform in the same situations we show how the multi level approach looks like for an image. For the image we use the Daubechies 1 wavelets.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport pywt\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nn = 2\nA = rgb2gray(im)\ncoeffs = pywt.wavedec2(A, wavelet=\"db1\", level=n)\n\ncoeffs[0] /= np.abs(coeffs[0]).max()\narr, coeff_slices = pywt.coeffs_to_array(coeffs)\n\nplt.imshow(arr, cmap='gray', vmin=-0.25, vmax=0.75)\nplt.axis(\"off\")\nplt.gca().set_aspect(1)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11.4: First three levels of the discrete wavelet transform.\n\n\n\n\n\nOf course we can use this to compress the image by removing small values from the transform.\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport pywt\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nw = \"db1\"\nA = rgb2gray(im)\ncoeffs = pywt.wavedec2(A, wavelet=w, level=4)\n\ncoeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\nCsort = np.sort(np.abs(coeff_arr.reshape(-1)))\nmyplot(A)\nfor c in (0.05, 0.01, 0.002):\n    thresh = Csort[int(np.floor((1 - c) * len(Csort)))]\n    Cfilt = coeff_arr * (np.abs(coeff_arr) &gt; thresh)\n\n    coeffs_filt = pywt.array_to_coeffs(Cfilt, coeff_slices, output_format='wavedec2')\n    A_r = pywt.waverec2(coeffs_filt, wavelet=w)\n    myplot(A_r.astype('uint8'))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) 5% of wavelets\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 1% of wavelets\n\n\n\n\n\n\n\n\n\n\n\n(d) 0.2% of wavelets\n\n\n\n\n\n\n\nFigure 11.5: Image of MCI I and the reconstruction with various amounts of wavelets.\n\n\n\nFor ne-noising filters are applied or we and this is going to be the subject for a lecture more focused on image processing.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "sensing/sparsity.html",
    "href": "sensing/sparsity.html",
    "title": "12  Sparsity and Compression",
    "section": "",
    "text": "As we have seen before and maybe know from our own experience, most image and audio signals are highly compressible. Here compressible means we can find a basis that allows for a sparse reprehension of our signal. Let us put this into a small definition.\n\n\n\n\n\n\n\nDefinition 12.1 (\\(K\\)-sparse data) A compressible signal \\(x\\in \\mathbb{R}^n\\) may be written in a sparse vector \\(s\\in \\mathbb{R}^n\\) with a basis transformation (see Definition 1.8 for the formal definition) expressed by the matrix \\(B\\in\\mathbb{R}^{n\\times n}\\) and \\[\nx = B s.\n\\] The vector \\(s\\) is called \\(K\\)-sparse if it contains exactly \\(K\\) non-zero elements.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that this does not imply that \\(B\\) is sparse.\n\n\nIf the basis is generic such as the Fourier or Wavelet basis, i.e. we do not need to store the matrix \\(B\\) but can generate it on the spot, we only need to store the \\(K\\) non-zero elements of \\(s\\) to reconstruct the original signal.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we have seen in Chapter 11 images (and audio) signals are highly compressible in the Fourier and Wavelet basis with view most entries small or zero. By setting the small value to zero we reduce the density further without a high loss of quality.\nWe see this in JPEG compression for images and MP3 compression for audio signals. If we stream an audio signal on view an image on the web we only need to transfer \\(s\\) and not \\(x\\), saving bandwidth and storage as we go.\n\n\nWe have seen in Figure 5.1 that we can use the SVD to reduce the size as well. The downside here is that we need to store \\(U\\) and \\(V\\) (Definition 5.2) even if we reduce the rank. This is rather inefficient. On the other hand, we have used SVD in Section 5.2.2 with the Eigenfaces example how we can create a basis with SVD that can be used to classify an entire class of images - human faces. Storing the basis matrices in this case is comparable cheap and it allows us to use certain aspects of the downsampling for learning purposes.\nWe also need to stress that SVD and Fourier are unitary transformations which make the move into and from the basis cheep. This is the basis for a lot of computation seen in the field of compressed sensing and compression in general.\n\n\n\n\n\n\nNote\n\n\n\nThe driving factors for compression are audio, image and video, but also raw data compression as seen with zip, 7z and all the other available algorithms.\nIt is wrong to assume that we do not see this in engineering applications. High dimensional differential equations usually have a solution on a low dimensional manifolds and therefore imply that sparsity can be seen here to.\n\n\nLet us return to image compression and follow along with the examples given in (Brunton and Kutz 2022, chaps. 3, pp98–101).\nWe can use the code provided earlier. We move from Figure 12.1 (a) to Figure 12.1 (b) via \\(\\mathcal{F}\\). From Figure 12.1 (b) to Figure 12.1 (d) we only keep the highest 5% of our values and move to Figure 12.1 (c) via \\(\\mathcal{F}^{-1}\\).\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\"https://www.mci.edu/en/download/27-logos-bilder?download=618:mci-eu-web\"))\n\n# https://raw.githubusercontent.com/dynamicslab/databook_python/refs/heads/master/DATA/jelly.jpg\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_fft = np.fft.fft2(A)\nA_fft_sort = np.sort(np.abs(A_fft.reshape(-1)))\nmyplot(A)\n\nmyplot(np.log(np.abs(np.fft.fftshift(A_fft)) + 1))\nc = 0.05\nthresh = A_fft_sort[int(np.floor((1 - c) * len(A_fft_sort)))]\nA_fft_th = A_fft * (np.abs(A_fft) &gt; thresh)\nA_th = np.fft.ifft2(A_fft_th).real\nmyplot(A_th)\nmyplot(np.log(np.abs(np.fft.fftshift(A_fft_th)) + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Fourier coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Compressed image\n\n\n\n\n\n\n\n\n\n\n\n(d) Truncated FFT coefficients (5%)\n\n\n\n\n\n\n\nFigure 12.1: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.\n\n\n\nIn order to get an idea why the Fourier transform is useful in this scenario we look at the image as a surface.\n\n\n\n\n\n\nNote\n\n\n\nIn order to make this feasible for interactive rendering we use only the upper left quarter of the image.\n\n\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nB = np.transpose(A[:int(A.shape[0]/2):5, :int(A.shape[1]/2):5])\ny = np.arange(B.shape[0])\nx = np.arange(B.shape[1])\n\nX,Y = np.meshgrid(x,y)\nfig = go.Figure()\nfig.add_trace(go.Surface(z=B, x=X, y=Y))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12.2: Upper left quarter of the MCI I image as a 3D surface\n\n\n\n\nAs can be seen in Figure 12.2 we can express the clouds with view modes and even the raster of the building seams to fit this model nicely.\nIt is not very surprising to have such structure in a so called natural image. The image or pixel space is big, very big. For an \\(n \\times n\\) black and white image there are \\(2^{n^2}\\) possible images. So for a \\(20 \\times 20\\) image we already have \\(2^400\\) possible images which a number with a number with 121 digits and it is assumed that there are (only) about \\(10^{82}\\) atoms in the universe.\n\n\n\n\n\n\nFigure 12.3: Medium article discussing probability models for data.\n\n\n\nSo finding structure in images, especially in images with high resolution is not surprising. The rest is basically random noise. Most of the dimensions of pixel space are only necessary if we want to encode this random noise images but for a cloud with some mountains and a building only need some dimensions and are therefore highly compressible.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Sparsity and Compression</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html",
    "href": "sensing/compsensing.html",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "13.1 The theoretic basics of compressed sensing\nNow that we have seen compressed sensing in action we need to bring in the theoretical basics that form this theory.\nThe key feature is to look into the geometry of sparse vectors and how these vectors are transformed via random measurements. More precisely, for large enough \\(p\\) (amount of measurements) our matrix \\(D = CB\\) of Definition 13.1 preserves the distance and inner product structure of sparse vectors. In turn this means, we need to find a matrix \\(C\\) such that \\(D\\) is a near-isometry map on sparse vectors.\nIf \\(D\\) behaves as a near isometry, it is possible to solve \\[\ny = D s\n\\] for the sparsest vector \\(s\\) using convex \\(\\ell_1\\) minimization.\nFurthermore, a general rule is, the more incoherent the measurements are the smaller we can choose \\(p\\).\nIt is difficult to compute the \\(\\delta_K\\) in the RIP and as \\(C\\) may be selected at random there is more information included in the statistical properties of \\(\\delta_K\\) for a family of matrices \\(C\\). In general, increasing \\(p\\) ill decrease \\(\\delta_K\\), same as having incoherence vectors and both improve the properties of \\(CB\\) by bringing it closer to an isometry.\nLuckily, there exist generic sampling matrices \\(C\\) that are sufficiently incoherent with resect to nearly all transform bases. In particular, Gaussian and Bernoulli random measurement matrices satisfy Definition 13.2 for a generic \\(B\\) (with high probability).",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#the-theoretic-basics-of-compressed-sensing",
    "href": "sensing/compsensing.html#the-theoretic-basics-of-compressed-sensing",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "Note\n\n\n\nAn isometry map is a map that is distance preserving, i.e. the distance between to points is not changed under this map.\n\\[\n\\| a - b \\| = \\| f(a) - f(b) \\|\n\\]\nA unitary map is a map that preserves distance and angle between vectors, i.e. \\[\n\\langle a, b \\rangle = \\langle f(a), f(b) \\rangle\n\\] For a linear map \\(f(x) = Ux\\) this results in \\(U^\\mathrm{H}U = UU^\\mathrm{H} = I\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 13.2 (Restricted Isometry Property (RIP)) For \\(p\\) incoherent measurements the matrix \\(D=CB\\) satisfies a restricted isometry property (RIP) for sparse vectors \\(s\\)\n\\[\n(1-\\delta_K)\\|s\\|_2^2 \\leq \\|C B s\\|_2^2 \\leq (1+\\delta_K)\\|s\\|_2^2,\n\\] with the restricted isometry constant \\(\\delta_K\\). For a small enough \\(\\delta_K\\) \\(CB\\) acts as a near-isometry on \\(K\\)-sparse vectors.\nIn particular, for \\(\\delta_K &lt; 1\\) and therefore \\((1-\\delta_K) &gt; 0\\) and \\((1+\\delta_K) &gt; 0\\) this means the norm induced by \\(CB\\) is equivalent to the two norm on the \\(K\\)-sparse vectors.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#sparse-regression",
    "href": "sensing/compsensing.html#sparse-regression",
    "title": "13  Compressed Sensing",
    "section": "13.2 Sparse Regression",
    "text": "13.2 Sparse Regression\nLet us return to regression for a moment an in particular to the LASSO method introduced in Section 8.1.1. We have seen that \\(\\ell_1\\), i.e. the \\(\\|\\cdot\\|_1\\), promotes sparsity and we can use this to create a more robust method that rejects outliers.\nWe split up our points into a training and test set (the classic 80:20 split). For the test set an varying the parameter \\(\\lambda_1\\) through a range of values we create a fit with the training set and test against the test set.\n\n\n\n\n\n\n\nExample 13.2 (LASSO fit for a regression problem) For our \\[\nA x = b\n\\] problem, we consider a problem the dimensions as \\(200\\) observations with \\(10\\) candidate predictions. This results in a matrix \\(A\\in\\mathcal{R}^{200 \\times 10}\\) and we select the vector \\(b\\in\\mathcal{R}^{200}\\) as the linear combination of exactly two of theses \\(10\\) candidates. As a result, the vector \\(x\\) is \\(2\\)-sparse by construction, and the aim is to recover this. In order to give the algorithm something to work on we add noise to \\(b\\) resulting in no zero element in \\(b\\).\nTo perform a \\(10\\) fold cross validation 1 for the LASSO method we use the features of sklearn 2.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 200\nn = 10\n\nA = np.random.randn(m, n)\nx = np.array([0, 0, 1, 0, 0, 0, -1, 0, 0, 0])\nb = A @ x + 2 * np.random.randn(m)\n\ncv = 10\nlassoCV = linear_model.LassoCV(cv=cv, random_state=6020, tol=1e-8)\nlassoCV.fit(A, b)\n\nlasso_best = linear_model.Lasso(alpha=lassoCV.alpha_, random_state=6020)\nlasso_best.fit(A, b)\nprint(lasso_best.coef_)\n\nplt.figure()\nLmean = lassoCV.mse_path_.mean(axis=-1)\nerror = [np.min(lassoCV.mse_path_, axis=-1), \n         np.max(lassoCV.mse_path_, axis=-1)] / np.sqrt(n)\nplt.errorbar(lassoCV.alphas_, Lmean, yerr=error, ecolor=\"lightgray\")\nplt.plot(lassoCV.alpha_, Lmean[lassoCV.alphas_==lassoCV.alpha_], \"go\", mfc='none')\nplt.xscale(\"log\")\nplt.ylabel(\"Means Square Error\")\nplt.xlabel(r\"$\\lambda_1$\")\nplt.gca().invert_xaxis()\nplt.show()\n\n\n[ 0.          0.          1.0978465   0.         -0.         -0.\n -1.06495486  0.          0.          0.        ]\n\n\n\n\n\n\n\n\nFigure 13.3: Cross-validation mean square error of Lasso fit, the green circle marks the optimal choice, the blue line the mean error of the k-folds and the error bar the maximal and minimal error form the k-folds.\n\n\n\n\n\nThe resulting coefficients for the best fit with lasso are\n\n\narray([ 0.        ,  0.        ,  1.0978465 ,  0.        , -0.        ,\n       -0.        , -1.06495486,  0.        ,  0.        ,  0.        ])\n\n\nand a appropriate fit for our toy example.\n(Compare Brunton and Kutz 2022, 115)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is also possible to build a dictionary and and look for a sparse representation in this dictionary.\nAn example how this can be done with the Eigenfaces example can be found in (Brunton and Kutz 2022, 117)",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#robust-principal-component-analysis-rpca",
    "href": "sensing/compsensing.html#robust-principal-component-analysis-rpca",
    "title": "13  Compressed Sensing",
    "section": "13.3 Robust Principal Component Analysis (RPCA)",
    "text": "13.3 Robust Principal Component Analysis (RPCA)\nWe discussed the principal component analysis in Section 5.2 as an application with the SVD. Similar as regression, also the PCA is sensitive to outliers and corrupt data. In Candès et al. (2011) an algorithm to make it more robust was developed.\nThe main idea of the paper is to decompose a matrix \\(X\\) into a structured low-rank matrix \\(L\\) and a sparse matrix \\(S\\) containing the outliers and corrupt data \\[\nX = L + S.\n\\] If we can recover the principal components of \\(L\\) from \\(X\\) we have a robust method as the influence of \\(S\\) have little influence.\nIt might not be immediately obvious but applications of this split are video surveillance where the background is represented by \\(L\\) and the foreground objects in \\(L\\), face recognition with the eigenfaces in \\(L\\) and shadows, occlusions (like glasses or masks) are in \\(S\\).\nWe have an optimization problem of the form \\[\n\\min_{L, S} \\operatorname{rank}(L) + \\| S \\|_0 \\quad\\text{subject to}\\quad $L+S = X$,\n\\tag{13.2}\\] where unfortunately both parts are not convex but we can search for it with high probability using \\[\n\\min_{L, S} \\|L\\|_{\\star} + \\lambda \\| S \\|_1 \\quad\\text{subject to}\\quad $L+S = X$,\n\\tag{13.3}\\] with \\(\\|\\cdot\\|_{\\star}\\) is called the nuclear norm consisting of the sum of all singular values and we use it as our proxy for the \\(\\operatorname{rank}\\).\nEquation 13.3 is called principal component pursuit (PCP) and in Candès et al. (2011) they show that Equation 13.3 converges to Equation 13.2 for\n\n\\(\\lambda = 1/\\sqrt{\\max(n,m)}\\) for \\(X^{m \\times n}\\) and\n\\(L\\) is not sparse\n\\(S\\) is not low-rank, where we assume that the entries do not span a low-dimensional column space.\n\nWe can solve the PCP with an augmented Lagrange multiplier algorithm such as \\[\n\\mathcal{L}(L, S, Y) = \\|L\\|_{\\star} + \\lambda \\| S \\|_1 + \\langle Y, X - L - S \\rangle + \\frac{\\mu}{2} \\|X-L-S\\|_F^2.\n\\] This is an iterative method where we solve for \\(L_k\\) and \\(S_k\\), update \\(Y_{k+1} = Y_k + \\mu (X-L_k - S_k)\\) and check for convergence. For this specific problem the alternations method (ADM) provides a simple procedure to solve for \\(L\\) and \\(S\\).\nThe following implementation of RPCA can be found in (Brunton and Kutz 2022, 121) with a similar application to the eigenfaces dataset.\n\n\nRobust principal component analysis and helper functions\ndef shrink(X, tau):\n    Y = np.abs(X) - tau\n    return np.sign(X) * np.maximum(Y, np.zeros_like(Y))\n\n\ndef SVT(X, tau):\n    U, S, VT = np.linalg.svd(X, full_matrices=False)\n    out = U @ np.diag(shrink(S, tau)) @ VT\n    return out\n\n\ndef RPCA(X):\n    n1, n2 = X.shape\n    mu = n1 * n2 / (4 * np.sum(np.abs(X.reshape(-1))))\n    lambd = 1 / np.sqrt(np.maximum(n1, n2))\n    thresh = 10**(-7) * np.linalg.norm(X)\n    \n    S = np.zeros_like(X)\n    Y = np.zeros_like(X)\n    L = np.zeros_like(X)\n    count = 0\n    while (np.linalg.norm(X - L - S) &gt; thresh) and (count &lt; 1000):\n        L = SVT(X - S + (1 / mu) * Y, 1 / mu)\n        S = shrink(X - L + (1 / mu) * Y, lambd / mu)\n        Y = Y + mu*(X - L - S)\n        count += 1\n    return L, S\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nresponse = requests.get(\"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\nXX = faces[:,:nfaces[0]]\nim = np.asarray(iio.imread(\"https://raw.githubusercontent.com/dynamicslab/databook_python/refs/heads/master/DATA/mustache.jpg\"))\nA = (rgb2gray(im)/255).astype(\"uint8\")\nX = np.append(XX, (XX[:, 2] * A.T.flatten()).reshape((-1, 1)), axis=1)\nL, S = RPCA(X)\n\nfor index in [2, 3, -1]:\n    plt.figure()\n    plt.imshow(np.reshape(X[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n    plt.figure()\n    plt.imshow(np.reshape(L[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n    plt.figure()\n    plt.imshow(np.reshape(S[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(X\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(L\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(S\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(X\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n(e) \\(L\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n(f) \\(S\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) \\(X\\) for image 3 with comic disguise\n\n\n\n\n\n\n\n\n\n\n\n(h) \\(L\\) for image 3 with comic disguise\n\n\n\n\n\n\n\n\n\n\n\n(i) \\(S\\) for image 3 with comic disguise\n\n\n\n\n\n\n\nFigure 13.4: RPCA decomposition for the Yale B dataset.\n\n\n\nFigure 13.4 we can see that RPCA effectively filters out occluded regions and shadows form \\(X\\) in \\(L\\). The missing part is filled in with the most consistent low-rank feature from the eigenfaces.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#sparse-sensor-placement",
    "href": "sensing/compsensing.html#sparse-sensor-placement",
    "title": "13  Compressed Sensing",
    "section": "13.4 Sparse Sensor Placement",
    "text": "13.4 Sparse Sensor Placement\nSo far we have looked how to construct a signal with random measurements in a generic basis. But how about placing sensors at the correct points to reconstruct the signal with high probability. This can dramatically reduce the amount of data to measure.\nWe can set tailored sensors for a particular library instead of random sensors in a generic library.\nThis can be used to reconstruct faces, or classify signals, see (Brunton and Kutz 2022, chap. 3.8) and references within.\nTo see this in action we use the Python package PySensors and the example Sea Surface Temperature (SST) sensors from their documentation, Online accessed on the 28th of November 2024.\n\n\n\n\n\n\n\nExample 13.3 (Sea Surface Temperature (SST) sensors) For a given dataset of sea surface temperature as training data we would like to place (sparse) sensors optimal that allow us to reconstruct the temperature at any other location. This is achieved with the SSPOR algorithm form Manohar et al. (2018).\n\nShow the code for the figure\nfrom ftplib import FTP\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport netCDF4\nimport pysensors as ps\n\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Import and save data locally\nftp = FTP('ftp.cdc.noaa.gov')\nftp.login()\nftp.cwd('/Datasets/noaa.oisst.v2/')\n\nfilenames = ['sst.wkmean.1990-present.nc', 'lsmask.nc']\n\nfor filename in filenames:\n    localfile = open(filename, 'wb')\n    ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n    localfile.close()\n\nftp.quit()\n\nf = netCDF4.Dataset('sst.wkmean.1990-present.nc')\n\nlat,lon = f.variables['lat'], f.variables['lon']\nSST = f.variables['sst']\nsst = SST[:]\n\nf = netCDF4.Dataset('lsmask.nc')\nmask = f.variables['mask']\n\nmasks = np.bool_(np.squeeze(mask))\nsnapshot = float(\"nan\")*np.ones((180,360))\nsnapshot[masks] = sst[0,masks]\n\nplt.figure()\nplt.imshow(snapshot, cmap=plt.cm.coolwarm)\nplt.xticks([])\nplt.yticks([])\nX = sst[:,masks]\nX = np.reshape(X.compressed(), X.shape)\n\n# Compute optimal sensor placement\nmodel = ps.SSPOR(\n    basis=ps.basis.SVD(n_basis_modes=25),\n    n_sensors=25\n)\nmodel.fit(X)\nsensors = model.get_selected_sensors()\n\n# Plot sensor locations\ntemp = np.transpose(0 * X[1,:])\ntemp[sensors] = 1\nimg = 0*snapshot\nimg[masks] = temp\nplt.figure()\nplt.imshow(snapshot, cmap=plt.cm.coolwarm)\nindx = np.where(img==1)\nplt.scatter(indx[1], indx[0], 4, color='black')\nplt.xticks([])\nplt.yticks([])\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sea surface temperature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal learned sensor placements to recover sea surface temperature\n\n\n\n\n\n\n\nFigure 13.5: Finding optimal sensor placements to recover the sea surface temperature.\n\n\n\n(Compare Online accessed on the 28th of November 2024)\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nCandès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. 2011. “Robust Principal Component Analysis?” J. ACM 58 (3). https://doi.org/10.1145/1970392.1970395.\n\n\nManohar, Krithika, Bingni W. Brunton, J. Nathan Kutz, and Steven L. Brunton. 2018. “Data-Driven Sparse Sensor Placement for Reconstruction: Demonstrating the Benefits of Exploiting Known Patterns.” IEEE Control Systems Magazine 38 (3): 63–86. https://doi.org/10.1109/MCS.2018.2810460.\n\n\nNeedell, D., and J. A. Tropp. 2009. “CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate Samples.” Applied and Computational Harmonic Analysis 26 (3): 301–21. https://doi.org/https://doi.org/10.1016/j.acha.2008.07.002.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#footnotes",
    "href": "sensing/compsensing.html#footnotes",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "We split our data into \\(10\\) sets, use 9 for the training and 1 for test and average over the results.↩︎\nInstall the package via pdm add scikit-learn.↩︎",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Collection of interesting reads",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#collection-of-interesting-reads",
    "href": "summary.html#collection-of-interesting-reads",
    "title": "Summary",
    "section": "",
    "text": "Semantic Versioning How to design the version of your project.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nCandès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. 2011.\n“Robust Principal Component Analysis?” J. ACM 58\n(3). https://doi.org/10.1145/1970392.1970395.\n\n\nCooley, James W., and John W. Tukey. 1965. “An Algorithm for the\nMachine Calculation of Complex Fourier Series.” Mathematics\nof Computation 19 (90): 297–301. https://doi.org/10.1090/s0025-5718-1965-0178586-1.\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A\nPractical Introduction for Mechanical, Civil, and Aerospace\nEngineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nGauß, Carl Friedrich. 1866. “Theoria Interpolationis Methodo Nova\nTractata.” Göttingen: Königliche Gesellschaft der Wissenschaften.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -.\nSebastopol: O’Reilly.\n\n\nManohar, Krithika, Bingni W. Brunton, J. Nathan Kutz, and Steven L.\nBrunton. 2018. “Data-Driven Sparse Sensor Placement for\nReconstruction: Demonstrating the Benefits of Exploiting Known\nPatterns.” IEEE Control Systems Magazine 38 (3): 63–86.\nhttps://doi.org/10.1109/MCS.2018.2810460.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed.\nSebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nMeyberg, Kurt, and Peter Vachenauer. 1992. Höhere\nMathematik 2. Springer-Lehrbuch. New York, NY: Springer.\n\n\nNeedell, D., and J. A. Tropp. 2009. “CoSaMP: Iterative Signal\nRecovery from Incomplete and Inaccurate Samples.” Applied and\nComputational Harmonic Analysis 26 (3): 301–21. https://doi.org/https://doi.org/10.1016/j.acha.2008.07.002.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber,\nEsteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population\nGrowth.” Our World in Data.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on\nIntroduction. München: No Starch Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in the form of Linear Algebra as well as some topics of statistics of sets. We will always immediately show how to use the discussed content in Python.\n\n\n\n\n\n\nNote\n\n\n\nThese notes assume that you have some basic knowledge of programming in Python and we build on this knowledge. In this sense, we use Python as a tool and only describe the inner workings if it helps us to better understand the topics at hand.\nIf this is not the case have a look at MECH-M-DUAL-1-SWD, a class on software design in the same master program and from the same authors.\nAdditionally, we can recommend the following books on Python:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming; Online Material.\nPython Cheat Sheet provided by Matthes (2023).\nMcKinney (2022): Python for data analysis 3e; Online and Print\nVasiliev (2022): Python for Data Science - A Hands-On Introduction\nInden (2023): Python lernen – kurz & gut; German\n\n\n\n\n\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -. Sebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed. Sebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on Introduction. München: No Starch Press.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "matrixdc/index.html",
    "href": "matrixdc/index.html",
    "title": "Matrix decompositions",
    "section": "",
    "text": "There are a lot of different matrix decompositions and they can be used to fulfil several tasks. We are going to look into the eigendecomposition as well as the singular value decomposition. Both of these can, for example, be used for picture compression and recognition.\nFor notation we are following again Golub and Van Loan (2013), which has plenty more to offer than we cover in these notes.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regression analysis",
    "section": "",
    "text": "In general, regression analysis can be understood as a set of tools that is used to estimate or establish a relationship between a dependent variable \\(Y\\) (also called outcome or response variable, label) and the independent variable \\(X\\) (also called regressor, predictors, covariates, explanatory variable or feature). If we add a regression function \\(f\\) and some unknown parameters \\(c\\) to the mix the problem can be written mathematically as \\[\nY = f(X, c)\n\\tag{1}\\] where \\(c\\) is found by optimizing for a good fit of \\(f\\) to the data.\nWe split up the discussion along the well known topics:\n\n6  Linear Regression\n7  Non-linear Regression\n\n7.1 Gradient Descent\n\n8  Optimizers\n\n8.2 Model Selection/Identification and over-/underfitting\n\n\nParts of this section are based on (Brunton and Kutz 2022, sec. 4).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis"
    ]
  },
  {
    "objectID": "signal/index.html",
    "href": "signal/index.html",
    "title": "Signal Processing",
    "section": "",
    "text": "In previous parts we have already discussed how changing a coordinate system or basis (Definition 1.8) can simplify expression or computation, see 4  Eigendecomposition and 5  Singular Value Decomposition, among others.\nOne of the most fundamental coordinate transformations was introduced by J.-B. Joseph Fourier in the early 1800s. While investigating heat he discovered that sine and cosine functions with increasing frequency form an orthogonal basis (Definition 1.6 & Definition 1.9). In fact, the sine and cosine functions for an eigenbasis for the heat equation \\[\n\\frac{\\partial u}{\\partial t} = \\Delta u\n\\] and solving it becomes trivial once you determine the corresponding eigenvalues that are connected to the geometry, amplitudes, and boundary conditions.\nIn the 200+ years since, this discovery has not only founded new corners of mathematics but also allows via the fast fourier transform or FFT the real-time image and audio compression that make our global communication networks work.\nIn the same area the related wavelets where developed to for advanced signal processing and compression.\nIn this section we are going to discuss basics of signal processing in terms of these and other signal transformations, see\n\n9  Fourier Transform\n10  Wavelet transform\n11  Two-Dimensional Transform\n\nParts of this section are based on (Brunton and Kutz 2022, chap. 2).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Signal Processing"
    ]
  },
  {
    "objectID": "sensing/index.html",
    "href": "sensing/index.html",
    "title": "Sparsity and compression",
    "section": "",
    "text": "We have seen in the sections before that our signals and data can often be expressed in an optimal way by changing basis. Moreover, this often results in sparse data and therefore gives us the opportunity for compression. By expressing our data in this basis most coefficients are zero or small.\nThe zero coefficients give rise to sparsity and the almost zero coefficients allow us to compress the data further without loosing too much information. We have seen this in the Eigendecomposition, the singular value decomposition, within regression choices, the Fourier transform, the Wavelet transform, and in other such transformations not covered here.\nRecent development in mathematics have given rise to the field of compressed sensing, where not high dimensional signals are collected and transformed or compressed but we start by a acquiring compressed signals and solve for the sparsest high-dimensional signal that is consistent with the collected data.\nHere we will discuss sparsity and compression and give an outlook on compressed sensing. We have already seen multiple examples and we will use this chapter to contextualize these results and combine them to give rise to new ideas and further aspects.\nIt is worth mentioning that quite often sparsity gives rise to so called parsimonious models that avoid overfitting and remain interpretable because they have a low number of terms. This fits neatly into the discussion of Occam’s razor: the simplest explanation is in general the correct one. Simple can also mean the least coefficients and this means sparsity. Furthermore, it can also help to create more robust algorithms are outliers have less influence.\n\n\n\n\n\n\nImportant\n\n\n\nParts of this section are based on (Brunton and Kutz 2022, chap. 3).\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Sparsity and compression"
    ]
  }
]