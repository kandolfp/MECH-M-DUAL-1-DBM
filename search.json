[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Basics of Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger and Matthias Panny for testing, checking, suggestions and general proofreading.\nThese notes are built with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basic concepts behind modern day Data Science techniques. We will always try to not only discuss the theory but also use Python to illustrate the content programmatically.\nThe main reference for these notes is the excellent Brunton and Kutz (2022), see citations throughout the notes. If python code is adapted form code blocks provided by Brunton and Kutz (2022), see github this is indicated and for these blocks you can also find the MATLAB equivalent in the book or on github.\nThese notes are intended for engineering students and therefore the mathematical concepts will rarely include rigorous proofs.\nWe start of by recalling the main concepts of 1  Linear Algebra and some statistics on 2  Data sets to make sure everybody as an the same page for notation and problematically in Python.\nWe continue with matrix decomposition, namely 3  Eigendecomposition and 4  Singular Value Decomposition. These basic decomposition are used to illustrate certain concepts we need in later chapters but also to show how the change of basis is influencing problems, their solution and computational properties. Furthermore, we dive into the first concepts used in machine learning where matrix computations build the foundation. As illustration we use applications for engineering and image processing.\nThe SVD allows us to neatly transition to 5  Linear Regression, generalize to 6  Non-linear Regression and discuss optimization and learning properties with the help of 7.1.1 LASSO and 7.2 Model Selection/Identification and over-/underfitting in 7  Optimizers. We mainly use toy examples but where appropriate we look at the world population or unemployment data to illustrate a concept.\nIn the next part we look at aspects of signal processing often found in engineering and especially mechatronics and therefore we discuss 8  Fourier Transform, 9  Laplace Transform as well as 10  Wavelet transform transform and how they are extended to 11  Two-Dimensional Transform. Examples range from toy examples to solving electric circuit problems or image processing.\nThe fifth part is based on Brunton and Kutz (2022) Chapter 3 where we look at aspects of 12  Sparsity and Compression as well as the rather new topic of 13  Compressed Sensing.\nTo round the content of these notes we look at statistics with some basis of 14  Bayesian Statistics and the engineering application via 15  Kalman Filter.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in the form of Linear Algebra as well as some topics of statistics of sets. We will always immediately show how to use the discussed content in Python.\n\n\n\n\n\n\nNote\n\n\n\nThese notes assume that you have some basic knowledge of programming in Python and we build on this knowledge. In this sense, we use Python as a tool and only describe the inner workings if it helps us to better understand the topics at hand.\nIf this is not the case have a look at MECH-M-DUAL-1-SWD, a class on software design in the same master program and from the same authors.\nAdditionally, we can recommend the following books on Python:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming; Online Material.\nPython Cheat Sheet provided by Matthes (2023).\nMcKinney (2022): Python for data analysis 3e; Online and Print\nVasiliev (2022): Python for Data Science - A Hands-On Introduction\nInden (2023): Python lernen – kurz & gut; German\n\n\n\n\n\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -. Sebastopol: O’Reilly.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed. Sebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on Introduction. München: No Starch Press.",
    "crumbs": [
      "Basics"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nThe aim of this section is to discuss the basics of matrix, vector and number theory that we need for the later chapters and not introduce the whole of linear algebra. Nevertheless, we will rely on some basic definitions that can be found in any linear algebra book. For notation we refer to Golub and Van Loan (2013).\nIn Python the module numpy is used to represent vectors and matrices, we can include it like this (and give it its short hand np):\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R},\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n.\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\nC = A^{\\mathsf{T}}, \\quad \\Rightarrow \\quad c_{ij} = a_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define two more mappings.\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor a \\({m \\times p}\\) matrix \\(A\\) and a \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition 1.1 (Vector space) For a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following properties need to hold true to form a vector space.\nFor the vector addition we need to have\n\nassociativity \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity \\[u + v = v + u,\\]\nthere needs to exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector such that \\[v + 0 =  v,\\]\nthere needs to exist an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] and this element is usually denoted by \\(-v\\).\n\nFor the scalar multiplication we need to have\n\nassociativity \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\ndistributivity with respect to the vector addition \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v,\\]\nand there needs to exist a multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v.\\]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n\nExercise 1.1 (Vector space in Python) Create some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.2 (Matrix as vector space?) How do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Linear map) A linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a field \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w),\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Dot product) The dot product, inner product, or scalar product of two vectors \\(v\\) and \\(w\\) as is defined by \\[\\langle v, w\\rangle = v \\cdot w = v^{\\mathsf{T}} w = \\sum_i v_i w_i.\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Outer product) We also have the outer product defined as: \\[\nv w^{\\mathsf{T}} = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.3 (Matrix multiplication?) Show that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\n\nDefinition 1.5 (Norm) A norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac{1}{p}},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAnd for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |a_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{\\sum_i \\sum_j |a_{ij}|^2},\\]\nthe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(\\sum_i \\sum_j |a_{ij}|^p\\right)^{\\frac1p},\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |a_{ij}|.\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "href": "basics/linearalgebra.html#sec-intro-linearalgebra-basis",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\n\nDefinition 1.6 (Basis) A set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\}, b_i \\in \\mathbb{R}^n\\):\n\nis called linear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}.\\]\n\nThe set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\nFor a shorter notation we often associate the matrix \\[\nB = \\left[b_1 | \\cdots | b_n\\right]\n\\] with the basis.\n\n\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\) and its associated matrix is \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & \\dots & 0\\\\\n                        0 & 1 & \\ddots & \\vdots \\\\  \n                        \\vdots & \\ddots & 1 & 0\\\\\n                        0 & \\dots & 0 & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n},\n\\] and called the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix.\nThe easiest way to create one of standard basis vectors, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\nand the identity matrix by\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n\nExample 1.1 (Standard basis)  \n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\n# compute the coefficients\na = np.dot(x, e_1) / np.dot(e_1, e_1)\nb = np.dot(x, e_2) / np.dot(e_2, e_2)\nc = np.dot(x, e_3) / np.dot(e_3, e_3)\ny = a * e_1 + b * e_2 + c * e_3\nprint(f\"{y=}\")\nprint(f\"{np.allclose(x, y)=}\")\nprint(f\"{LA.norm(x-y)=}\")\n\nx=array([0.37842701, 0.28016588, 0.12295015])\ny=array([0.37842701, 0.28016588, 0.12295015])\nnp.allclose(x, y)=True\nLA.norm(x-y)=np.float64(0.0)\n\n\nSee numpy.testing for more ways of testing in numpy.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\n\nDefinition 1.7 (Matrix inverse) For matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\n\n\nThe following holds true for the inverse of matrices:\n\nthe inverse of a product is the product of the inverses \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^{\\mathsf{T}} = (A^{\\mathsf{T}})^{-1} \\equiv A^{-\\mathsf{T}}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nprint(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n# Note that the equality is hard to achieve for floats\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]), verbose=False)\n\nA=array([[0.34396981, 0.87727258, 0.15323862],\n       [0.35230319, 0.00968911, 0.74542401],\n       [0.39368043, 0.63845048, 0.76933446]])\nX=array([[ 4.35433053,  5.36393464, -6.06453701],\n       [-0.20839142, -1.89896482,  1.88145425],\n       [-2.05524023, -1.1689079 ,  2.8417734 ]])\nA @ X=array([[ 1.00000000e+00,  1.46861801e-16,  2.30346454e-16],\n       [ 1.36964472e-16,  1.00000000e+00, -2.37868717e-16],\n       [-6.57911808e-18,  1.30769680e-16,  1.00000000e+00]])\nnp.allclose(A @ X, np.eye(A.shape[0]))=True\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 8\n      6 print(f\"{np.allclose(A @ X, np.eye(A.shape[0]))=}\")\n      7 # Note that the equality is hard to achieve for floats\n----&gt; 8 np.testing.assert_equal(A @ X, np.eye(A.shape[0]), verbose=False)\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.8/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 7 / 9 (77.8%)\nMax absolute difference among violations: 2.37868717e-16\nMax relative difference among violations: 2.22044605e-16\n\n\n\n\n\n\n\n\n\n\nDefinition 1.8 (Change of basis) If we associate the matrices \\(B\\) and \\(C\\) with the matrix consisting of the basis vectors of two bases \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\) of a vector space we can define the transformation matrix \\(T_{\\mathcal{C}}^{\\mathcal{B}}\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{C}\\) as \\[\nT_{\\mathcal{C}}^{\\mathcal{B}} = C^{-1}B.\n\\]\nSo if we have a vector \\(b\\) represented in \\(\\mathcal{B}\\) we can compute its representation in \\(\\hat{b}\\) in \\(\\mathcal{C}\\) as \\[\n\\hat{b} = T_{\\mathcal{C}}^{\\mathcal{B}} b = C^{-1}B b.\n\\]\nA special form is if we have the standard basis \\(I\\) and move to a basis \\(C\\) we get \\[\n\\hat{b} = T_{C}^{I} b = C^{-1} b.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 1.2 (Basis Change) For \\[\nB = \\left[\n    \\begin{array}{ccc} 1 & 3 & 2 \\\\\n                       0 & 1 & 1 \\\\  \n                       2 & 0 & 1 \\\\  \\end{array}\n\\right]\\quad \\text{and}\\quad\nC = \\left[\n    \\begin{array}{ccc} 1 & 0 & 1 \\\\\n                       0 & 1 & 1 \\\\  \n                       1 & 1 & 0 \\\\  \\end{array}\n\\right]\n\\] we get \\[\nT_{C}^{B} = \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right],\n\\] and for a \\(v = 2 b_1 - b_2 + 3 b_3\\) we can compute its representation in \\(C\\) as \\[\n\\hat{v} = T_{C}^{B} v\n= \\left[\n    \\begin{array}{ccc} \\frac32 & 1 & 1 \\\\\n                       \\frac12 & -1 & 0 \\\\  \n                       -\\frac12 & 2 & 1 \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} 5 \\\\ 2 \\\\ 0 \\end{array}\n\\right],\n\\] and therefore, \\(v = 5c_1 + 2c_2 + 0 c_3\\).\n(Compare Wikipedia)\n\n\n\n\nThere are special basis vectors, respectively matrices that allow for easy computation of the inverse.\n\n\n\n\n\n\n\nDefinition 1.9 (Orthonormal vector) We call a set of vectors \\(\\mathcal{V}=\\{u_1, u_2, \\ldots, u_m\\}\\) orthonormal if and only if \\[\n\\forall i,j: \\langle u_i, u_j \\rangle = \\delta_{ij}\n\\] where \\(\\delta_{ij}\\) is called the Kronecker delta which is \\(1\\) if and only if \\(i=j\\) and \\(0\\) otherwise. This is true for a inner product, see Definition 1.3.\n\n\n\n\nExtending this to a matrix (and to that end a basis) as follows.\n\n\n\n\n\n\n\nDefinition 1.10 (Orthogonal matrix) We call a matrix \\(Q\\in\\mathbb{R}^{n\\times n}\\), here the real and square is important, orthogonal if its columns and rows are orthonormal vectors. This is the same as \\[\nQ^{\\mathsf{T}} Q = Q Q^{\\mathsf{T}} = I\n\\] and this implies that \\(Q^{-1} = Q^{\\mathsf{T}}\\).\n\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/sets.html",
    "href": "basics/sets.html",
    "title": "2  Data sets",
    "section": "",
    "text": "2.1 Basic properties of a data set\nWe continue our little introduction by looking at data sets in the sense of a list of values that we want to describe closer.\nWe use the mieten3.asc from Open Data LMU. The data set contains information about rents in Munich from the year 2003. The columns have the following meaning, see DETAILS:\nFor now, we’ll just show the code without much explanation because we want to jump right in and do not want to delve into how it works. We use a structured array of numpy for it.\nNow that we have some data we can look at it more closely, for this we interpret a row as a vector.\nFirst we are looking at the total net rent, i.e. the row nm.\nFor a vector \\(v \\in \\mathbb{R}^n\\) we have:\nnm_max = np.max(data['nm'])\nprint(f\"{nm_max=}\")\n\nnm_min = np.min(data['nm'])\nprint(f\"{nm_min=}\")\n\nnm_mean = np.mean(data['nm'])\n# round to 2 digits\nnm_mean_r = np.around(nm_mean, 2)\nprint(f\"{nm_mean_r=}\")\n\nnm_median = np.median(data['nm'])\nprint(f\"{nm_median=}\")\n\nnm_quartiles = np.quantile(data['nm'], [1/4, 1/2, 3/4])\nprint(f\"{nm_quartiles=}\")\n\nnm_max=np.float64(1789.55)\nnm_min=np.float64(77.31)\nnm_mean_r=np.float64(570.09)\nnm_median=np.float64(534.3)\nnm_quartiles=array([389.95, 534.3 , 700.48])\nFrom this Python snippet we know that for tenants the rent varied between 77.31 and 1789.55, with an average of 570.09 and a median of 534.3. Of course there are tricky questions that require us to dig a bit deeper into these functions, e.g. how many rooms does the most expensive flat have? The surprising answer is 3 and it was built in 1994, but how do we obtain these results?\nWe can use numpy.argwhere or a function which returns the index directly like numpy.argmax.\nmax_index = np.argmax(data['nm'])\nrooms = int(data['rooms'][max_index])\nyear = int(data['bj'][max_index])\nprint(f\"{rooms=}, {year=}\")\n\nrooms=3, year=1994",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#basic-properties-of-a-data-set",
    "href": "basics/sets.html#basic-properties-of-a-data-set",
    "title": "2  Data sets",
    "section": "",
    "text": "the maximal value, i.e. the maximum \\[\nv^{max} = \\max_i v_i,\n\\]\nthe minimal value, i.e. the minimum \\[\nv^{min} = \\min_i v_i,\n\\]\nthe mean of all values (often called the arithmetic mean) \\[\n\\overline{v} = \\frac1n \\sum_{i=1}^n v_i = \\frac{v_1 + v_2 + \\cdots + v_n}{n},\n\\]\nthe median, i.e. the value where half of all the other values are bigger and the other half is smaller, for a sorted \\(v\\) this is \\[\n\\widetilde{v} = \\begin{cases}\n              v_{(n+1)/2}& n\\quad \\text{odd}\\\\\n              \\frac{v_{n/2} + v_{n/2+1}}{2}& n\\quad \\text{even}\n              \\end{cases},\n\\]\nmore general, we have quantiles. For a sorted \\(v\\) and \\(p\\in(0,1)\\) \\[\n\\overline{v}_p = \\begin{cases}\n               \\frac12\\left(v_{np} + v_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n              v_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n              \\end{cases}.\n\\] Some quantiles have special names, like the median for \\(p=0.5\\), the lower and upper quartile for \\(p=0.25\\) and \\(p=0.75\\) (or first, second (median) and third quartile), respectively.\n\n\n\n\n\n\n2.1.1 Visualization\n\n\n\n\n\n\nTip\n\n\n\nThere are various ways of visualizing data in Python. Two widely used packages are matplotlib and plotly.\n\n\nIt often helps to visualize the values to see differences and get an idea of their use.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nnm_sort = np.sort(data[\"nm\"])\nx = np.linspace(0, 1, len(nm_sort), endpoint=True,)\n\nplt.plot(x, nm_sort, label=\"net rent\")\nplt.axis((0, 1, np.round(nm_min/100)*100, np.round(nm_max/100)*100))\nplt.xlabel('Scaled index')\nplt.ylabel('Net rent - nm')\n\nplt.plot([0, 0.25, 0.25], [nm_quartiles[0], nm_quartiles[0], nm_min], \n         label='1st quartile')\nplt.plot([0, 0.5, 0.5], [nm_quartiles[1], nm_quartiles[1], nm_min],\n         label='2st quartile')\nplt.plot([0, 0.75, 0.75], [nm_quartiles[2], nm_quartiles[2], nm_min],\n         label='3st quartile')\nplt.plot([0, 1], [nm_mean, nm_mean],\n         label='mean')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the different measurements.\n\n\n\n\n\nWhat is shown in Figure 2.1 is often combined into a single boxplot (see Figure 2.2) that provides way more information at once.\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"Standard\"))\nfig.add_trace(go.Box(y=data[\"nm\"], name=\"With points\", boxpoints=\"all\"))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.2: Boxplot done in plotly with whiskers following 3/2 IQR.\n\n\n\n\nThe plot contains the box which is defined by the 1st quantile \\(Q_1\\) and the 3rd quantile \\(Q_3\\), with the median as line in between these two. Furthermore, we can see the whiskers which help us identify so called outliers. By default they are defined as \\(\\pm 1.5(Q_3 - Q_1)\\), where (\\(Q_3 - Q_1\\)) is often called the interquartile range (IQR).\n\n\n\n\n\n\nNote\n\n\n\nFigure 2.2 is an interactive plot in the html version.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#spread",
    "href": "basics/sets.html#spread",
    "title": "2  Data sets",
    "section": "2.2 Spread",
    "text": "2.2 Spread\nThe spread (or dispersion, variability, scatter) are measures used in statistics to classify how data is distributed. Common examples are variance, standard deviation, and the interquartile range that we have already seen above.\n\n\n\n\n\n\n\nDefinition 2.1 (Variance) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the variance is defined as \\[\n\\operatorname{Var}(v) = \\frac1n \\sum_{i=1}^n (v_i - \\mu)^2, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}\n\\] or directly \\[\n\\operatorname{Var}(v) = \\frac{1}{n^2} \\sum_{i=1}^n\\sum_{j&gt;i} (v_i - v_j)^2.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Standard deviation) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\frac1n \\sum_{i=1}^n (v_i - \\mu)^2}, \\quad \\mu = \\overline{v} \\quad\\text{(the mean)}.\n\\] If we interpret \\(v\\) as a sample this is often also called uncorrected sample standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.3 (Interquartile range (IQR)) For a finite set represented by a vector \\(v\\in\\mathbb{R}^n\\) the interquartile range is defined as the difference of the first and third quartile, i.e. \\[\nIQR = \\overline{v}_{0.75} - \\overline{v}_{0.25}.\n\\]\n\n\n\n\nWith numpy they are computed as follows\n\nnm_var = np.var(data[\"nm\"])\nprint(f\"{nm_var=}\")\n\nnm_std = np.std(data[\"nm\"])\nprint(f\"{nm_std=}\")\n\nnm_IQR = nm_quartiles[2] - nm_quartiles[0]\nprint(f\"{nm_IQR=}\")\n\nnm_var=np.float64(60208.75551600402)\nnm_std=np.float64(245.37472468859548)\nnm_IQR=np.float64(310.53000000000003)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#histogram",
    "href": "basics/sets.html#histogram",
    "title": "2  Data sets",
    "section": "2.3 Histogram",
    "text": "2.3 Histogram\nWhen exploring data it is also quite useful to draw histograms. For the net rent this makes not much sense but for rooms this is useful.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['rooms'])\nplt.xlabel('rooms')\nplt.ylabel('# of rooms')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of the number of rooms in our dataset.\n\n\n\n\n\nWhat we see in Figure 2.3 is simply the amount of occurrences of \\(1\\) to \\(6\\) in our dataset. Already we can see something rather interesting, there are flats with \\(5.5\\) rooms in our dataset.\nAnother helpful histogram is Figure 2.4 showing the amount of buildings built per year.\n\n\nShow the code for the figure\nindex = np.array(range(0, len(data['rooms'])))\n\nplt.hist(data['bj'])\nplt.xlabel('year of building')\nplt.ylabel('# of buildings')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.4: Histogram of buildings built per year.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "basics/sets.html#correlation",
    "href": "basics/sets.html#correlation",
    "title": "2  Data sets",
    "section": "2.4 Correlation",
    "text": "2.4 Correlation\nIn statistics, the terms correlation or dependence describe any statistical relationship between bivariate data (data that is paired) or random variables.\nFor our dataset we can, for example, check:\n\nthe living area in \\(m^2\\) - wfl vs. the net rent - nm\nthe year of construction - bj vs. if central heating - zh0 is available\nthe year of construction - bj vs. the city district - bez\n\n\n\nShow the code for the figure\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=data[\"wfl\"], y=data[\"nm\"], mode=\"markers\"),\n                row=1, col=1)\nfig.update_xaxes(title_text=\"living area in m^2\", row=1, col=1)\nfig.update_yaxes(title_text=\"net rent\", row=1, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"zh0\"], mode=\"markers\"),\n                row=2, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=2, col=1)\nfig.update_yaxes(title_text=\"central heating\", row=2, col=1)\n\nfig.add_trace(go.Scatter(x=data[\"bj\"], y=data[\"bez\"], mode=\"markers\"),\n                row=3, col=1)\nfig.update_xaxes(title_text=\"year of construction\", row=3, col=1)\nfig.update_yaxes(title_text=\"city district\", row=3, col=1)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2.5: Scatterplot to investigate correlations in the data set.\n\n\n\n\nIn the first plot of Figure 2.5 we see that the rent tends to go up with the size of the flat but there are for sure some rather cheap options in terms of space.\nThe second plot of Figure 2.5 tells us that central heating became a constant around \\(1966\\). Of course we can also guess that the older buildings with central heating were renovated, but we have no data to support this claim.\nThe third plot of Figure 2.5 does not yield an immediate correlation.\nMore formally, we can describe possible correlations using the covariance. The covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nDefinition 2.4 (Covariance) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the covariance is defined as \\[\n\\operatorname{cov}(v, w) = \\frac1n \\langle v -\\overline{v}, w - \\overline{w}\\rangle.\n\\]\n\n\n\n\nThe covariance is tricky to interpret, e.g. the unities of the two must not make sense. In the example below, we have rent per square meter, which makes some sense.\nFrom the covariance we can compute the correlation.\n\n\n\n\n\n\n\nDefinition 2.5 (Correlation) For two finite sets represented by vectors \\(v, w\\in\\mathbb{R}^n\\) the correlation is defined as \\[\n\\rho_{v,w} = \\operatorname{corr}(v, w) = \\frac{\\operatorname{cov}(v, w)}{\\sigma_v \\sigma_w},\n\\] where \\(\\sigma_v\\) and \\(\\sigma_w\\) are the standard deviation of these vectors, see Definition 2.2.\n\n\n\n\nIn numpy the function numpy.cov computes a matrix where the diagonal is the variance of the values and the off-diagonals are the covariances of the \\(i\\) and \\(j\\) samples. Consequently, numpy.corrcoef is a matrix as well.\n\ncov_nm_wfl = np.cov(data[\"nm\"], data[\"wfl\"])\nprint(f\"{cov_nm_wfl[0, 1]=}\")\n\ncorr_nm_wfl = np.corrcoef(data[\"nm\"], data[\"wfl\"])\nprint(f\"{corr_nm_wfl[0, 1]=}\")\n\ncov_nm_wfl[0, 1]=np.float64(4369.1195844122)\ncorr_nm_wfl[0, 1]=np.float64(0.7074626685750687)\n\n\nThe above results, particularly \\(\\rho_{\\text{nm},\\text{wfl}}=0.707\\) suggest that the higher the rent, the more space you get.\n\n\n\n\n\n\nTip\n\n\n\nCorrelation and causation are not the same thing!\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe showed some basic tests for correlation, there are more elaborate methods but they are subject to a later chapter.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data sets</span>"
    ]
  },
  {
    "objectID": "matrixdc/index.html",
    "href": "matrixdc/index.html",
    "title": "Matrix decompositions",
    "section": "",
    "text": "There are a lot of different matrix decompositions and they can be used to fulfil several tasks. We are going to look into the eigendecomposition as well as the singular value decomposition. Both of these can, for example, be used for picture compression and recognition.\nFor notation we are following again Golub and Van Loan (2013), which has plenty more to offer than we cover in these notes.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html",
    "href": "matrixdc/eigen.html",
    "title": "3  Eigendecomposition",
    "section": "",
    "text": "3.1 Examples for the application\nWe start off with the so called eigendecomposition. The main idea is to compute a decomposition (or factorization) of a square matrix into a canonical form. We will represent a matrix by its eigenvalues and eigenvectors. In order to do so we need to recall some more definitions from our favourite linear algebra book, such as Golub and Van Loan (2013).\nSome important properties of the determinant are:\nSo let us check in numpy:\nFrom the above properties of the determinant we can conclude that, if \\(X \\in \\mathbb{C}^{n\\times n}\\) is nonsingular and \\(B = X^{-1} A X\\), then \\(A\\) and \\(B\\) are called similar and two similar matrices have exactly the same eigenvalues.\nNot all matrices \\(A \\in \\mathbb{R}^{n\\times n}\\) are diagonalizable.\nWe can use what we have learned about the basis of a vector space and the transformation between two bases, see Section 1.3 to get a different interpretation of the eigendecomposition.\nWe recall, if \\(x\\) is represented in the standard basis with matrix \\(I\\) we can change to the basis represented by \\(V\\) and therefore \\(\\hat{x} = V^{-1}x\\).\nConsequently, the equation \\[\nA = V \\Lambda V^{-1}\n\\] means that in the basis created by \\(V\\) the matrix \\(A\\) is represented by a diagonal matrix.\nTo get a better idea what the eigendecomposition can do we look into some examples.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#examples-for-the-application",
    "href": "matrixdc/eigen.html#examples-for-the-application",
    "title": "3  Eigendecomposition",
    "section": "",
    "text": "3.1.1 Solving system of linear equations\nFor a system of linear equations \\(Ax=b\\) we get \\[\n\\begin{array}{lll}\nA x & = b  & \\iff \\\\\nV \\Lambda V^{-1} x & = b  & \\iff \\\\\n\\Lambda V^{-1} x & = V^{-1} b  & \\iff \\\\\nV^{-1} x & = \\Lambda^{-1}V^{-1} b  & \\iff \\\\\nx & = V\\Lambda^{-1}V^{-1} b  & \\iff\n\\end{array}\n\\] As \\(\\Lambda^{-1} = \\operatorname{diag}\\left(\\lambda_1^{-1}, \\ldots, \\lambda_n^{-1}\\right)\\) this is easy to compute once we have the eigenvalue decomposition.\n\n\n\n\n\n\nNote\n\n\n\nThe computation of the eigendecomposition is not cheap, therefore this is not always worth the effort and there are other ways of solving linear systems.\n\n\n\n\n3.1.2 Linear Ordinary Differential Equations\nIn this example we use the eigendecomposition to efficiently solve a system of differential equations \\[\n\\dot{x} = A x,\\quad x(0) = x_0\n\\] By changing to the basis \\(V\\) and using the notation \\(\\hat{x}=z\\) we have the equivalent formulations \\[\nz = V^{-1}x \\iff x = Vz,\n\\] and if follows \\[\n\\begin{array}{lll}\n\\dot{x} = A x & \\iff V \\dot{z} &= A V z \\\\\n              & \\iff \\dot{z} &= V^{-1} A V z \\\\\n              & \\iff \\dot{z} &= \\Lambda z\n\\end{array}.\n\\] So for an initial value \\(z_0\\) the solution in \\(t\\) is \\[\nz(t) = \\operatorname{diag}\\left(e^{t\\lambda_1}, \\ldots, e^{t\\lambda_n}\\right) z_0.\n\\]\nWe often say that it is now a decoupled differential equation.\n\n\n3.1.3 Higher Order Linear Differential Equations\nIf we have a higher order linear ODE such as \\[\nx^{(n)} + a_{n-1} x^{(n-1)} + \\cdots + a_2 \\ddot{x} + a_1 \\dot{x} + a_0 x = 0.\n\\tag{3.2}\\] we can stack the derivatives into a vector \\[\n\\begin{array}{ccc}\nx_1 & = & x\\\\\nx_2 & = & \\dot{x}\\\\\nx_3 & = & \\ddot{x}\\\\\n\\vdots & = & \\vdots \\\\\nx_{n-1} & = & x^{(n-2)} \\\\\nx_{n} & = & x^{(n-1)} \\\\\n\\end{array}\n\\quad\n\\Leftrightarrow\n\\quad\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{c} x \\\\ \\dot{x} \\\\ \\ddot{x} \\\\ \\vdots \\\\ x^{(n-2)} \\\\ x^{(n-1)} \\end{array}\n\\right],\n\\] and taking the derivative of this vector yields the following system \\[\n\\underbrace{\n\\frac{d}{d t}\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_{\\dot{x}}\n=\n\\underbrace{\\left[\n    \\begin{array}{cccccc} 0 & 1 & 0 & \\dots & 0 & 0\\\\\n                          0 & 0 & 1 & \\dots & 0 & 0\\\\  \n                          0 & 0 & 0 & \\dots & 0 & 0\\\\\n                          \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n                          0 & 0 & 0 & \\dots & 0 & 1\\\\\n                          -a_0 & -a_1 & -a_2 & \\dots & -a_{n-2} & -a_{n-1}\\\\\n    \\end{array}\n\\right]\n}_{A}\n\\underbrace{\n\\left[\n    \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\n\\right]\n}_x.\n\\]\nWe transformed it into a system of coupled 1st order ODEs \\(\\dot{x}=Ax\\) and we can solve this as seen above. More importantly, the characteristic polynomial of Equation 3.2 is equal to the characteristic polynomial of Definition 3.2 and the eigenvalues are the roots of this polynomial.\n\n\n3.1.4 Generalized eigenvalue problem\nLet us motivate this by the example of modal analysis. If we consider the free vibrations of a undamped system we get the equation \\[\nM\\ddot{u} + K u = 0, \\quad u(0) = u_0,\n\\] with the mass matrix \\(M\\) and the stiffness matrix \\(K\\) and \\(u(t)\\) being the displacement. As we know, the solution of this linear differential equation has the form \\(u(t)=e^{i\\omega t}u_0\\) and thus we get \\[\n(-\\omega^2 M + K) u_0 = 0.\n\\tag{3.3}\\]\n\n\n\n\n\n\n\nDefinition 3.5 (Generalized eigenvalue problem) If \\(A, B \\in \\mathbb{C}^{n\\times n}\\), then the set of all matrices of the form \\(A-\\lambda B\\) with \\(\\lambda\\in\\mathbb{C}\\) is a pencil. The generalized eigenvalues of \\(A-\\lambda B\\) are elements of the set \\(\\lambda(A,B)\\) defined by \\[\n\\lambda(A,B) = \\{z\\in\\mathbb{C}: \\det(A-zB)=0\\}.\n\\] If \\(\\lambda \\in \\lambda(A,B)\\) and \\(0\\neq v\\in\\mathbb{C}^n\\) satisfies \\[\nA v = \\lambda B v,\n\\tag{3.4}\\] then \\(v\\) is an eigenvector of \\(A-\\lambda B\\). The problem of finding a nontrivial solution to Equation 3.4 is called the generalized eigenvalue problem.\n(Compare Golub and Van Loan 2013, chap. 7.7)\n\n\n\n\nIn our example Equation 3.3 the eigenvalues are \\(\\lambda = \\omega^2\\) and correspond to the square of the natural frequencies and the eigenvectors \\(v=u\\) correspond to the modes of vibration.\nIf \\(M\\) is invertible we can write \\[\nM^{-1}(K -\\omega^2 M ) u_0 = (M^{-1}K -\\omega^2 I ) u_0 = 0.\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nIn most cases inverting the matrix is not recommended, to directly solve the generalized eigenvalue problem, see (Golub and Van Loan 2013, chap. 7.7) for details.\n\n\nLet us walk through this with an example from (Downey and Micheli 2024, sec. 8.3).\n\n\n\n\n\n\n\nExample 3.1 (Two Story Building)  \n\n\n\n\n\n\nFigure 3.1: Two story frame where the floors have different dynamic properties.\n\n\n\nIn Figure 3.1 we can see a two story building consisting of a two column frame with a floor. The first floor columns are fixed at the base and have a height \\(h\\), the second floor is fixed to the first and has the same height. The columns of each frame are modelled as beams with flexural rigidity \\(EI\\) and \\(2 EI\\), respectively Where \\(E\\) is called Young’s modulus and \\(I\\) the second moment of area. The mass is centred at the floor level.\nThis allows us to model such a building as a system with two degrees of freedom \\(x_1\\) and \\(x_2\\) as the displacement indicated in blue. The dashed blue line would be such a displacement for the frame.\nThe resulting equations of motion become \\[\n\\begin{array}{r}\nm_1 \\ddot{x_1} + k_1 x_1 + k_2 (x_2 - x_1) = 0,\\\\\nm_2 \\ddot{x_1} + k_2 (x_2 - x_1) = 0,\n\\end{array}\n\\] resulting in the matrices \\[\nM = \\left[\n\\begin{array}{cc}\nm_1 & 0 \\\\\n0 & m_2\n\\end{array}\n\\right], \\quad\nK = \\left[\\begin{array}{cc}\nk_1 + k_2 & -k_2 \\\\\n-k_2 & k_2\n\\end{array}\n\\right].\n\\]\nFor the derivation of the stiffness coefficients we refer to (Downey and Micheli 2024, 188) and recall the result here as follows \\[\nk_1 = \\frac{48 EI}{h^3}, \\quad k_2 = \\frac{24 EI}{h^3}.\n\\] This results in the stiffness matrix \\[\nK = \\underbrace{\\frac{24 EI}{h^3}}_{=k}\n\\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right].\n\\]\nNow we can manually compute \\[\n\\det(-\\omega^2 M + K) = 0 \\Leftrightarrow 2m^2\\omega^4 - 5 m k \\omega^2 + 2 k^2 = 0,\n\\] and solving this equation for \\(\\omega^2\\) results in \\[\n\\omega_1^2 = \\frac{k}{2m}, \\quad \\omega_2^2 = \\frac{2k}{m}.\n\\]\nTo compute the eigenvectors \\(v_1\\) and \\(v_2\\) we use Equation 3.1, i.e. for \\(v_1\\) this reads as \\[\n-\\frac{k}{2m} \\left[\n\\begin{array}{cc}\n2m & 0 \\\\\n0 & m\n\\end{array}\n\\right]\n+\nk \\left[\\begin{array}{cc}\n3 & -1 \\\\\n-1 & 1\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n=\n\\left[\\begin{array}{cc}\n2 k & -k \\\\\n-k & \\frac{k}{2}\n\\end{array}\n\\right]\n\\left[\\begin{array}{c}\nv_{11}\\\\\nv_{21}\n\\end{array}\n\\right]\n\\overset{!}{=}\n\\left[\\begin{array}{c}\n0\\\\\n0\n\\end{array}\n\\right],\n\\] and results in the relation \\(2v_{11} = v_{21}\\). We can select a solution as \\[\nv_1 = \\left[\\begin{array}{c}\n\\frac12\\\\\n1\n\\end{array}\n\\right].\n\\] For \\(v_2\\) we proceed similarly and derive a solution as: \\[\nv_2 = \\left[\\begin{array}{c}\n-1\\\\\n1\n\\end{array}\n\\right].\n\\]\nThe eigenvectors illustrate how the displacement functions and are not just some theoretical value. The following figure visualizes the two modes.\n\n\n\n\n\n\n\n\n\n\n\n(a) First mode\n\n\n\n\n\n\n\n\n\n\n\n(b) Second mode\n\n\n\n\n\n\n\nFigure 3.2: Model of a two story building with the shape of the modes according to the modal analysis.\n\n\n\nTwo wrap up the example our overall temporal response consists of the time invariant part defined by our eigenvectors \\(v_1\\), and \\(v_2\\), as well as the time dependent part with our eigenfrequencies \\(\\omega_1\\) and \\(\\omega_2\\) as well as the constants \\(A_1\\), \\(A_2\\), \\(\\phi_1\\), \\(\\phi_2\\) depending on the initial condition (see Downey and Micheli 2024, chap. 5). \\[\n\\left[\\begin{array}{c}\nx_1(t)\\\\\nx_2(t)\n\\end{array}\n\\right]\n=\n\\left[\nv_1, v_2\n\\right]\n\\left[\\begin{array}{c}\nA_1 \\sin(\\omega_1 t + \\phi_1)\\\\\nA_2 \\sin(\\omega_2 t + \\phi_2)\n\\end{array}\n\\right]\n\\]\n(Compare Downey and Micheli 2024, chap. 8.3, pp. 189-191)\n\n\n\n\n\n\n3.1.5 Low-rank approximation of a square matrix\nWe can use the eigenvalue decomposition to approximate a square matrix.\nLet us sort the eigenvalues in \\(\\Lambda\\) and let us call \\(U^{\\mathsf{T}}=V^{-1}\\) then we can write \\[\nA = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^n\\lambda_i v_i u_i^{\\mathsf{T}}\n\\] where \\(v_i\\) and \\(u_i\\) correspond to the rows of the matrices. Now we can define the rank \\(r\\) approximation of \\(A\\) as \\[\nA\\approx A_r = V\\Lambda U^{\\mathsf{T}} = \\sum_{i=1}^r\\lambda_i v_i u_i^{\\mathsf{T}}.\n\\]\nTo make this a bit easier to understand the following illustration is helpful:\n\n\n\nLow Rank Approximation\n\n\nThis approximation can be used to reduce the storage demand of an image.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = [\"svg\"]\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_cut = im_gray[1500:3001,1500:3001] / 255\n\nlam_, V_ = LA.eig(im_cut)\norder = np.argsort(np.abs(lam_))\nlam = lam_[order[::-1]]\nV = V_[:, order[::-1]]\n\nrec = [1/1000, 10/100, 25/100, 50/100, 75/100]\nVinv = LA.inv(V)\n\nplt.figure()\nplt.plot((np.abs(lam)))\nplt.yscale(\"log\")\nplt.ylabel(r\"$|\\lambda_i|$\")\nplt.xlabel(\"index\")\nplt.gca().set_aspect(7e1)\n\nfor p in rec:\n    plt.figure()\n    r = int(np.ceil(len(lam) * p))\n    A_r = np.real(V[:, 0:r] @ np.diag(lam[0:r], 0) @ Vinv[0:r, :])\n    plt.imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    plt.gca().set_axis_off()\n\nplt.figure()\nplt.imshow(im_cut, cmap=plt.get_cmap(\"gray\"))\nplt.gca().set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Absolute value of Eigenvalues from largest to smallest\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) 0.1% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(c) 10% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(d) 25% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 50% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(f) 75% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(g) original image\n\n\n\n\n\n\n\nFigure 3.3: Image of MCI I and the reconstruction with approximated matrix.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/eigen.html#summary",
    "href": "matrixdc/eigen.html#summary",
    "title": "3  Eigendecomposition",
    "section": "3.2 Summary",
    "text": "3.2 Summary\nThe eigenvalue decomposition is an important tool but it has its limitations:\n\nthe matrices involved need to be square\neigenvalues might be complex, even if the problem at hand is real\nwe only get a diagonal matrix \\(\\Lambda\\) if all eigenvectors are linear independent\nthe computation of \\(V^{-1}\\) is non-trivial unless \\(A\\) is symmetric and \\(V\\) becomes unitary (\\(V^{-1} = V^{\\mathsf{T}}\\)).\n\nTherefore, we will look into a generalized decomposition called the singular value decomposition in the next section.\n\n\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A Practical Introduction for Mechanical, Civil, and Aerospace Engineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Eigendecomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html",
    "href": "matrixdc/svd.html",
    "title": "4  Singular Value Decomposition",
    "section": "",
    "text": "4.1 Low rank approximation\nThe Singular Value Decomposition (SVD) is an orthogonal matrix reduction. In contrast to the previously discussed Eigendecomposition it can be applied to rectangular matrices. As we will see the interpretation of the SVD can be linked to the eigendecomposition but first, lets provide a proper definition.\nInstead of providing a concise proof (if you are interested see (Golub and Van Loan 2013, 76)) we show a possible motivation of the definition.\nIf we compute the eigendecompositions of \\(AA^{\\mathsf{T}}\\) and \\(A^{\\mathsf{T}}A\\). The two matrices have the same positive eigenvalues - the squares of the eigenvalues of \\(A\\) and we get\n\\[\n\\begin{array}{lll} (A A^{\\mathsf{T}})U &= & U(\\Lambda\\Lambda^{\\mathsf{T}}),\\\\\n                   (A^{\\mathsf{T}} A)V &= & V(\\Lambda^{\\mathsf{T}}\\Lambda).\n                   \\end{array}\n\\] For \\(A \\in \\mathbb{R}^{m\\times n}\\) with \\(m&gt;n\\) we get \\[\n\\Lambda = \\left[\n    \\begin{array}{c} \\tilde{\\Lambda}\\\\0\\end{array}\n\\right]\n\\] with the diagonal matrix \\(\\tilde\\Lambda\\in\\mathbb{R}^{n\\times n}\\) and \\[\n\\begin{array}{llclc} \\Lambda\\Lambda^{\\mathsf{T}} & = & \\left[\n    \\begin{array}{cc}\\tilde{\\Lambda}^2 & 0 \\\\0 & 0 \\end{array}\n\\right]  & = & \\left[\n    \\begin{array}{cc}\\tilde{\\Sigma} & 0 \\\\0 & 0 \\end{array}\n\\right],  \\\\\n        \\Lambda^{\\mathsf{T}}\\Lambda & = & \\tilde{\\Lambda}^2 &= &\\tilde{\\Sigma}.\n                   \\end{array}\n\\] If we expand the matrices with zeros to match the correct dimensions this corresponds to our singular value decomposition \\[\nA = U \\Sigma V^{\\mathsf{T}}.\n\\]\nAgain, to visualize the composition helps to better understand what is happening\nIn the case that of \\(m\\geq n\\) we can save storage by reducing the matrices \\(U\\) and \\(\\Sigma\\), to their counterpart \\(U_1\\) and \\(\\Sigma_1\\) by removing the zeros.\nIn Python we can compute the (Thin)SVD as follows\nAgain, we can cut of the reconstruction at a certain point and create an approximation. More formally this is defined in the next definition.\nWe can use this for image compression.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n\nim_gray = rgb2gray(im)\nim_scale = im_gray[1500:3001, 1500:3001] / 255\n\nU, s, Vh = LA.svd(im_scale, full_matrices=False)\n\nrec = [1/1000, 10/100, 25/100, 50/100, 75/100]\n\nplt.figure()\nplt.plot(s)\nplt.yscale(\"log\")\nplt.ylabel(r\"$|\\sigma_i|$\")\nplt.xlabel(\"index\")\nplt.gca().set_aspect(5e1)\n\nfor p in rec:\n    plt.figure()\n    r = int(np.ceil(len(s) * p))\n    A_r = U[:, :r] @ np.diag(s[:r]) @ Vh[:r, :]\n    plt.imshow(A_r, cmap=plt.get_cmap(\"gray\"))\n    plt.gca().set_axis_off()\n\nplt.figure()\nplt.imshow(im_scale, cmap=plt.get_cmap(\"gray\"))\nplt.gca().set_axis_off()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Singular values from largest to smallest\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) 0.1% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(c) 10% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(d) 25% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) 50% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(f) 75% of eigenvalue\n\n\n\n\n\n\n\n\n\n\n\n(g) original image\n\n\n\n\n\n\n\nFigure 4.1: Image of MCI I and the reconstruction with reduced rank matrices.\nAs the matrices \\(U\\) and \\(V\\) are orthogonal, they also define a basis of the corresponding (sub) vector spaces. As mentioned before, the SVD automatically selects these and they are optimal.\nConsequently, the matrices \\(U\\) and \\(V\\) can be understood as reflecting patterns in the image. We can think of the columns of \\(U\\) and \\(V\\) as the vertical respectively horizontal patterns of \\(A\\).\nWe can illustrate this by looking at the modes of our decomposition \\[\nM_k = U(:, k) V^{\\mathsf{T}}(k, :).\n\\]\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\n\nrec = [0, 1, 2, 3, 4, 5]\n\nfor r in rec:\n    plt.figure()\n    M_r = np.outer(U[:, r], Vh[r, :])\n    plt.imshow(M_r, cmap=plt.get_cmap(\"gray\"))\n    plt.gca().set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) First mode\n\n\n\n\n\n\n\n\n\n\n\n(b) Second mode\n\n\n\n\n\n\n\n\n\n\n\n(c) Third mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Fourth mode\n\n\n\n\n\n\n\n\n\n\n\n(e) Fifth mode\n\n\n\n\n\n\n\n\n\n\n\n(f) Sixth mode\n\n\n\n\n\n\n\nFigure 4.2: Modes of the SVD decomposition of the MCI I image.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#low-rank-approximation",
    "href": "matrixdc/svd.html#low-rank-approximation",
    "title": "4  Singular Value Decomposition",
    "section": "",
    "text": "Definition 4.2 (Low-Rank Approximation) If \\(A \\in \\mathbb{R}^{m\\times n}\\) and has the SVD \\(A = U\\Sigma V^{\\mathsf{T}}\\) than \\[\nA_k = U(:, 1:k)\\, \\Sigma(1:k, 1:k)\\, V^{\\mathsf{T}}(1:k, :)\n\\] is the optimal low-rank approximation of \\(A\\) with rank \\(k\\). This is often called the truncated SVD.\n(See Golub and Van Loan 2013, Corollary 2.4.7 p. 79)\n\n\n\n\n\n\n\nTruncated SVD\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf we compare this to Figure 3.3 we can see that we get a much better result for smaller \\(r\\). Let us have a look why.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe big advantage here is, that the selection is optimal. A disadvantage is that the need to store the basis separately and this increases the necessary storage. We will see in later sections about wavelets and Fourier decomposition how a common basis can be used to reduce the storage by still keeping good reconstructive properties.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#sec-matrixdc-pca",
    "href": "matrixdc/svd.html#sec-matrixdc-pca",
    "title": "4  Singular Value Decomposition",
    "section": "4.2 Principal Component Analysis",
    "text": "4.2 Principal Component Analysis\nOn of the most important applications of SVD is in the stable computation of the so called principal component analysis (PCA). It is a common technique in data exploration, analysis, visualization, and preprocessing.\nThe main idea of PCM is to transform the data in such a way that the main directions (principal components) capture the largest variation. In short we perform a change of the basis, see Definition 1.8.\nLet us investigate this in terms of a (artificial) data set.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: Noisy Gaussian Data, pp. 25-27).\n\n\nWe generate a noisy cloud (see Figure 4.3) that consists of \\(10000\\) points in 2D, generated from a normal distribution with zero mean and unit variance. The data is than:\n\nscaled by \\(2\\) in the first direction and by \\(\\frac12\\) in second,\nrotated by \\(\\frac\\pi3\\)\ntranslation in the direction \\(\\left[2\\ 1\\right]^{\\mathsf{T}}\\).\n\nThe resulting matrix \\(X\\) is a long and skinny matrix with each measurement (or experiment) stacked next to each other. This means, each column represents a new set, e.g. a time step, and each row corresponds to the same sensor.\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is a slight adaptation (for nicer presentation in these notes) of the (Brunton and Kutz 2022, Code 1.4) also see notebook on github.\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nxC = np.array([2, 1])      # Center of data (mean)\nsig = np.array([2, 0.5])   # Principal axes\n\ntheta = np.pi / 3            # Rotate cloud by pi/3\n\nR = np.array([[np.cos(theta), -np.sin(theta)],     # Rotation matrix\n              [np.sin(theta), np.cos(theta)]])\n\nnPoints = 10000            # Create 10,000 points\nX = R @ np.diag(sig) @ np.random.randn(2, nPoints) + \\\n        np.diag(xC) @ np.ones((2, nPoints))\n\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(X[0, :], X[1, :], \".\", color=\"k\")\nax1.grid()\nax1.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\n## f_ch01_ex03_1b\n\nXavg = np.mean(X, axis=1)                  # Compute mean\nB = X - np.tile(Xavg, (nPoints, 1)).T       # Mean-subtracted data\n\n# Find principal components (SVD)\nU, S, VT = np.linalg.svd(B, full_matrices=False)\nS = S / np.sqrt(nPoints - 1)\n\nax2 = fig.add_subplot(122)\nax2.plot(X[0, :], X[1, :], \".\", color=\"k\")   # Plot data to overlay PCA\nax2.grid()\nax2.set_aspect(\"equal\")\nplt.xlim((-6, 8))\nplt.ylim((-6, 8))\n\ntheta = 2 * np.pi * np.arange(0, 1, 0.01)\n\n# 1-std confidence interval\nXstd = U @ np.diag(S) @ np.array([np.cos(theta), np.sin(theta)])\n\nax2.plot(Xavg[0] + Xstd[0, :], Xavg[1] + Xstd[1, :],\n        \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 2 * Xstd[0, :], Xavg[1] + 2 * Xstd[1, :],\n        \"-\", color=\"r\", linewidth=3)\nax2.plot(Xavg[0] + 3 * Xstd[0, :], Xavg[1] + 3 * Xstd[1, :],\n        \"-\", color=\"r\", linewidth=3)\n\n# Plot principal components U[:,0]S[0] and U[:,1]S[1]\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,0] * S[0]]),\n         np.array([Xavg[1], Xavg[1] + U[1,0] * S[0]]),\n         \"-\", color=\"cyan\", linewidth=5)\nax2.plot(np.array([Xavg[0], Xavg[0] + U[0,1] * S[1]]),\n         np.array([Xavg[1], Xavg[1] + U[1,1] * S[1]]),\n         \"-\", color=\"cyan\", linewidth=5)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.3: Principal components of the mean-subtracted Gaussian data on the left as, as well as the first three standard deviation ellisoids and the two scaled left singular vectors.\n\n\n\n\n\n\n4.2.1 Computation\nFor the computation we follow the outline given in (Brunton and Kutz 2022, chap. 1.5). First we need to center our matrix \\(X\\) according to the mean per feature, in our case per row. \\[\n\\overline{x}_j = \\frac1n \\sum_{i=1}^n X_{ij}\n\\] and our mean matrix is the outer product with the one vector \\[\n\\overline{X} = \\left[\\begin{array}{c}1\\\\\\vdots\\\\1\\end{array}\\right] \\overline{x}\n\\] which can be used to compute the centred matrix \\(B = X - \\overline{X}\\).\nThe PCA is the eigendecomposition of the covariance matrix \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}} B\n\\tag{4.2}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe normalization factor of \\(n-1\\) in Equation 4.2 an not \\(n\\) is called Bassel’s correction and compensates for the bias in the estimation of the population variance.\n\n\nAs \\(C\\) is symmetric and positive semi-definite, therefore it has non-negative real eigenvalues and the matrix \\(V\\) of the eigendecomposition satisfies \\(V^{-1} = V^{\\mathsf{T}}\\) (i.e. it is orthogonal Definition 1.10). The principal components are the eigenvectors and the eigenvalue are the variance along these components.\nIf we instead compute the SVD of \\(B = U\\Sigma V^{\\mathsf{T}}\\) we get \\[\nC = \\frac{1}{n-1} B^{\\mathsf{T}}B = \\frac{1}{n-1} V \\Sigma V^{\\mathsf{T}} = \\frac{1}{n-1} V (\\Lambda^{\\mathsf{T}}\\Lambda) V^{\\mathsf{T}}\n\\] leading to a way of computing the principal components in a robust way as \\[\n\\lambda_k = \\frac{\\sigma_k^2}{n-1}.\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIf the sensor ranges of our matrix are very different in magnitude the correlation matrix is scaled by the row wise standard deviation of \\(B\\) similar as for the mean.\n\n\nIn our example we get our scaled \\(\\sigma_1=1.988\\approx 2\\) and \\(\\sigma_2=0.496\\approx \\frac12\\). These results recover our given parameters very well. Additionally we can see that our rotation matrix is closely matched by \\(U\\) (up to signs) from our SVD: \\[\nR_{\\frac\\pi3} = \\left[\n\\begin{array}{cc} 0.5&0.866\\\\-0.866&0.5\\end{array}\n\\right], \\quad U = \\left[\n\\begin{array}{cc}-0.501&-0.865\\\\-0.865&0.501\\end{array} \\right]\n\\]\n\n\n4.2.2 Example Eigenfaces\nWe combine SVD/PCA in a illustrative example called eigenfaces as introduced in (Brunton and Kutz 2022, Sec 1.6, pp. 28-34).\nThe idea is to apply the PCA techniques to a large set of faces to extract the dominate correlations between the images and create a face basis that can be used to represent an image in these coordinates. For example you can reconstruct a face in this space by projecting onto the eigen vectors or it can be used for face recognition as similar faces usually cluster under this projection.\nThe images are taken from the Yale Face Dataset B, in our case we use a GitHub that provides Julia Pluto notebooks for Chapter 1 to 4 of Brunton and Kutz (2022).\nOur training set, so to speak, consists of the first 36 people in the dataset. We compute the average face and subtract it from our dataset to get our matrix \\(B\\). From here a SVD provides us with our basis \\(U\\). To test our basis we use individual 37 and a portion of the image of the MCI Headquarter (to see how well it performs on objects). For this we use the projection \\[\n\\tilde{x} = U_r U_r^{\\mathsf{T}} x.\n\\] If we split this up, we first project onto our found patterns (encode) and than reconstruct from them (decode).\n\n\n\n\n\n\nNote\n\n\n\nWe can understand this as encoding and decoding our test image, which is the general setup of an autoencoder (a topic for another lecture).\nThe correlation coefficients \\(x_r = U_r^{\\mathsf{T}} x\\) might reveal patterns for different \\(x\\). In the case of faces, we can use this for face recognition, i.e. if the coefficients of \\(x_r\\) are in the same cluster as other images, they are probably from the same person.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following code is an adaptation of the (Brunton and Kutz 2022, Code 1.7 and 1.9).\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nresponse = requests.get(\n    \"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos\"\n    \"/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\ntrainingFaces = faces[:, : np.sum(nfaces[:36])]\navgFace = np.mean(trainingFaces, axis=1)\n\nB = trainingFaces - np.tile(avgFace, (trainingFaces.shape[1], 1)).T\nU, _, _ = LA.svd(B, 'econ')\n\ntestFace = faces[:, np.sum(nfaces[:36])]\ntestFaceMS = testFace - avgFace\nrec = [25, 100, 400]\n\nfig = plt.figure()\naxs = [] \naxs.append(fig.add_subplot(2, 4, 1))\naxs.append(fig.add_subplot(2, 4, 2))\naxs.append(fig.add_subplot(2, 4, 3))\naxs.append(fig.add_subplot(2, 4, 4))\naxs.append(fig.add_subplot(2, 4, 5))\naxs.append(fig.add_subplot(2, 4, 6))\naxs.append(fig.add_subplot(2, 4, 7))\naxs.append(fig.add_subplot(2, 4, 8))\n\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[i].set_axis_off()\n    axs[i].set_title(f\"${r=}$\")\n\naxs[3].imshow(np.reshape(testFace, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\naxs[3].set_axis_off()\naxs[3].set_title(f\"Original image\")\n\nshift = 1500\ntestFaceMS = np.reshape(im_gray[shift:shift+n, shift:shift+m].T, n*m) - \\\n             avgFace\nrec = [100, 400, 1600]\nfor i, p in enumerate(rec):\n    r = p\n    A_r = avgFace + U[:, :r] @ U[:, :r].T @ testFaceMS\n    axs[4 + i].imshow(np.reshape(A_r, (m, n)).T, cmap=plt.get_cmap(\"gray\"))\n    axs[4 + i].set_axis_off()\n    axs[4 + i].set_title(f\"${r=}$\")\naxs[7].imshow(im_gray[shift:shift+n, shift:shift+m], cmap=plt.get_cmap(\"gray\"))\naxs[7].set_axis_off()\naxs[7].set_title(f\"Original image\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.4: Approximate reconstruction of a test face and an object using the eigenfaces basis for different order r.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDue to resource limitations the above computation can not be done for each build. We try to make sure that the code matches the image but if something is different if you try it yourself we apologise for that.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "matrixdc/svd.html#further-applications-of-the-svd",
    "href": "matrixdc/svd.html#further-applications-of-the-svd",
    "title": "4  Singular Value Decomposition",
    "section": "4.3 Further applications of the SVD",
    "text": "4.3 Further applications of the SVD\nThere are many more applications of the SVD but we want to highlight some regarding systems of linear equations, \\[\nA x = b\n\\tag{4.3}\\] where the matrix \\(A\\), as well as the vector \\(b\\) is known an \\(x\\) is unknown.\nDepending on the structure of \\(A\\) and the specific \\(b\\) we have no, one, or infinitely many solutions. For now the interesting case is where \\(A\\) is rectangular and therefore we have either an\n\nunder-determined system \\(m\\ll n\\), so more unknowns than equations,\nover-determined system \\(m\\gg n\\), so more equations than unknowns.\n\nFor the second case (more equations than unknowns) we often switch to solving the optimization problem that minimizes \\[\n\\|Ax-b\\|_2^2.\n\\tag{4.4}\\] This is called the least square solution. The least square solution will also minimize \\(\\|Ax-b\\|_2\\). For an under-determined system we might seek the solution which minimizes \\(\\|x\\|_2\\) called the minimum norm solution.\nIf we us the SVD decomposition for \\(A = U \\Sigma V^{\\mathsf{T}}\\) we can define the following\n\n\n\n\n\n\n\nDefinition 4.3 (Pseudo-inverse) We define the matrix \\(A^\\dagger \\in \\mathbb{R}^{m\\times n}\\) by \\(A^\\dagger = V\\Sigma^\\dagger U^{\\mathsf{T}}\\) where \\[\n\\Sigma^\\dagger = \\operatorname{diag}\\left(\\frac{1}{\\sigma_1}. \\frac{1}{\\sigma_2}, \\ldots, \\frac{1}{\\sigma_r}, 0, \\ldots, 0\\right) \\in \\mathbb{R}^{m\\times n}, \\quad r=\\operatorname{rank}(A).\n\\]\nThe matrix \\(A^\\dagger\\) is often called the Moore-Penrose left pseudo-inverse as it fulfils the Moore-Penrose conditions conditions. It is also the matrix to provides the minimal Frobenius norm solution to \\[\n\\min_{X \\in \\mathbb{R}^{m\\times n}}\\| A X - I_n\\|_F.\n\\]\n(Compare Golub and Van Loan 2013, 290)\n\n\n\n\nIf we only use the truncated version, i.e. where we only use non-zero singular values, we can use it to find good solutions to Equation 4.4.\n\nIn numpy it can be computed by numpy.linalg.pinv.\n\n\n\n\n\n\n\n\nDefinition 4.4 (Condition number) The condition number of a matrix provides a measure how sensitive the solution of Equation 4.3 is to perturbations in \\(A\\) and \\(b\\). For a square matrix \\(A\\) the condition number is defined as \\[\n\\kappa(A) = \\|A\\| \\left\\|A^{-1}\\right\\|,\n\\] for an appropriate underlying norm. For the 2-norm \\(\\kappa_2\\) is \\[\n\\kappa_2(A) = \\|A\\|_2 \\left\\|A^{-1}\\right\\|_2 = \\frac{\\sigma_{max}}{\\sigma_{min}}.\n\\]\nTo get a better idea on what this means think of it in this way. For the perturbed linear system \\[\nA(x + \\epsilon_x) = b + \\epsilon_b,\n\\] we can outline the worst case, where \\(\\epsilon_x\\) aligns with the singular vector of the largest singular vector and \\(x\\) with the smallest singular value, i.e. \\[\nA(x + \\epsilon_x) = \\sigma_{min}x + \\sigma_{max}\\epsilon_x.\n\\] Consequently, the output signal-to-noise \\(\\|b\\|/\\|\\epsilon_b\\) is equivalent with the input signal-to-noise \\(\\|x\\|/\\|\\epsilon_x\\) and the factor between those two is \\(\\kappa_2(A)\\).\nIn this sense \\(\\kappa_2\\) can be extended for more general matrices.\n(Compare Golub and Van Loan 2013, 87; and Brunton and Kutz 2022, 18–19)\n\n\n\n\n\n4.3.1 Linear regression with SVD\nBefore we go into more details about regression in the next section we give a brief outlook in terms of how to solve such a problem with SVD.\n\n\n\n\n\n\nImportant\n\n\n\nThis example is adapted from (Brunton and Kutz 2022, Example: One-Dimensional Linear Regression, Example: Cement Heat Generation Data, pp. 19-22).\n\n\n\n4.3.1.1 Linear Regression (see Brunton and Kutz 2022, 19–21)\nFirst we just take a linear correlation that we augment with some Gaußian Noise. So our matrix \\(A\\) is simple a vector with our \\(x\\)-coordinates and \\(b\\) is the augmented image under our linear correlation. \\[\n\\left[\n    \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{array}\n\\right]x =\n\\left[\n    \\begin{array}{c} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{array}\n\\right]\n\\quad\n\\Leftrightarrow\n\\quad\nU\\Sigma V^{\\mathsf{T}} x = b\n\\quad\n\\Leftrightarrow\n\\quad\nx = A^\\dagger b\n\\]\nFor this example \\(\\Sigma = \\|a\\|_2\\), \\(V=1\\), and \\(U=\\tfrac{a}{\\|a\\|_2^2}\\). This is basically just the projection of \\(b\\) along our basis \\(a\\) and this is \\[\nx = \\frac{a^{\\mathsf{T}} b}{a^{\\mathsf{T}} a}.\n\\]\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)       # Make sure to stay reproducible\n\nk = 3\nA = np.arange(-2, 2, 0.25).reshape(-1, 1)\nb = k*A + np.random.randn(*A.shape) * 0.5\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(A, k*A, color=\"k\", label=\"Target\")\nplt.plot(A, b, 'x', color=\"r\", label=\"Noisy data\")\nplt.plot(A, A*x, '--', color=\"b\", label=\"Regression line\")\nplt.legend()\nplt.xlabel(\"$a$\")\nplt.ylabel(\"$b$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.5: Linear regression with SVD.\n\n\n\n\n\nOur reconstructed unknown \\(x=\\) 2.855 and is a reasonable good match for \\(k=\\) 3.0\n\n\n4.3.1.2 Multi-Linear Regression (see Brunton and Kutz 2022, 21–23)\nThe second example is based on the Portland Cement Data build in with MATLAB. In Python we again use the dataset provided on GitHub. The data set contains the heat generation during the hardening of 12 cement mixtures comprised of 4 basic ingredients, i.e. \\(A\\in \\mathbb{R}^{13\\times 4}\\). The aim is to determine the weights \\(x\\) that relate the proportion of the ingredients to the heat generation in the mixture.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy.linalg as LA\nimport requests\nimport io\n%config InlineBackend.figure_formats = ['svg']\n\n# Transform the content of the file into a numpy.ndarray\nresponse = requests.get(\n    \"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos\"\n    \"/raw/refs/heads/main/DATA/hald_ingredients.csv\")\n# Transform the content of the file into a numpy.ndarray\nA = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nresponse = requests.get(\n    \"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos\"\n    \"/raw/refs/heads/main/DATA/hald_heat.csv\")\nb = np.genfromtxt(io.BytesIO(response.content), delimiter=\",\")\n\nU, s, VT = LA.svd(A, full_matrices=False)\n\nx = VT.T @ np.diag(1/s) @ U.T @ b\n\nplt.plot(b, color=\"k\", label=\"Target - Heat data\")\nplt.plot(A@x, '--', color=\"b\", label=\"Regression\")\nplt.legend()\nplt.xlabel(\"mixture\")\nplt.ylabel(\"Heat[cal/g]\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.6: Estimate for hardening in cement mixtures.\n\n\n\n\n\nThis concludes our investigation of matrix decompositions, we will investigate further decompositions of signals later, but for now we dive deeper into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Matrix decompositions",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regression analysis",
    "section": "",
    "text": "In general, regression analysis can be understood as a set of tools that is used to estimate or establish a relationship between a dependent variable \\(Y\\) (also called outcome or response variable, label) and the independent variable \\(X\\) (also called regressor, predictors, covariates, explanatory variable or feature). If we add a regression function \\(f\\) and some unknown parameters \\(c\\) to the mix the problem can be written mathematically as \\[\nY = f(X, c)\n\\tag{1}\\] where \\(c\\) is found by optimizing for a good fit of \\(f\\) to the data.\nWe split up the discussion along the well known topics:\n\n5  Linear Regression\n6  Non-linear Regression\n\n6.1 Gradient Descent\n\n7  Optimizers\n\n7.2 Model Selection/Identification and over-/underfitting\n\n\nParts of this section are based on (Brunton and Kutz 2022, sec. 4).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis"
    ]
  },
  {
    "objectID": "regression/linear.html",
    "href": "regression/linear.html",
    "title": "5  Linear Regression",
    "section": "",
    "text": "5.1 Ordinary Least Square\nThe general idea of linear regression is to approximate a point cloud by a mathematical function, this is often called curve fitting and it is closely related to optimization techniques, (compare Brunton and Kutz 2022, sec. 4.1).\nLet us assume that we want to establish a relationship between our \\(n\\) observations with \\(m\\) independent variables. In order to get the notation down correctly we should describe the variables more closely.\nWe have \\(n\\) observations (the \\(k\\)-th representation is denoted by \\(y_k\\)) resulting in a vector: \\[\ny = \\left[\n    \\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n \\end{array}\n    \\right].\n\\]\nWe have \\(m\\) independent variables and consequently for each of this \\(n\\) observations this results in a matrix \\[X = \\left[\n    \\begin{array}{c} X_{1-} \\\\ X_{2-} \\\\ X_{3-} \\\\ \\vdots \\\\ X_{n-} \\end{array}\n    \\right] = \\left[X_{-1}, \\dots, X_{-m}\\right],\\] where the second representation more closely fits to our idea of the \\(m\\) independent variables.\nLet us model the relation in a linear fashion with the parameters \\(c\\) as \\[\ny_k \\overset{!}{=} X_{k1} c_1 + X_{k2} c_2 + \\cdots + X_{km} c_m, \\quad 1\\leq k \\leq n,\n\\] or in short \\[\ny = X c = f(X, c).\n\\]\nLet us assume for a moment we already have a realisation of \\(f\\), i.e. we know the parameters \\(c\\), we get the error of the approximation as \\[\n\\mathbf{e} = y - f(X, c)\\quad \\Leftrightarrow \\quad \\mathbf{e}_k = y_k - f(X_{k-}, c).\n\\tag{5.1}\\]\nThere are various possibilities to select the metric for minimizing \\(\\mathbf{e}\\) and therefore characterizing the quality of the fit. This is done by selecting the underlying norm. Most commonly we use the \\(1\\)-norm, the \\(2\\)-norm and the \\(\\infty\\)-norm, i.e. \\[\nE_1 (f) = \\frac1n \\sum_{k=1}^n|f(X_{k-}, c) - y_k |,\n\\tag{5.2}\\] for the \\(1\\)-norm or mean absolute error, \\[\nE_2 (f) = \\sqrt{\\frac1n \\sum_{k=1}^n|f(X_{k-}, c) - y_k |^2},\n\\tag{5.3}\\] for the \\(2\\)-norm or least-square error, \\[\nE_\\infty (f) = \\max_{k}|f(X_{k-}, c) - y_k |\n\\tag{5.4}\\] for the \\(\\infty\\)-norm or maximum error. Of course \\(p\\)-norms work as well \\[\nE_p (f) = \\left(\\frac1n \\sum_{k=1}^n|f(X_{k-}, c) - y_k |^p\\right)^{1/p}.\n\\] If we go back and we want to solve Equation 5.1 for \\(c\\) we have different realisations, depending on the selected norm.\nTo illustrate this we use the example from (Brunton and Kutz 2022, 136–67).\nIf we use Equation 5.3 the regression fit is called the least square fit.\nIt is worth looking into the least square solution \\[\nE_2 (f) = \\sqrt{\\frac1n \\sum_{k=1}^n|\\underbrace{f(X_{k-}, c) - y_k}_{\\mathbf{e}_k} |^2},\n\\] more closely. We can interpret it as the optimization problem \\[\nc = \\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2\n\\] and with some linear algebra we get \\[\n\\begin{array}{ccl}\nc &= &\\underset{v}{\\operatorname{argmin}} \\| y - Xv\\|_2,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} \\langle y -Xv, y -Xv\\rangle,\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} (y -Xv)^{\\mathsf{T}} (y -Xv),\\\\\n  &= &\\underset{v}{\\operatorname{argmin}} y^{\\mathsf{T}}y - y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v.\\\\\n\\end{array}\n\\] In order to find a solution we compute the derivative with respect to \\(v\\) set it to \\(0\\) and simplify, i.e. \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}\\,v} y^{\\mathsf{T}}y- y^{\\mathsf{T}} X v - v^{\\mathsf{T}} X^{\\mathsf{T}} y + v^{\\mathsf{T}} X^{\\mathsf{T}} X v = - 2X^{\\mathsf{T}}y + 2 X^{\\mathsf{T}}Xv\n\\tag{5.5}\\] and \\[\nv = (X^{\\mathsf{T}}X)^{-1}X^{\\mathsf{T}}y \\equiv X^\\dagger y.\n\\] We recall, that \\(X^\\dagger\\) is called the Moore-Penrose pseudo-inverse, see Definition 4.3.\nSee Figure 5.1 for the result when using the pseudo-inverse.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#sec-regr-linear-ols",
    "href": "regression/linear.html#sec-regr-linear-ols",
    "title": "5  Linear Regression",
    "section": "",
    "text": "5.1.1 Alternative computation\nThe pseudo-inverse provides us with the optimal solution but for large systems the computation can be inefficient, or more precisely, there are more efficient ways to get the same results.\nFollowing (Brunton and Kutz 2022, 137–38) we can find an alternative for the above example in Figure 5.1.\nWe want to fit the data points \\((x_i, y_i)\\) with the function \\(f(x) = c_2 x + c_1\\) resulting in the error \\[\nE_2(f) = \\sqrt{\\frac1n \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2}.\n\\] A solution that minimizes the above equation also minimizes \\[\nE_2 = \\sum_{k=1}^n(c_2 x_k + c_1 - y_k )^2\n\\] and we find the solution by partial differentiation \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_1} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) = 0,\n\\] \\[\n\\frac{\\mathrm{d} E_2}{\\mathrm{d}\\, c_2} = 0 \\Leftrightarrow \\sum_{k=1}^n 2 (c_2 x_k + c_1 - y_k ) x_k = 0,\n\\] and this results in the system \\[\n\\left[\n\\begin{array}{cc}\nn & \\sum_k x_k \\\\\n\\sum_k x_k & \\sum_k x_k^2\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\nc_1 \\\\ c_2\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\n\\sum_k y_k\\\\\\sum_k x_k y_k\n\\end{array}\n\\right].\n\\tag{5.6}\\] This ansatz can be extended to polynomials of degree \\(k\\), where the result is always a \\((k+1) \\times (k+1)\\) matrix.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#polynomial-regression",
    "href": "regression/linear.html#polynomial-regression",
    "title": "5  Linear Regression",
    "section": "5.2 Polynomial Regression",
    "text": "5.2 Polynomial Regression\nPolynomial regression, despite its name, is linear regression with a special function \\(f\\) where the relation is polynomial in \\(x= \\left[a_1, \\dots, x_m\\right]^{\\mathsf{T}}\\) \\[\ny_k = x_k^0 + x_k^1 c_1 + x_k^2 c_2 + \\cdots + x_k^m c_m, \\quad 1\\leq k \\leq n,\n\\] With the matrix form \\[\nX = \\left[\n\\begin{array}{ccccc}\n1 & x_1    &x_1^2  & \\cdots &x_1^m  \\\\\n1 & x_2    &x_2^2  & \\cdots &x_2^m  \\\\\n1 & \\vdots &\\vdots & \\ddots &\\vdots \\\\\n1 & x_n    &x_n^2  & \\cdots &x_n^m\n\\end{array}\n\\right]\n\\tag{5.7}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe matrix Equation 5.7 is called the Vandermonde matrix.\n\n\nThis can be solved in the same ways as the before with \\(X^\\dagger\\) or the direct system, but it should not as Equation 5.7 is badly conditioned. There are other methods like divided differences, Lagrange interpolation for this task.\n\n\n\n\n\n\n\nExample 5.1 (Parameter estimation of a falling object) Just because we deal with linear regression this does not means that the model needs to be linear too. As long as we are linear in the parameters \\(c\\) we can apply our findings, even for non linear independent variables.\nTo illustrate this, let us consider an object falling without aerodynamic drag, described by the differential equation \\[\nm \\ddot{y}(t) = -m g,\n\\] for the gravitational constant \\(g\\). Integration with respect to \\(t\\) results in \\[\ny(t) = y(0) + v(0) t - \\frac{g}{2} t^2.\n\\] So we get \\[\nX{k-} =\n\\left[\n    \\begin{array}{ccc}\n    1 & t_k & -\\frac{1}{2} t_k^2 \\\\\n    \\end{array}\n\\right],\n\\quad \\text{and} \\quad\ny = \\left[\n\\begin{array}{c}\ny^{(0)}\\\\ v^{(0)} \\\\ g\n\\end{array}\n\\right]\n\\] or in the long form, for \\(t_{k+1}-t_k = 0.1\\) \\[\nX = \\left[\n\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n1 & 0.1 & -0.005 \\\\\n1 & 0.2 & -0.020 \\\\\n1 & 0.3 & -0.045 \\\\\n1 & 0.4 & -0.080 \\\\\n\\vdots & \\vdots & \\vdots\n\\end{array}\n\\right]\n\\] and we can, for example, estimate our unknowns \\(y^{(0)}\\), \\(v^{(0)}\\), and \\(g\\) by \\[\nc = X^\\dagger y.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 (Polynomial regression) In the following example we generate an artificial sample of \\(n=100\\) points resulting in the samples \\[\ny_k = \\frac12 x_k^2 + x_k + 2 + \\epsilon_k\n\\] where \\(\\epsilon_k\\) is a random number that simulates the error. We perform the interpolation with \\(X^\\dagger\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore', np.exceptions.RankWarning)\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 100\nx = 6 * np.random.rand(m) - 3\ny = 1/2 * x ** 2 + x + 2 + np.random.randn(m)\n\nX1 = np.vander(x, 2)\nX2 = np.vander(x, 3)\nX3 = np.vander(x, 16)\n\np1 = np.linalg.pinv(X1) @ y\np2 = np.linalg.pinv(X2) @ y\np3 = np.linalg.pinv(X3) @ y\np4 = np.polyfit(x, y, 300)\n\nxf = np.arange(-3, 3, 0.1)\ny1 = np.polyval(p1, xf)\ny2 = np.polyval(p2, xf)\ny3 = np.polyval(p3, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\", linewidth=3)\nplt.plot(xf, y1, label=r\"$m=1$\")\nplt.plot(xf, y2, label=r\"$m=2$\")\nplt.plot(xf, y3, label=r\"$m=16$\")\nplt.plot(xf, y4, label=r\"$m=300$\")\n\nplt.ylim(0, 10)\nplt.xlim(-3, 3)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.legend(loc=\"upper left\")\n#plt.gca().set_aspect(0.25)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe condition of the Vandermonde matrix increases rapidly:\n\n\n\ndegree\n\\(m=2\\)\n\\(m=3\\)\n\\(m=5\\)\n\\(m=10\\)\n\\(m=15\\)\n\\(m=20\\)\n\n\n\n\n\\(\\kappa_2\\)\n6.07e0\n1.63e1\n1.89e2\n154e5\n1.24e8\n1.41e11\n\n\n\nThe result of the \\(m=300\\) is unstable and we can not compute it via \\(X^\\dagger\\).\n\n\nAs can be seen in Figure 5.2 we do not necessarily get a good result if we use a higher degree polynomial. This is especially true if we extrapolate and not interpolate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/linear.html#data-linearization",
    "href": "regression/linear.html#data-linearization",
    "title": "5  Linear Regression",
    "section": "5.3 Data Linearization",
    "text": "5.3 Data Linearization\nQuite often it is possible to linearize our model at hand. For example if we want to fit for \\[\nf(x, c) = c_2 \\exp(c_1 x),\n\\tag{5.8}\\]\nand use the same derivation as for Equation 5.6 we end up with the corresponding system as \\[\nc_2 \\sum_k x_k \\exp(2 c_1 x_k) - \\sum_k x_k y_k \\exp(c_1 x_k) =0,\n\\] \\[\nc_2 \\sum_k \\exp(2 c_1 x_k) - \\sum_k y_k \\exp(c_1 x_k) =0.\n\\]\nThis non-linear system can not be solved in a straight forward fashion but we can avoid it by linearization with the simple transformation \\[\n\\begin{array}{ccl}\n\\hat{y} &=& \\ln(y), \\\\\n\\hat{x} &=& x, \\\\\nc_3 &=& \\ln c_2,\n\\end{array}\n\\] and taking the natural logarithm of both sides of Equation 5.8 and simplifying \\[\n\\ln y = \\ln(c_2 \\exp(c_1 x)) = \\ln(c_2) + c_1 x.\n\\] Now all that is left to apply \\(\\ln\\) to the data \\(y\\) and solve the linear problem. In order to apply it to the original function the parameters transform needs to be reversed.\n\n\n\n\n\n\n\nExample 5.3 (World population) We take a look at the population growth were the data is kindly provided by Ritchie et al. (2023). Have a look at there excellent work on ourworldindata.org.\n\nShow the code for the figure\nfrom owid.catalog import charts\nimport numpy as np\nimport scipy.optimize\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndf = charts.get_data(\n    \"https://ourworldindata.org/grapher/population?country=~OWID_WRL\")\ndata = df[df[\"entities\"] == \"World\"]\nx = data[\"years\"].to_numpy()\ny = data[\"population\"].to_numpy()\nylog = np.log(y)\n\ndef fit3(x0, t):\n    x, y = t\n    return np.sum(np.power(np.abs(x0[0] * x + x0[1] - y), 2))\n\nstart = [-np.inf, 0, 1700, 1900, 1980]\n\nyest = []\n\nfor s in start:\n    filter = x &gt;= s\n    t = (x[filter], ylog[filter])\n    x0 = np.array([1, 1])\n    b = scipy.optimize.fmin(fit3, x0, args=(t,), disp=False)\n    yest.append(np.exp(b[1]) * np.exp(b[0] * x))\n\nfig = go.Figure()\nfig2 = go.Figure()\nfig.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"],\n                y=data[\"population\"], name=\"data\"))\nfig2.add_trace(go.Scatter(mode=\"markers\", x=data[\"years\"],\n                y=data[\"population\"], name=\"data\"))\n\nfor i, ye in enumerate(yest):\n    fig.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n    fig2.add_trace(go.Scatter(x=x, y=ye, name=f\"fit from {start[i]}\"))\n\n\nfig.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig.update_yaxes(title_text=\"population\")\nfig.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig.show()\n\nfig2.update_xaxes(title_text=\"year\", range=[1700, 2023])\nfig2.update_yaxes(title_text=\"population\", type=\"log\", range=[8.5,10])\nfig2.update_layout(legend=dict(\n    yanchor=\"top\",\n    y=0.99,\n    xanchor=\"left\",\n    x=0.01\n))\nfig2.show()\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(a) World population with regression lines normal scale.\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n(b) World population with regression lines logarithmic scale.\n\n\n\n\n\n\n\nFigure 5.3: Regression analysis for world population with different scales for y axis.\n\n\n\n\n\n\n\nNext, we are going to look into actual non-linear regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber, Esteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population Growth.” Our World in Data.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html",
    "href": "regression/nonlinear.html",
    "title": "6  Non-linear Regression",
    "section": "",
    "text": "6.1 Gradient Descent\nWe extend our theory of curve fitting to a non-linear function \\(f(X, c)\\) with coefficients \\(c\\in\\mathbb{R}^m\\) and our \\(n\\) \\(X_{k-}\\). We assume that \\(m &lt; n\\).\nIf we define our root-mean-square error depending on \\(c\\) as \\[\nE_2(c) = \\sum_{k=1}^n(f(X_{k-}, c) - y_k )^2\n\\] and we can minimize with respect to each \\(c_j\\) resulting in a \\(m \\times m\\) system \\[\n\\sum_k (f(X_{k-}, c) - y_k)\\frac{\\partial f}{\\partial c_j} = 0\\quad\\text{for}\\quad j=1, 2, \\ldots, m.\n\\]\nDepending on the properties of the function at hand it can be guaranteed to find an extrema or not. For example convex functions have guarantees of convergence while non-convex functions can have chelating features that make it hard to work with optimization algorithms.\nTo solve such a system we employ iterative solvers that use an initial guess. Let us look at the most common, the gradient descent method.\nFor a higher dimensional system or function \\(f\\) the gradient must be zero\n\\[\n\\nabla f(x) = 0\n\\] to know that we are in an extrema. Since we can have saddle points this is not the sole criteria but a necessary one. Gradient descent, as the name suggest uses the gradient as direction in an iterative algorithm to find a minimum.\nThe idea is basically, if you are lost on a mountain in the fog and you can not see the path, the fastest and a reliable way that only uses local information is to follow the steepest slope down.\nWe express this algorithm in terms of the iterations \\(x^k\\) for guesses of the minimum with the updates \\[\nx^{(k+1)} = x^{(k)} - \\delta\\, \\nabla f(x^{(k)})\n\\] where the parameter \\(\\delta\\) defines how far along the gradient descent curve we move. This formula is an update for a Newton method where we use the derivative as the update function. This leaves us with the problem to find an algorithm to determine \\(\\delta\\).\nAgain, we can view this as an optimization problem for a new function \\[\nF(\\delta) = f(x^{(k+1)}(\\delta))\n\\] and \\[\n\\partial_\\delta F = -\\nabla f(x^{(k+1)})\\nabla f(x^{(k)}) = 0.\n\\tag{6.1}\\]\nNow the interpretation of Equation 6.1 is that we want that the gradient of the current step is orthogonal to the gradient of the next step.\nIn order to make it clearer we follow the example given in (Brunton and Kutz 2022, sec. 4.2,pp. 141-144).\nIn order to get a better idea on how this is working for curve fitting we apply the gradient descent method to our curve fitting from Section 5.1.\nIn Equation 5.5 we computed the gradient and instead of computing \\(X^\\dagger\\) with high cost we get the low cost iterative solver:\n\\[\nc^{(k+1)} = c^{(k)} - \\delta (2 X^{\\mathsf{T}} X c^{(k)} - 2 X^{\\mathsf{T}} y)\n\\]\nAs \\(\\delta\\) is tricky to compute we go ahead and introduce we do not update it but prescribe it. This will not grant us the optimal convergence (if there is convergence) but if we choose it right we still get convergence.\nSo lets try it with our example from Figure 5.1.\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef gd(c, delta, X, y, n, stop=1e-10):\n    diff = 1\n    for _ in range(1, n):\n        cnew = update(c, delta, X, y)\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, \n              1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = gd(c, delta, X, y, 50)\nc_20 = gd(c_10, delta, X, y, 50)\nc_30 = gd(c_20, delta, X, y, 200)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\ny4 = np.polyval(p4, xf)\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"$n=50$\")\nplt.plot(xf, y2, label=r\"$n=100$\")\nplt.plot(xf, y3, label=r\"$n=300$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6.2: Line fit with gradient descent for different number of iterations and learning rate 2e-3.\nThe above algorithm uses the entire set \\(X\\) for the computation. For a large enough set \\(X\\) this is quite cost intense, even if it is still cheaper than computing \\(X^\\dagger\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "href": "regression/nonlinear.html#sec-regression-nonlinear-gd",
    "title": "6  Non-linear Regression",
    "section": "",
    "text": "Warning\n\n\n\nA function does not necessarily experience gravity in the same way as we do, so please do not try this in real live, i.e. cliffs tend to be hard to walk down.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6.1 (Gradient descent) For the function \\[\nf(x) = x_1^2 + 3 x_2^2\n\\] we can compute the gradient as \\[\n\\nabla f (x)= \\left[ \\begin{array}{c} \\partial_{x_1} f(x)\\\\ \\partial_{x_2} f(x) \\end{array} \\right] = \\left[ \\begin{array}{c} 2 x_1 \\\\ 6 x_2 \\end{array} \\right]\n\\] Resulting in \\[\nx^{k+1} = x^{(k)} - \\delta \\, \\nabla f(x^{(k)}) =\n\\left[ \\begin{array}{c} (1 - 2 \\delta) x^{(k)}_1 \\\\ (1 - 6 \\delta)x^{(k)}_2 \\end{array} \\right].\n\\] Consequently \\[\nF(\\delta) = (1-2\\delta)^2 x_1^2 + (1-6\\delta)^2 x_2^2,\n\\] \\[\n\\partial_\\delta F = -2^2(1-2\\delta)x_1^2 - 6^2(1-6\\delta)x_2^2,\n\\] and \\[\n\\partial_\\delta F(\\delta) = 0 \\Leftrightarrow \\delta = \\frac{x_1^2 + 9 x_2^2}{2 x_1^2 + 54 x_2^2}.\n\\]\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\nimport numpy as np\n\nx_ = np.linspace(-3, 3, 20)\ny_ = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_, y_)\n\nf = lambda x, y: np.pow(x, 2) + 3 * np.pow(y, 2)\ngrad_f = lambda x: x * np.array([2, 6]).reshape(x.shape)\ndelta = lambda x: (x[0]**2 + 9 * x[1]**2)/(2 * x[0]**2 + 54 * x[1]**2)\n\nZ = f(X, Y)\n\nfig = go.Figure()\nfig.add_trace(go.Surface(z=Z, x=X, y=Y,\n                        colorscale='greys', name=\"Function\"))\nfig.update_traces(contours_z=dict(show=True, usecolormap=True,\n                                  highlightcolor=\"limegreen\",\n                                  project_z=True))\nfig.update_scenes(xaxis_title_text=r\"x_1\",  \n                  yaxis_title_text=r\"x_2\",  \n                  zaxis_title_text=r\"f(x)\")\nx = np.array([3, 2]).reshape((1, 2))\nz = np.array(f(x[0, 0], x[0, 1]))\ndiff = 1\n\nwhile diff &gt; 1e-10:\n    x_new = x[-1, :] - delta(x[-1, :]) * grad_f(x[-1, :])\n    z = np.hstack((z, f(x_new[0], x_new[1])))\n    diff = np.linalg.norm(z[-1] - z[-2])\n    x = np.vstack((x, x_new))\n\nfig.add_scatter3d(x=x[:, 0], y=x[:, 1], z=z,\n                  line_color='red', name=\"Descent path\")\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 6.1: Gradient descent applied for the function \\(f(x) = x_1^2 + 3x_2^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can not compute the gradient analytically there are numerical methods to help do the computation.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn machine learning the parameter \\(\\delta\\) is often called the learning rate.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#stochastic-gradient-descent",
    "href": "regression/nonlinear.html#stochastic-gradient-descent",
    "title": "6  Non-linear Regression",
    "section": "6.2 Stochastic Gradient Descent",
    "text": "6.2 Stochastic Gradient Descent\nIn order to reduce cost we can randomly select some points of our training set and only train with those. Obviously the computation of the gradient becomes much faster. We call this method Stochastic Gradient descent (SGD).\nIn Figure 6.3 we see the convergence for randomly selecting 1, 3, and 6 indices of our possible 10.\nThe downside of the SGD algorithm is that the algorithm does not settle down for a long time and will jump. In the other side it might get less stuck in local minima.\nOne possibility to try to get the strength of both is to use SDG to get a good guess for your initial value and SD for the fine tuning.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\ngrad = lambda c, X, y: 2 * X.T @ (X @ c - y)\nupdate = lambda c, delta, X, y: c - delta * grad(c, X, y)\n\ndef sgd(c, delta, X, y, n, indices=-1, stop=1e-10):\n    if indices == -1:\n        indices = X.shape[0]\n    diff = 1\n    for _ in range(1, n):\n        I = np.random.choice(X.shape[0], size=indices, replace=False)\n        I.sort()\n        cnew = update(c, delta, X[I, :], y[I])\n        diff = np.linalg.norm(cnew - c)\n        c = cnew\n        if diff &lt; stop: break\n    return c\n\n# The data\nx = np.arange(1, 11)\ny = np.array([0.2, 0.5, 0.3, 0.5, 1.0, \n              1.5, 1.8, 2.0, 2.3, 2.2]).reshape((-1, 1))\n\nX = np.array([x, np.ones(x.shape)]).T\ndelta = 0.002\nc = np.random.random((2, 1))\n\nc_10 = sgd(c, delta, X, y, 200, 1)\nc_20 = sgd(c, delta, X, y, 200, 3)\nc_30 = sgd(c, delta, X, y, 200, 5)\nc_ft = gd(c_20, delta, X, y, 150, -1)\np4 = np.linalg.pinv(X) @ y\n\nxf = np.arange(0, 11, 0.1)\ny1 = np.polyval(c_10, xf)\ny2 = np.polyval(c_20, xf)\ny3 = np.polyval(c_30, xf)\nyft = np.polyval(c_ft, xf)\ny4 = np.polyval(p4, xf)\n\n\nfig = plt.figure()\nplt.plot(x, y, \"o\", color=\"r\", label=\"observations\")\nplt.plot(xf, y1, label=r\"#I=$1$\")\nplt.plot(xf, y2, label=r\"#I=$3$\")\n#plt.plot(xf, y3, label=r\"#I=$5$\")\nplt.plot(xf, yft, label=r\"#I=$3$ GD $n=100$\")\nplt.plot(xf, y4, label=r\"$E_2$\")\n\nplt.ylim(0, 4)\nplt.xlim(0, 11)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.gca().set_aspect(1)\nplt.grid(visible=True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6.3: Line fit with stochastic gradient descent with 1 or 3 samples and 200 iterations as well as the 3 sample version as initial guess for GD with 100 iterations.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/nonlinear.html#categorical-variables",
    "href": "regression/nonlinear.html#categorical-variables",
    "title": "6  Non-linear Regression",
    "section": "6.3 Categorical Variables",
    "text": "6.3 Categorical Variables\nEven with our excursion to non-linear regression we still had somewhat regular data to work with. This is not always the case. Sometimes there are trends in the data, like per month, or day. The inclusion of categorical variables can help to control for trends in the data.\nWe can integrate such variables to the regressor by adding columns to the matrix \\(X\\) for each of the categories. Note, they can be interpreted as to correspond to the offset (the constant \\(1\\)) so this column can be omitted and each category gets a separate offset.\nWe can see this in action in the following example. We investigate the unemployment data in Austria. There is a strong seasonality Figure 6.4 (b) in the data. This is largely due to the fact that the Austrian job market has a large touristic sector with its season and the construction industry employs less people during summer.\nFor the regression Figure 6.4 (b) we can see that this captures the seasonal change quite well.\nThe data is taken from Arbeitsmarktdaten online.\n\n\n\n\n\n\n\n\n\n\n\n(a) Regression with categorical variables per month.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Seasonality of the unemployment average over the years.\n\n\n\n\n\n\n\nFigure 6.4: Unemployment data from Austria for the years 2010 to 2017.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is also quite a difference between man and woman that could be categorized separately.\n\n\nWe wrap up this section about regression by talking more abstract about the regression of linear systems and some general thoughts about the selection of the model and consequences.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html",
    "href": "regression/optimizers.html",
    "title": "7  Optimizers",
    "section": "",
    "text": "7.1 Over-Determined Systems\nAs we have seen in the previous section the task of regression usually results in an optimization problem. It is worth investigating this further by looking closely on the \\[\nA x = b\n\\tag{7.1}\\] problem for different dimensions of \\(A\\).\nWe investigate the impact of restricting our solution not just by Equation 7.1 but with the help of the \\(\\ell_1\\) and \\(\\ell_2\\) norm imposed on the solution \\(x\\). As we have seen in Figure 5.1 the choice of norm has an implication on the result and the same is true here.\nWe speak of an over-determined system if we have more rows than columns, i.e. \\(A\\) is tall and skinny and in general there is no solution to Equation 7.1 but rather we minimize the error according to a norm, see Section 5.1. If we further impose a restriction on \\(x\\) we can select a more specific solution.\nThe generalized form is \\[\nx = \\underset{v}{\\operatorname{argmin}} \\|Av - b\\|_2 + \\lambda_1 \\|v\\|_1 + \\lambda_2\\|v\\|_2\n\\tag{7.2}\\] where the parameters \\(\\lambda_1\\) and \\(\\lambda_2\\) are called the penalization coefficients, with respect to the norm. Selecting these coefficients is the first step towards model selection.\nLet us have a look at this in action for solving a random system with different parameters \\(\\lambda_1\\) and setting \\(\\lambda_2\\).",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#over-determined-systems",
    "href": "regression/optimizers.html#over-determined-systems",
    "title": "7  Optimizers",
    "section": "",
    "text": "7.1.1 LASSO\nThe least absolute shrinkage and selection operator LASSO solves Equation 7.2 with \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2=0\\), i.e. only optimizing with the \\(\\ell_1\\) norm. The theory tells us that for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0 = np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 1]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda_1={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 50, rf\"$\\lambda_1={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 70)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.1: LASSO regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.\n\n\n\n\n\nThe last row of Figure 7.1 confirms this quite impressively, interesting enough the solution also becomes positive.\n\n\n7.1.2 RIDGE\nThe Ridge Regression solves Equation 7.2 with \\(\\lambda_1 = 0\\) and \\(\\lambda_2 &gt; 0\\), i.e. only optimizing with the \\(\\ell_2\\) norm. The theory tells us that for for increasing \\(\\lambda_1\\) we should get more and more zeros in our solution \\(x\\).\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = np.random.rand(m, n)\nb = np.random.rand(m)\nx0 = np.linalg.pinv(A) @ b\n\noptimize = lambda x, A, b, lam, norm: np.linalg.norm(A @ x - b, ord=norm[0]) +\\\n    lam * np.linalg.norm(x, ord=norm[1])\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=(A, b, lam, [2, 2]), x0=x0)\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda_2={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 15, rf\"$\\lambda_2={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 20)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.2: Ridge regression coefficients of an over-determined system with 500 constraints and 100 unknowns. Top three show the values for the solution and the bottom three a histogram of this solution. The label with lambda maps the two.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.1 (Implement the above optimization yourself.) Fill out the missing parts:\n\n\nCode fragment for implementation.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\nnp.random.seed(6020)\n\nm = 500\nn = 100\nA = # Random matrix with m rows and 100 columns \nb = np.random.rand(m)\nx0 = # Optimal 2 norm solution without penalization as initial start\n\noptimize = # function to optimize\n\nfig = plt.figure()\naxs = []\naxs.append(fig.add_subplot(4, 1, 1))\naxs.append(fig.add_subplot(4, 3, 10))\naxs.append(fig.add_subplot(4, 1, 2))\naxs.append(fig.add_subplot(4, 3, 11))\naxs.append(fig.add_subplot(4, 1, 3))\naxs.append(fig.add_subplot(4, 3, 12))\n\nfor i, lam in enumerate([0, 0.1, 0.5]):\n    res = minimize(optimize, args=, x0=x0)    # use your correct arguments here\n    axs[i * 2].bar(range(n), res.x)\n    axs[i * 2].text(5, 0.05, rf\"$\\lambda={lam}$\")\n    axs[i * 2].set_xlim(0, 100)\n    axs[i * 2].set_ylim(-0.1, 0.1)\n    axs[i * 2 + 1].hist(res.x, 20)\n    axs[i * 2 + 1].text(-0.08, 15, rf\"$\\lambda={lam}$\")\n    axs[i * 2 + 1].set_xlim(-0.1, 0.1)\n    axs[i * 2 + 1].set_ylim(0, 20)\naxs[0].set_xticks([])\naxs[2].set_xticks([])\naxs[3].set_yticks([])\naxs[5].set_yticks([])\n\nplt.subplots_adjust(top = 0.99, bottom=0.1, hspace=0.35, wspace=0.2)\nplt.show()",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "regression/optimizers.html#sec-regression-optimizers-msou",
    "href": "regression/optimizers.html#sec-regression-optimizers-msou",
    "title": "7  Optimizers",
    "section": "7.2 Model Selection/Identification and over-/underfitting",
    "text": "7.2 Model Selection/Identification and over-/underfitting\nLet us use the results we have obtain so far for a discussion on model selection.\nSo far, we have mostly explicitly proposed a model that we think will fit our data and we have seen that even it this case we can still choose multiple parameters to fin tune our selection.\nNow consider the other possibility, we have data where the model is unknown. For example, in Example 5.1 we stopped with degree 2 for our polynomial because we know about Newton’s principles, if we don’t know it, we might extend the model for a higher degree.\nOne of the leading assumptions to use in such a case is:\n\nAmong competing hypotheses, the one with the fewest assumptions should be selected, or when you have two competing theories that make exactly the same predictions, the simpler one is the more likely. - Occam’s razor\n\nThis plays an intimate role in over- and underfitting of models. To illustrate this we recall Example 5.2 with Figure 5.2 as seen below once more.\n\n\n\n\n\n\n\n\nFigure 7.3: Fitting for different degrees of polynomial \\(m\\)\n\n\n\n\n\nFor \\(m=1\\), a straight line, we have an underfitted model. We can not adequately capture the underlying model, at least not in the entire region.\nIf we move to \\(m=16\\) and the extreme \\(m=300\\) we see an overfitted system. The \\(m=16\\) curve follows clusters of points too close, e.g. in the region around \\(x=-2\\), this is more pronounced for \\(m=300\\) where we quite often closely follow our observations but between them we clearly overshoot.\nIn this way we can also say that an overfitted system follows the training set to closely and will not generalize good for another testing/evaluation set.\nAs a consequence model selection should always be followed by a cross-validation. Meaning we need to check if our model is any good.\nA classic method is the k-fold cross validation:\n\nTake random portions of your data and build a model. Do this \\(k\\) times and average the parameter scores (regression loadings) to produce a cross-validated model. Test the model predictions against withheld (extrapolation) data and evaluate whether the model is actually any good. - (see Brunton and Kutz 2022, 159)\n\nAs we can see, there are a lot of further paths to investigate but for now this concludes our excursion into regression.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Regression analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Optimizers</span>"
    ]
  },
  {
    "objectID": "signal/index.html",
    "href": "signal/index.html",
    "title": "Signal Processing",
    "section": "",
    "text": "In previous parts we have already discussed how changing a coordinate system or basis (Definition 1.8) can simplify expression or computation, see 3  Eigendecomposition and 4  Singular Value Decomposition, among others.\nOne of the most fundamental coordinate transformations was introduced by J.-B. Joseph Fourier in the early 1800s. While investigating heat he discovered that sine and cosine functions with increasing frequency form an orthogonal basis (Definition 1.6 & Definition 1.9). In fact, the sine and cosine functions for an eigenbasis for the heat equation \\[\n\\frac{\\partial u}{\\partial t} = \\Delta u\n\\] and solving it becomes trivial once you determine the corresponding eigenvalues that are connected to the geometry, amplitudes, and boundary conditions.\nIn the 200+ years since, this discovery has not only founded new corners of mathematics but also allows via the fast fourier transform or FFT the real-time image and audio compression that make our global communication networks work.\nIn the same area the related wavelets where developed to for advanced signal processing and compression.\nIn this section we are going to discuss basics of signal processing in terms of these and other signal transformations, see\n\n8  Fourier Transform\n10  Wavelet transform\n11  Two-Dimensional Transform\n\nParts of this section are based on (Brunton and Kutz 2022, chap. 2).\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Signal Processing"
    ]
  },
  {
    "objectID": "signal/fourier.html",
    "href": "signal/fourier.html",
    "title": "8  Fourier Transform",
    "section": "",
    "text": "8.1 Fourier Series\nThe fourier transform helps us convert a signal from the time domain to the frequency domain. In this section our main concern is going to be one dimensional signals, but the concepts can be applied to multiple dimensions.\nBefore we can start defining the Fourier Series we need to extend our notion of vector space to functions space. This is done with Hilbert spaces. The computational rules follow the same principal as in Definition 1.1, what we want to investigate is the inner product.\nAt first this looks strange but it is closely related to our already known Definition 1.3.\nAs a first step, if we move from real to complex vector spaces the transpose is replaced by the conjugate transposed or hermit transpose, in notation the \\(^\\mathbf{T}\\) becomes \\(^\\mathbf{H}\\).\nNow consider a discrete version of \\(f\\) and \\(g\\) at regular intervals \\(\\Delta x = \\frac{b-a}{n-1}\\) where \\[\nf_i = f(x_i) = f(a + (i - 1) \\Delta x),\\quad i = 1, \\ldots n,\n\\] same for \\(g_i\\) and accordingly \\(x_1 = a + 0 \\Delta x = a\\) and \\(x_n = a + (n - 1)\\Delta x = b\\).\nThe inner product is than \\[\n\\langle f, g \\rangle = \\langle\\left[\\begin{array}{c}f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_n \\end{array}\\right], \\left[\\begin{array}{c}g_1 \\\\ g_2 \\\\ \\vdots \\\\ g_n \\end{array}\\right] \\rangle = \\sum_{i=1}^n f_i \\overline{g}_i = \\sum_{i=1}^n f(x_i)\\overline{g}(x_i).\n\\]\nAs this sum will increase by increasing \\(n\\) we should normalize it by the factor \\(\\Delta x\\).\n\\[\n\\frac{b-a}{n-1} \\langle g, f \\rangle = \\sum_{i=1}^n f(x_i)\\overline{g}(x_i) \\Delta x.\n\\] If we now increase \\(n\\to \\infty\\) we get \\(\\Delta x \\to 0\\) and the sum transforms into the integral.\nSimilar as we saw projection in vector spaces related to the inner product this is true here as well.\nThe Fourier series is nothing else as the projection of a function with an integer period on the domain \\([a, b]\\) onto the orthogonal basis defined by the sine and cosine functions.\nIn Fourier analysis the first result is stated for a periodic and piecewise smooth function \\(f(t)\\).\nWith the help of Euler’s formula: \\[\n\\mathrm{e}^{\\mathrm{i} k t} = \\cos(k t) + \\mathrm{i} \\sin(k t)\n\\tag{8.2}\\] we can rewrite Equation 8.1 as \\[\nf(t) = \\sum_{k=-\\infty}^\\infty c_k \\mathrm{e}^{\\omega\\mathrm{i} k t}\n\\] with \\[\nc_k = \\frac{1}{L} \\int_0^L f(t) \\mathrm{e}^{-\\omega\\mathrm{i} k t}\\, \\mathrm{d}t.\n\\] and for \\(n=1, 2, 3, \\ldots\\) \\[\nc_0 = \\tfrac12 a_0, \\quad c_n = \\tfrac12 (a_n - \\mathrm{i} b_n), \\quad c_{-n} = \\tfrac12 (a_n + \\mathrm{i} b_n).\n\\]",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fourier-series",
    "href": "signal/fourier.html#fourier-series",
    "title": "8  Fourier Transform",
    "section": "",
    "text": "Definition 8.4 (Fourier Series) For a \\(L\\)-periodic function \\(f(t)\\) we can write \\[\nf(t) = \\frac{a_0}{2} + \\sum_{k=1}^\\infty \\left(a_k \\cos\\left(\\omega k t \\right)+ b_k \\sin\\left(\\omega k t\\right)\\right),\n\\tag{8.1}\\] for \\[\n\\begin{aligned}\na_k &= \\frac{2}{L}\\int_0^L f(t) \\cos\\left(\\omega k t \\right)\\, \\mathrm{d}t,\\\\\nb_k &= \\frac{2}{L}\\int_0^L f(t) \\sin\\left(\\omega k t \\right)\\, \\mathrm{d}t.\n\\end{aligned}\n\\] where we can view the last two equations as the projection onto the orthogonal basis \\(\\{\\cos(k t), \\sin(k t)\\}_{k=0}^\\infty\\), i.e. \\[\n\\begin{aligned}\na_k &= \\frac{1}{\\|\\cos\\left(\\omega k t\\right)\\|_2^2} \\langle f(t), \\cos\\left(\\omega k t\\right)\\rangle, \\\\\nb_k &= \\frac{1}{\\|\\sin\\left(\\omega k t\\right)\\|_2^2} \\langle f(t), \\sin\\left(\\omega k t\\right)\\rangle.\n\\end{aligned}\n\\]\nIf we perform a partial reconstruction by truncating the series at \\(M\\) we get \\[\n\\hat{f}_M(t) = \\frac{a_0}{2} + \\sum_{k=1}^M \\left(a_k \\cos\\left(\\omega k t \\right)+ b_k \\sin\\left(\\omega k t \\right)\\right).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(f(t)\\) is real valued than \\(c_k = \\overline{c}_{-k}\\).\n\n\n\n\n\n\n\n\n\nExample 8.1 (Fourier Series of Hat functions) We test the Fourier Series with two different hat functions. The first represents a triangle with constant slope up and down, the second a rectangle with infinite slope in the corners.\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n# Parameters\nL = 2 * np.pi\nM = 5\nM2 = 50\nN = 512\n# Hat functions\nfun = lambda t, L: 0 if abs(t) &gt; L / 4 else (1 - np.sign(t) * t * 4 / L)\nfun2 = lambda t, L: 0 if abs(t) &gt; L / 4 else 1\n\n# t\n1t = np.linspace(-L/2, L/2, N, endpoint=False)\ndt = t[1] - t[0]\nw = np.pi * 2 / L\n\nf = np.fromiter(map(lambda t: fun(t, L), t), t.dtype)\nf2 = np.fromiter(map(lambda t: fun2(t, L), t), t.dtype)\n\n# Necessary functions\nscalarproduct = lambda f, g, dt: dt * np.vecdot(f, g)\na_coeff = lambda n, f: 2 / L * scalarproduct(f, np.cos(w * n * t), dt)\nb_coeff = lambda n, f: 2 / L * scalarproduct(f, np.sin(w * n * t), dt)\n\n# f_hat_0\nf_hat = np.zeros((M + 1, N))\nf_hat[0, :] = 1/2 * a_coeff(0, f)\nf2_hat = np.zeros((M2 + 1, N))\nf2_hat[0, :] = 1/2 * a_coeff(0, f2)\n\n# Computation of the approximation\na = np.zeros(M)\nb = np.zeros(M)\nfor i in range(M):\n    a[i] = a_coeff(i + 1, f)\n    b[i] = b_coeff(i + 1, f)\n    f_hat[i + 1, :] = f_hat[i, :] + \\\n        a[i] * np.cos(w * (i + 1) * t) + \\\n        b[i] * np.sin(w * (i + 1) * t)\n\nfor i in range(M2):\n    f2_hat[i + 1, :] = f2_hat[i, :] + \\\n        a_coeff(i + 1, f2) * np.cos(w * (i + 1) * t) + \\\n        b_coeff(i + 1, f2) * np.sin(w * (i + 1) * t)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f, label=r\"$f$\")\nplt.plot(t, f_hat[-1, :], label=r\"$\\hat{f_7}$\")\nplt.xticks([])\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, f_hat[0, :], label=rf\"$a_{0}$\")\nfor i in range(M):\n    plt.plot(t, a[i] * np.cos(w * (i+1) * t),\n             label=rf\"$a_{i+1}\\cos({i+1}\\omega t)$\")\nplt.legend(ncol=np.ceil((M + 1) / 2), bbox_to_anchor=(1, -0.1))\nplt.xticks([])\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, f2, label=r\"$f$\")\nplt.plot(t, f2_hat[7, :], label=r\"$\\hat{f}_7$\")\nplt.plot(t, f2_hat[20, :], label=r\"$\\hat{f}_{20}$\")\nplt.plot(t, f2_hat[50, :], label=r\"$\\hat{f}_{50}$\")\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n1\n\nNot including the endpoint is important, as this is part of the periodicity of the function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sawtooth function and the reconstruction with 7 nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Nodes of the reconstruction\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Step function and the reconstruction with various nodes\n\n\n\n\n\n\n\nFigure 8.1: Fourier transform of a two hat functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.1 (Self implementation Example 8.1) Implement the code yourself by filling out the missing sections:\n\n\nCode fragment for implementation.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nL = 2 * np.pi   # Interval\nM = 5           # Nodes for the first function\nM2 = 50         # Nodes for the second function\nN = 512         # Interpolation points\n# Hat functions\nfun = lambda x, L: # tooth\nfun2 = lambda x, L: # step\n\n# x and y for the functions\nt = # points for the evaluation ()\ndt = t[1] - t[0]\nw = np.pi * 2 / L\n\nf = np.fromiter(map(lambda t: fun(t, L), t), t.dtype)\nf2 = np.fromiter(map(lambda t: fun2(t, L), t), t.dtype)\n\n# Necessary functions\nscalarproduct = lambda f, g, dt: # see definition in the notes\na_coeff = lambda n, f: # see definition in the notes\nb_coeff = lambda n, f: # see definition in the notes\n\n# f_hat_0\nf_hat = np.zeros((M + 1, N))\nf_hat[0, :] = # a_0 for f\nf2_hat = np.zeros((M2 + 1, N))\nf2_hat[0, :] = # a_0 for f2\n\n# Computation of the approximation\na = np.zeros(M)\nb = np.zeros(M)\nfor i in range(M):\n    a[i] = a_coeff(i + 1, f)\n    b[i] = b_coeff(i + 1, f)\n    f_hat[i + 1, :] = f_hat[i, :] + \\\n        a[i] * np.cos(w * (i + 1) * t) + \\\n        b[i] * np.sin(w * (i + 1) * t)\n\nfor i in range(M2):\n    f2_hat[i + 1, :] = f2_hat[i, :] + \\\n        a_coeff(i + 1, f2) * np.cos(w * (i + 1) * t) + \\\n        b_coeff(i + 1, f2) * np.sin(w * (i + 1) * t)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f, label=r\"$f$\")\nplt.plot(t, f_hat[-1, :], label=r\"$\\hat{f_7}$\")\nplt.xticks([])\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, f_hat[0, :], label=rf\"$a_{0}$\")\nfor i in range(M):\n    plt.plot(t, a[i] * np.cos(w * (i+1) * t),\n             label=rf\"$a_{i+1}\\cos({i+1}\\omega t)$\")\nplt.legend(ncol=np.ceil((M + 1)/2), bbox_to_anchor=(1, -0.1))\nplt.xticks([])\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, f2, label=r\"$f$\")\nplt.plot(t, f2_hat[7, :], label=r\"$\\hat{f}_7$\")\nplt.plot(t, f2_hat[20, :], label=r\"$\\hat{f}_{20}$\")\nplt.plot(t, f2_hat[50, :], label=r\"$\\hat{f}_{50}$\")\nplt.xlabel(r\"$x$\")\nplt.legend()\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe phenomenon that the truncated Fourier series oscillates in Figure 8.1 (c) due to the discontinuity of the function is called the Gibbs phenomenon.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fourier-transform",
    "href": "signal/fourier.html#fourier-transform",
    "title": "8  Fourier Transform",
    "section": "8.2 Fourier Transform",
    "text": "8.2 Fourier Transform\nThe Fourier Series is defined for \\(L\\)-periodic functions. The Fourier transform extends this to functions with the domain extended to \\(\\pm\\infty\\).\nLet us start of with the series representation we already know: \\[\nf(t) = \\sum_{k=-\\infty}^\\infty c_k \\mathrm{e}^{\\mathrm{i} \\omega k t}\n\\] with the coefficients \\[\nc_k = \\frac{1}{2L} \\int_{-L}^{L} f(t) \\mathrm{e}^{-\\mathrm{i} \\omega_k t}\\, \\mathrm{d}t,\n\\] with \\(\\omega_k=\\frac{k\\pi}{L} = k\\Delta \\omega\\).\nIf we now perform the transition for \\(L \\to \\infty\\) resulting in \\(\\Delta\\omega \\to 0\\) and basically moving from discrete frequencies to a continuous set of frequencies. This results in \\[\nf(t) = \\lim_{\\Delta \\omega \\to 0} \\sum_{k=-\\infty}^\\infty \\frac{\\Delta \\omega}{2\\pi}\n\\int_{-\\tfrac{\\pi}{\\Delta \\omega}}^{\\tfrac{\\pi}{\\Delta \\omega}} f(\\xi) \\mathrm{e}^{-\\mathrm{i} k \\Delta \\omega \\xi}\\, \\mathrm{d}\\xi\\,\\, \\mathrm{e}^{\\Delta\\omega\\mathrm{i} k t}\n\\] which is a Riemann integral and the kernel becomes the Fourier Transform of our function.\n\n\n\n\n\n\n\nDefinition 8.5 (Fourier Transform) A function \\(f\\,:\\, \\mathbb{R} \\to \\mathbb{R}\\) is called fourier transposable if \\[\n\\hat{f}(\\omega) = \\mathcal{F}\\{f(t)\\} = \\int_{-\\infty}^{\\infty} f(t)\\mathrm{e}^{-\\mathrm{i} \\omega t}\\, \\mathrm{d}t\n\\] exists for all \\(\\omega\\in\\mathbb{R}\\). In this case we call \\(\\hat{f}(\\omega) \\equiv \\mathcal{F}\\{f(t)\\}\\) the Fourier transform of \\(f(t)\\).\nThe inverse Fourier transform is defined as \\[\n\\mathcal{F}^{-1}\\{\\hat{f}(\\omega)\\} = \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} \\hat{f}(\\omega)\\mathrm{e}^{\\mathrm{i} \\omega x}\\, \\mathrm{d}\\omega\n\\]\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pair \\((f, \\hat{f})\\) is often called the Fourier transform pair.\nThe two integrals converge, as long as both functions are Lebesgue integrable, i.e.  \\[\\int_{-\\infty}^\\infty|f(t)|\\, \\mathrm{d}t \\le \\infty,\\] or \\(f, \\hat{f} \\in L^1[(-\\infty, \\infty)]\\).\n\n\nAs could be expected, the Fourier transform has properties that lead to computational advantages.\nFor tow functions \\(f, g \\in L^1[(-\\infty, \\infty)]\\) and \\(\\alpha, \\beta\\in\\mathbb{C}\\) the following properties hold:\n\nLinearity \\[\n\\mathcal{F}\\{\\alpha f(t) + \\beta g(t)\\} =\n\\alpha \\mathcal{F}\\{f(t)\\} + \\beta \\mathcal{F}\\{g(t)\\} =\n\\alpha \\hat{f}(\\omega)+ \\beta \\hat{g}(\\omega),\n\\] and \\[\n\\mathcal{F}^{-1}\\{\\alpha \\hat{f}(\\omega) + \\beta \\hat{g}(\\omega)\\} =\n\\alpha \\mathcal{F}^{-1}\\{\\hat{f}(\\omega)\\} + \\beta \\mathcal{F}^{-1}\\{\\hat{g}(\\omega)\\} =\n\\alpha f(t) + \\beta g(t).\n\\]\nConjugation \\[\n\\mathcal{F}\\{\\overline{f(t)}\\} = \\overline{\\hat{f}(-\\omega)}.\n\\]\nScaling, for \\(\\alpha \\neq 0\\) \\[\n\\mathcal{F}\\{f(\\alpha t)\\} = \\frac{1}{|\\alpha|}\\hat{f}\\left(\\frac{\\omega}{\\alpha}\\right).\n\\]\nDrift in time, for \\(a\\in\\mathbb{R}\\) \\[\n\\mathcal{F}\\{f(t - a)\\} = \\mathrm{e}^{-\\mathrm{i}\\omega a}\\hat{f}(\\omega).\n\\]\nDrift in frequency, for \\(a\\in\\mathbb{R}\\) \\[\n\\mathrm{e}^{\\mathrm{i} a t} \\mathcal{F}\\{f(t)\\} = \\hat{f}(\\omega - a).\n\\]\nIf \\(f\\) is even or odd, than \\(\\hat{f}\\) is even or odd, respectively.\nDerivative in time \\[\n\\mathcal{F}\\{\\partial_t f(t)\\} = \\mathrm{i} \\omega \\hat{f}(\\omega)\n\\] We are going to prove this by going through the lines \\[\n\\begin{aligned}\n\\mathcal{F}\\left\\{\\frac{d}{d\\,t}f(t)\\right\\} &= \\int_{-\\infty}^\\infty f'(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\left[f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^\\infty -\\mathrm{i} \\omega f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\mathrm{i} \\omega \\int_{-\\infty}^\\infty f(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t \\\\\n&= \\mathrm{i} \\omega \\mathcal{F}\\{f(t)\\}\n\\end{aligned}\n\\] For higher derivatives we get \\[\n\\mathcal{F}\\{\\partial_t^n f(t)\\} = \\mathrm{i}^n \\omega^n \\hat{f}(\\omega)\n\\]\nDerivative in frequency \\[\n\\mathcal{F}\\{t^n f(t)\\} = \\mathrm{i}^n \\partial_\\omega^n\\hat{f}(\\omega)\n\\]\nThe convolution of two functions is defined as \\[\n(f \\ast g)(t) = \\int_{-\\infty}^{\\infty}f(t - \\xi) g(\\xi)\\, \\mathrm{d}\\xi,\n\\] and for the Fourier transform \\[\n\\mathcal{F}\\{(f \\ast g)(t)\\} = \\hat{f} \\cdot \\hat{g}.\n\\]\nParseval’s Theorem \\[\n\\|f\\|_2^2 = \\int_{-\\infty}^{\\infty}|f(t)|^2\\, \\mathrm{d}t = \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty}|\\hat{f}(\\omega)|^2\\, \\mathrm{d}\\omega\n\\] stating that the Fourier Transform preserves the 2-norm up to a scaling factor.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#discrete-fourier-transform",
    "href": "signal/fourier.html#discrete-fourier-transform",
    "title": "8  Fourier Transform",
    "section": "8.3 Discrete Fourier Transform",
    "text": "8.3 Discrete Fourier Transform\nThe Discrete Fourier Transform (DFT) is a way of approximating the Fourier transform on discrete vectors of data and it essentially a discretized version of the Fourier transform by sampling the function and numerical integration.\n\n\n\n\n\n\n\nDefinition 8.6 (Discrete-Fourier Transform) For equally spaced values \\(t_k = k\\Delta t\\), for \\(k\\in\\mathbb{Z}\\) and \\(\\Delta t&gt;0\\) and the discrete values of the function evaluations \\(f_k=f(t_k)\\). If the function is periodic with \\(L=N\\Delta t\\) than the discrete Fourier transform is given as \\[\n\\hat{f}_k = \\sum_{j=0}^{N-1}f_j\\, \\mathrm{e}^{-\\mathrm{i} j k\\tfrac{2 \\pi}{N}},\n\\tag{8.3}\\] and its inverse (iDFT) as \\[\nf_k = \\frac{1}{N}\\sum_{j=0}^{N-1}\\hat{f}_j\\, \\mathrm{e}^{\\mathrm{i} j k\\tfrac{2 \\pi}{N}}.\n\\]\n\n\n\n\nAs we can see, the DFT is a linear operator and therefore it can be written as a matrix vector product \\[\n\\left[\n    \\begin{array}{c} \\hat{f}_1 \\\\ \\hat{f}_2 \\\\ \\hat{f}_3 \\\\ \\vdots \\\\ \\hat{f}_N \\end{array}\n\\right]\n=\n\\left[\n    \\begin{array}{ccccc} 1 & 1 & 1 & \\dots & 1 \\\\\n                         1 & \\omega_N & \\omega_N^2 & \\dots & \\omega_N^{N-1} \\\\  \n                         1 & \\omega_N^2 & \\omega_N^4 & \\dots & \\omega_N^{2(N-1)} \\\\\n                         \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                         1 & \\omega_N^{N-1} & \\omega_N^{2(N-1)} & \\dots & \\omega_N^{(N-1)^2 } \\\\\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} f_1 \\\\ f_2 \\\\ f_3 \\\\ \\vdots \\\\ f_N \\end{array}\n\\right]\n\\tag{8.4}\\] with \\(\\omega_N = \\exp({-\\mathrm{i} \\tfrac{2 \\pi}{N}})\\).\n\n\n\n\n\n\nNote\n\n\n\nThe matrix of the DFT is a unitary Vandermonde matrix.\n\n\nAs we can transfer the properties of the Fourier transform to the DFT we get the nice properties for sampled signals.\nThe downside of the DFT is that it does not scale well for large \\(N\\) as the matrix-vector multiplication is \\(\\mathcal{O}(N^2)\\) and becomes slow.\n\n\n\n\n\n\nTip\n\n\n\nCode for the computation of the DFT matrix.\n\n\nShow the code.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nN = 256\nw = np.exp(-1j * 2 * np.pi / N )\n\nJ, K = np.meshgrid(np.arange(N), np.arange(N))\nDFT = np.power(w, J*K)",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#fast-fourier-transform",
    "href": "signal/fourier.html#fast-fourier-transform",
    "title": "8  Fourier Transform",
    "section": "8.4 Fast Fourier Transform",
    "text": "8.4 Fast Fourier Transform\nIn 1965, James W. Cooley (IBM) and John W. Tukey (Princeton) developed the so called fast Fourier transform (FFT) that scales with \\(\\mathcal{O}(N \\log(N))\\). which becomes almost linear for large enough \\(N\\), see Cooley and Tukey (1965).\n\n\n\n\n\n\nNote\n\n\n\nTo give an idea of what this change means and why this algorithm was a game changer. Audio is most of the time sampled with 44.1kHz, i.e. 44 100 samples per second. For a 10s audio clip the vector \\(f\\) will have the length \\(N = 4.41 \\times 10^5\\). The DFT computation (without generating the matrix) results ins approximately \\(2\\times 10^{11}\\) multiplications. The FFT on the other hand requires \\(6\\times 10^6\\) leading to a speed-up of about \\(30 000\\).\nHow this influenced our world we know from the use in our daily communication networks.\n(Compare Brunton and Kutz 2022, 65–66)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe should note that Cooley and Tukey where not the first to propose a FFT but the provided the formulation used today. Gauss already formulated the FFT 150 years earlier in 1805 for orbital approximations. Apparently, he did the necessary computations in his head and needed a fast algorithm so he developed the FFT. Gauss being Gauss did not see this as something important and it did not get published until 1866 in his compiled notes, Gauß (1866).\n\n\nThe main idea of the FFT is to exploit symmetries in the Fourier transform and to relate the \\(N\\)-dimensional DFT to a lower dimensional DFT by reordering the coefficients.\n\n\n\n\n\n\n\nDefinition 8.7 (Fast-Fourier Transform) If we assume that \\(N = 2^n\\), i.e. a power of \\(2\\), in particular \\(N=2M\\), and \\(F_N\\) denotes the matrix of Equation 8.4 for dimension \\(N\\) and we have \\(\\hat{f} = F_N f\\) and \\(f = \\tfrac1N \\overline{F_N} \\hat{f}\\). By splitting \\(f\\) in the even and odd indices as \\[e = [f_0, f_2, \\ldots, f_{N-2}]^{\\mathsf{T}}\\in \\mathbb{C}^{M}\\] and \\[o = [f_1, f_3, \\ldots, f_{N-1}]^{\\mathsf{T}}\\in \\mathbb{C}^{M}\\] and for Equation 8.4 we get \\[\n\\begin{aligned}\n\\hat{f}_k &= \\sum_{j=0}^{N-1}f_j\\, \\omega^{j k} = \\sum_{j=0}^{M-1}f_{2j}\\, \\omega^{(2j) k} + \\sum_{j=0}^{M-1}f_{2j+1}\\, \\omega^{(2j+1) k} \\\\\n&= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} + \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k}.\n\\end{aligned}\n\\] If we further split \\(\\hat{f}\\) in an upper and lower part \\[u = [\\hat{f}_0, \\hat{f}_2, \\ldots, \\hat{f}_{M-1}]^{\\mathsf{T}}\\in \\mathbb{C}^{M}\\] and \\[l = [\\hat{f}_{M}, \\hat{f}_{M+1}, \\ldots, \\hat{f}_{N-1}]^{\\mathsf{T}}\\in \\mathbb{C}^{M}\\] and with the property \\(\\omega^{k+M} = \\omega^k \\omega^M = - \\omega^k\\) we get\n\\[\n\\begin{aligned}\nu_k &= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} + \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k},\\\\\nl_k &= \\sum_{j=0}^{M-1}e_j\\, (\\omega^2)^{j k} - \\omega^k\\sum_{j=0}^{M-1}o_{j}\\, (\\omega^2)^{j k}.\n\\end{aligned}\n\\] This results in the more visual matrix representation \\[\n\\hat{f}= F_N f =\n\\left[\n    \\begin{array}{cc}\n    I_M & D_M \\\\\n    I_M & -D_M\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{cc}\n    F_M & 0 \\\\\n    0 & F_M\n    \\end{array}\n\\right]\n\\left[\n    \\begin{array}{cc}\n    f_{even}\\\\\n    f_{odd}\n    \\end{array}\n\\right],\n\\] for \\(I_M\\) being the identity matrix in dimension \\(M\\) and \\[\nD_M =\n\\left[\n    \\begin{array}{ccccc} 1 & 0 & 0 & \\dots & 0 \\\\\n                         0 & \\omega & 0 & \\dots & 0 \\\\  \n                         0 & 0 & \\omega^2 & \\dots & 0 \\\\\n                         \\vdots & \\vdots & \\ddots & \\ddots &\\vdots\\\\\n                         0 & 0 & 0 & \\dots & \\omega^{(M-1)} \\\\\n    \\end{array}\n\\right]\n\\] Now repeat this \\(n\\) times.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(N\\) is not a power of \\(2\\) padding is used to make the size fit by extending the vector with zeros.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe original FFT paper (Cooley and Tukey 1965) uses bit flipping and similar techniques to boost performance even more. It can even be implemented to allow for in place computation to save storage.\nIMAGE\n(Compare Meyberg and Vachenauer 1992, 331)\n\n\n\n8.4.1 Examples for the FFT in action\nIn order to give an idea how FFT works in an application we follow the examples given in (Brunton and Kutz 2022, 66–76).\n\n\n\n\n\n\n\nExample 8.2 (FFT for de-noising) For a signal consisting of two main frequencies \\(f_1 = 50\\) and \\(f_2=120\\) we construct a signal \\[\nf(t) = \\sin(2\\pi f_1 t) + \\sin(2\\pi f_2 t)\n\\] and add some Gaussian white noise np.random.randn.\nWe compute the FFT from the two signals and their power spectral density (PSD), i.e.  \\[\nPSD(\\hat{f})=\\frac1N \\|\\hat{f}\\|^2.\n\\] We use the PSD to take all frequencies with a \\(PSD &lt; 100\\) out of our reconstruction as a filter. This removes noise from the signal.\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)\n\nf_clean = fun(t)\nf_noise = fun(t) + 2.5 * np.random.randn(len(t))              # Add some noise\n\nfhat_noise = np.fft.fft(f_noise)\nfhat_clean = np.fft.fft(f_clean)\n\nPSD_noise = np.abs(fhat_noise)**2 / N\nPSD_clean = np.abs(fhat_clean)**2 / N\n\nfreq = (1 / (dt * N)) * np.arange(N)\nL = np.arange(1, np.floor(N/4), dtype='int')\n\n# Apply filter in spectral space\nfilter = PSD_noise &gt; 100\nPSDclean = PSD_noise * filter\nfhat_filtered = filter * fhat_noise\nf_filtered = np.fft.ifft(fhat_filtered)\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_noise, \":\", label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.figure(1)\nplt.plot(freq[L], PSD_noise[L], label=r\"Noisy\")\nplt.plot(freq[L], PSD_clean[L], label=r\"Clean\")\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"PSD\")\nplt.xlim(0, int(freq[L[-1] + 1]))\nplt.legend(loc=1)\nplt.gca().set_aspect(1)\n\nplt.figure(3)\nplt.plot(t, np.real(f_filtered), label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original clean signal and noisy signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaled square norm of of the Fourier coefficients (PSD), only parts are shown.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Original signal and de-noised signal.\n\n\n\n\n\n\n\nFigure 8.2: Signal noise filter with FFT.\n\n\n\nAs can be seen in the Figure 8.2 (c), the reconstruction is not exact. This is due to the fact that the reconstructed frequencies are not matched exactly plus we have some multiples that show up as well. In particular:\n\n\n\n\n\n\n\n\n\nFrequency\nPSD\n\n\n\n\n0\n52.0\n145.259\n\n\n1\n120.0\n230.273\n\n\n2\n3976.0\n230.273\n\n\n3\n4044.0\n145.259\n\n\n\n\n\n\n\nNote: For Figure 8.2 (c) we discarded the imaginary part of the reconstruction.\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.2 (Self implementation Example 8.2) Implement the code yourself by filling out the missing sections:\n\n\nCode fragment for implementation.\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = # specify the sample steps for t\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: # define the function as described in the example\n\nf_clean = fun(t)\nf_noise = fun(t) + 2.5 * np.random.randn(len(t))\n\nfhat_noise = # transform the noisy signal with fft\nfhat_clean = # transform the clean signal with fft\n\nPSD_noise = # Compute the PSD for the noisy signal \nPSD_clean = # Compute the PSD for the clean signal \n\nfreq = (1 / (dt * N)) * np.arange(N)\nL = np.arange(1, np.floor(N/4), dtype='int')\n\n# Apply filter in spectral space\nfilter = # create the filter for the PSD values grater 100 \nPSDclean = # set everything not part of the filter to zero\nfhat_filtered = # apply the filter to the transformed function\nf_filtered = # apply the inverse fft\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_noise, \":\", label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.figure(1)\nplt.plot(freq[L], PSD_noise[L], label=r\"Noisy\")\nplt.plot(freq[L], PSD_clean[L], label=r\"Clean\")\nplt.xlabel(\"Frequency [Hz]\")\nplt.ylabel(\"PSD\")\nplt.xlim(0, int(freq[L[-1] + 1]))\nplt.legend(loc=1)\nplt.gca().set_aspect(1)\n\nplt.figure(3)\nplt.plot(t, np.real(f_filtered), label=r\"Noisy\")\nplt.plot(t, f_clean, label=r\"Clean\")\nplt.xlabel(\"Time [s]\")\nplt.ylabel(r\"$f$\")\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nExercise DFT vs. FFT\n\n\n\n\n\n\nExercise 8.3 (DFT vs. FFT) Implement Example 8.2 with DFT and FFT such that you can evaluate the runtime and create a plot showing the different runtime as well as check if the two produce the same result\n\n\n\n\nFor the Fourier transform we stated that the multiplication with \\(\\mathcal{F}\\{\\partial f\\}=\\mathrm{i}\\omega\\mathcal{F}\\{f\\}\\), similarly we can derive a formula for the numerical derivative of a sampled function by multiplying each entry of the transformed vector by \\(\\mathrm{i}\\kappa\\) for \\(\\kappa=\\tfrac{2\\pi k}{N}\\). \\(\\kappa\\) is called the discrete wavenumber associated with the component \\(k\\).\nLet us explore this with the example stated in (Brunton and Kutz 2022, 68–69).\n\n\n\n\n\n\n\nExample 8.3 (Spectral derivative) We compute the so called spectral derivative for the function \\[\n\\begin{aligned}\nf(t) &= \\cos(t) \\exp\\left(-\\frac{t^2}{25}\\right) \\\\\n\\partial_t f(t) &= -\\sin(t) \\exp\\left(-\\frac{t^2}{25}\\right) - \\frac{2}{25}t f(t)\n\\end{aligned}\n\\]\nIn order to porovide something to compare our results to we also compute the forward Euler finite-differences for the derivative \\[\n\\partial_t f(t_k) \\approx \\frac{f(t_{k+1}) - f(t_k)}{t_{k+1} - t_k}.\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n# Parameters\nN = 128\na, b = -15, 15\nL = b - a\n\nfun = lambda t: np.cos(t) * np.exp(-np.power(t, 2) / 25)\ndfun = lambda t: -(np.sin(t) * np.exp(-np.power(t, 2) / 25) + \\\n                 (2 / 25) * t * fun(t))\n\ndef fD(N, fun, dfun, a, b):\n    t = np.linspace(a, b, N, endpoint=False)\n    dt = t[1] - t[0]\n    f = fun(t)\n    df_DD = np.diff(f) / dt\n    df_DD = np.append(df_DD, (f[-1] - f[0]) / dt)\n    return df_DD, np.linalg.norm(df_DD - dfun(t)) / np.linalg.norm(df_DD)\n\ndef spD(N, fun, dfun, a, b):\n    t = np.linspace(a, b, N, endpoint=False)\n    f = fun(t)\n    fhat = np.fft.fft(f)\n    kappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n    df_hat = kappa * fhat * (1j)\n    df_r = np.fft.ifft(df_hat).real\n    return df_r, np.linalg.norm(df_r - dfun(t)) / np.linalg.norm(df_r)\n\n\n# Finite differences\ndf_fD, e = fD(N, fun, dfun, a, b)\n# Spectral derivative\ndf_spD, e = spD(N, fun, dfun, a, b)\n\n# Figures\nt = np.linspace(a, b, N, endpoint=False)\nplt.figure(0)\nplt.plot(t, dfun(t), label=\"Exact\")\nplt.plot(t, df_fD, \"-.\", label=\"Finite Differences\")\nplt.plot(t, df_spD, \"--\", label=\"Spectral\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\partial_t f$\")\nplt.legend(loc=1)\nplt.gca().set_aspect(5)\n\nplt.figure(1)\nn = 19\nM = range(3, n)\ne_spD = np.ones(len(M))\ne_fD = np.ones(len(M))\nfor i, j in enumerate(M):\n    _, e_fD[i] = fD(2**j, fun, dfun, a, b)\n    _, e_spD[i] = spD(2**j, fun, dfun, a, b)\n\nplt.loglog(np.pow(2, M), e_fD, label=\"Finite differences\")\nplt.loglog(np.pow(2, M), e_spD, label=\"Spectral derivative\")\nplt.grid()\nplt.xlabel(\"N\")\nplt.ylabel(\"Relative Error\")\nplt.legend(loc=1)\nplt.gca().set_aspect(2.5e-1)\n\nfun_saw = lambda t, L: 0 if abs(t) &gt; L / 4 else (1 - np.sign(t) * t * 4 / L)\na, b = -np.pi, np.pi\nL = b - a\nfun2 = lambda t: np.fromiter(map(lambda t: fun_saw(t, L), t), t.dtype)\nN = 2**10\n\nt = np.linspace(a, b, N, endpoint=False)\n\nplt.figure(2)\ndf_spD, _ = spD(N, fun2, fun2, a, b)\ndf_fD, _ = fD(N, fun2, fun2, a, b)\nplt.plot(t, fun2(t), label=r\"$f$\")\nplt.plot(t, df_fD, \"-.\", label=\"Finite derivative\")\nplt.plot(t, df_spD, \"--\", label=\"Spectral derivative\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$f, \\partial_t f$\")\nplt.xlim(-2, 2)\nplt.legend(loc=1)\nplt.gca().set_aspect(0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Computation of the derivative with different methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Accuracy of the methods for computing the derivative\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Gibbs phenomenon for the spectral derivative for discontinuous functions\n\n\n\n\n\n\n\nFigure 8.3: Computing the derivative of a function\n\n\n\nAs can be seen in Figure 8.3 (b) we can reduce the error of both methods by increasing \\(N\\). Nevertheless, the spectral method is more accurate and converges faster.\n\n\n\n\nIn Section 3.1.2 we have already seen how the eigendecomposition can be used to compute the solution of ordinary differential equations by changing the basis and transforming the equation into a basis that can be handled easy. The same is true with the Fourier transformation.\nAs mentioned before, Fourier introduced it to solve the heat equation and we will do the same.\n\n\n\n\n\n\nImportant\n\n\n\nAbove we have always transformed the time component of a function into the frequency domain via FFT. In the next example we have the time evolution of a signal and therefore the function depends on time and space. We apply the FFT to the space component to compute the derivative in frequency domain w.r.t. the transformed variable.\n\n\n\n\n\n\n\n\n\nExample 8.4 (Heat Equation) The heat equation in 1D is given by \\[\n\\dot{u}(t, x) = \\alpha^2 \\partial_x^2 u(\\tau, x),\n\\] for \\(u(t, x)\\) as the temperature distribution in space (\\(x\\)) and time (\\(t\\)). By applying the Fourier transform we get \\(\\mathcal{F}\\{u(t,x)\\}=\\hat{u}(t, \\omega)\\) and \\[\n\\dot{\\hat{u}}(t, \\omega) = - \\alpha^2\\omega^2\\hat{u}(t, \\omega).\n\\] This transformed the partial differential equation into an ordinary differential equation and we can solve it for each fixed frequency \\(\\omega\\) as \\[\n\\hat{u}(t, \\omega) = \\mathrm{e}^{-\\alpha^2\\omega^2 t} \\hat{u}(0, \\omega).\n\\] where \\(\\hat{u}(0, \\omega)\\) is nothing else than the Fourier transform of the initial temperature distribution at time \\(t=0\\). To compute the inverse Fourier transform we can make use of the convolution property introduced above.\n\\[\n\\begin{aligned}\nu(t, x) &= \\mathcal{F}^{-1}\\{\\hat{u}(t, \\omega)\\} = \\mathcal{F}^{-1}\\{\\mathrm{e}^{-\\alpha^2\\omega^2 t}\\} \\ast u(0, x) \\\\\n&= \\frac{1}{2 \\alpha \\sqrt{\\pi t}} \\mathrm{e}^{-\\tfrac{x^2}{4\\alpha^2t}} \\ast u(0, x).\n\\end{aligned}\n\\]\nFor the numerical computation it is more convenient to stay in the frequency domain and use \\(\\kappa\\) as before.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nalpha = 1\na, b = -50, 50\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = np.zeros_like(x)\nu0[np.abs(x) &lt; 10] = 1\nu0hat = np.fft.fft(u0)\n\n# Simulate in Fourier frequency domain\ndt = 0.001\nT = np.arange(0, 10, dt)\n\nfun = lambda t, x: - alpha**2 * (np.power(kappa, 2)) * x\neuler = lambda y, dt, t, fun: y + dt * fun(t, y)\n\nuhat = np.zeros((len(T), len(u0hat)), dtype=\"complex\")\nuhat[0, :] = u0hat\nfor i, t in enumerate(T[1:]):\n    uhat[i + 1, :] = euler(uhat[i, :], dt, t, fun)\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k, :])\n\nu = u.real    \n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\n#ax.view_init(elev=45, azim=-20, roll=0)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:int(1 / dt), :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u[0:-1:100]), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of the heat equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of the transport equation.\n\n\n\n\n\n\n\nFigure 8.4: Simulation of the heat equation in 1D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 8.4 (Self implementation Example 8.4) Implement the code yourself by filling out the missing sections:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nalpha = 1\na, b = -50, 50\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = np.zeros_like(x)\nu0[np.abs(x) &lt; 10] = 1\nu0hat = np.fft.fft(u0)\n\n# Simulate in Fourier frequency domain\ndt = 0.001\nT = np.arange(0, 10, dt)\n\nfun = lambda t, x: #right hand side for the euler function\neuler = lambda y, dt, t, fun: # euler step\n\nuhat = np.zeros((len(T), len(u0hat)), dtype=\"complex\")\nuhat[0, :] = u0hat\nfor i, t in enumerate(T[1:]):\n    uhat[i + 1, :] = euler(uhat[i, :], dt, t, fun)\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k,:])\n\nu = u.real    \n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:int(1 / dt), :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u[0:-1:100]), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\nNext, we look at the transport equation.\n\n\n\n\n\n\n\nExample 8.5 (Transport or advection) The transport equation in 1D is given by \\[\n\\dot{u} = - c \\partial_x u.\n\\] It is called the transport equation, as the initial value simply propagates in time to the right hand side of the domain with speed \\(c\\), i.e. \\(u(t, x) = u(0, x -c t)\\). The solution is simply computed by exchanging the right hand side from the above code.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nc = 2\na, b = -20, 20\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = 1/np.cosh(x)\nu0hat = np.fft.fft(u0)\n\n# SciPy's odeint function doesn't play well with complex numbers,\n# so we recast the state u0hat from an N-element complex vector \n# to a 2N-element real vector\nu0hat_ri = np.concatenate((u0hat.real, u0hat.imag))\n\n# Simulate in Fourier frequency domain\ndt = 0.025\nT = np.arange(0, 600*dt, dt)\n\ndef rhsWave(uhat_ri,t,kappa,c):\n    uhat = uhat_ri[:N] + (1j) * uhat_ri[N:]\n    d_uhat = -c * (1j ) *kappa * uhat\n    d_uhat_ri = np.concatenate((d_uhat.real, d_uhat.imag)).astype('float64')\n    return d_uhat_ri\n\nuhat_ri = odeint(rhsWave, u0hat_ri, T, args=(kappa, c))\n\nuhat = uhat_ri[:, :N] + (1j) * uhat_ri[:, N:]\n\nu = np.zeros_like(uhat)\n\nfor k in range(len(T)):\n    u[k, :] = np.fft.ifft(uhat[k, :])\n\nu = u.real    \n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\nu_plot = u[0:-1:60, :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u), aspect=1.5)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of the transport equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of the transport equation.\n\n\n\n\n\n\n\nFigure 8.5: Computing the derivative of a function\n\n\n\n\n\n\n\nTo increase complexity from these simple equations we move to the nonlinear Burger’s equation which is a combination of the two above and an example that produces shock waves in fluids.\n\n\n\n\n\n\n\nExample 8.6 (Burger’s equation) Burger’s equation in 1D is given by \\[\n\\dot{u} + u \\partial_x u = \\nu \\partial_x^2 u.\n\\] The equation consists of a nonlinear convection part \\(u \\partial_t u\\) as well as diffusion. The convection part is designed in such a way that larger amplitudes travel faster and therefore causes a shock wave to form.\nRegarding the solution of this equation, the interesting part is that we need to map back and forth between the Fourier space and time space to apply the nonlinearity.\nThis example is also very useful to test the capabilities of your solver as by decreasing the diffusion factor further and further we can approach an infinitely steep shock and in theory it can break like an ocean wave.\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.5 and Code 2.6).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnu = 0.001\na, b = -10, 10\nL = b - a\nN = 1000\nx = np.linspace(a, b, N, endpoint=False)\ndx = x[1] - x[0]\n\n# Define discrete wavenumbers\n#kappa = 2 * np.pi * np.fft.fftfreq(N, d=dx)\nkappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))\n\n# Initial condition\nu0 = 1/np.cosh(x)\n\n# Simulate in Fourier frequency domain\ndt = 0.025\nT = np.arange(0,100*dt,dt)\n\ndef rhsBurgers(u, t, kappa, nu):\n    uhat = np.fft.fft(u)\n    d_uhat = (1j) * kappa * uhat\n    dd_uhat = -np.power(kappa, 2) * uhat\n    d_u = np.fft.ifft(d_uhat)\n    dd_u = np.fft.ifft(dd_uhat)\n    du_dt = -u * d_u + nu * dd_u\n    return du_dt.real\n\nu = odeint(rhsBurgers, u0, T, args=(kappa, nu))\n\n# Waterfall plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$t$\")\nax.set_zlabel(r\"$u(t, x)$\")\nax.set_zticks([])\n\nu_plot = u[0:-1:10, :]\nfor j in range(u_plot.shape[0]):\n    ys = j * np.ones(u_plot.shape[1])\n    ax.plot(x, ys, u_plot[j, :])\n    \n# Image plot\nplt.figure()\nplt.imshow(np.flipud(u), aspect=8)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$t \\to$\")\nplt.gca().axes.get_xaxis().set_ticks([])\nplt.gca().axes.get_yaxis().set_ticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Evolution of Burger’s equation in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) x-t diagram of Burger’s equation.\n\n\n\n\n\n\n\nFigure 8.6: Time integration of the Burger’s equation\n\n\n\n\n\n\n\nThe following example is taken from (Meyberg and Vachenauer 1992).\n\n\n\n\n\n\n\nExample 8.7 (Ground Emitter Circuit) We can use FFT to compute some base properties of a ground emitter circuit Figure 8.7 and how the signal is propagated through this circuit Figure 8.8.\n\n\n\n\n\n\nFigure 8.7: Electric circuit\n\n\n\n\n\nShow the raw data for the example\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nfrequ = 2 * np.pi * 50\nf = lambda t: 0.8 * np.sin(t * frequ)\ndiod = lambda t: (np.exp(1.2 + t) - 1)\n\nt = np.linspace(0, 2 * np.pi / frequ, 1024, endpoint=False)\nx = np.linspace(np.min(f(t)) * 1.3, np.max(f(t)) * 1.1, 1024)\n\nic = lambda t: diod(f(t))\n\nk = np.arange(0, 16) * 1 / 16\n\n# The provided figures where used to create the illustation\nif False:\n    plt.figure()\n    plt.plot(f(t), t)\n    plt.plot([np.min(t),np.max(t)], [0,0], \"gray\")\n    plt.axis(\"off\")\n    plt.savefig(\"sin.svg\", transparent=True)\n    plt.figure()\n    plt.plot(x, diod(x))\n    z = np.array([0, np.min(f(t)), np.max(f(t))])\n    plt.plot(z, diod(z), \"bx\")\n    plt.axis(\"off\")\n    plt.xlim([-2,5])\n    plt.gcf().patch.set_visible(False)\n    plt.savefig(\"diode.svg\", transparent=True)\n\n    plt.figure()\n    plt.plot(t, ic(t))\n    plt.plot(2*np.pi*k/frequ, ic(2*np.pi*k/frequ), \"ro\")\n    plt.axis(\"off\")\n    plt.savefig(\"ic.svg\", transparent=True, bbox_inches =\"tight\")\n\ny = ic(k)\nyhat = (np.fft.fft(y))\n#Necessary correction factor for the FFT\nfactor = 1 / 16\nyy = factor * yhat\n\nic_mean = np.mean(ic(np.linspace(0, 1/50, 2**20)))\nc0 = yy[0].real\neffective_value = np.linalg.norm(yy[1:])\nharmonic_distortion = np.linalg.norm(yy[3:-2])/np.linalg.norm(yy[1:])\n\n\nThe transition of the transistor can be approximated by a diode and is illustrated in the \\(u_{BE}\\) vs. \\(i_C\\) diagram on the top left (orange signal in Figure 8.8). The exciting function \\(u\\) on the bottom left is a normal sine wave with \\(50\\mathrm{Hz}\\).\nAll together we get the equation \\[\ni_c = \\mathrm{e}^{1.2 + 0.8 \\sin(2 \\pi 50 t)} - 1\n\\] for the current in \\([\\mathrm{mA}]\\) running through the transistor.\n\n\n\n\n\n\nFigure 8.8: Signal transition for the electric circuit shown above\n\n\n\nWe can see, that the signal is boosted depending on the characteristic curve of the diode, i.e. the positive part of the sine more than the negative part. If we compute take the fourier transform as illustrated with the red points on the curve on the right we get from the FFT coefficients \\(c_i\\):\n\nthe direct current component:\n\n\\[\nc_0 = 2.873\n\\]\n\nthe effective value from Parseval’s Theorem:\n\n\\[\n\\overline{i_c} = \\sqrt{\\sum_{i=0}^{15} |c_i|^2}= 2.071\n\\]\n\nthe total harmonic distortion (how the signal is deformed):\n\n\\[\nTHD = \\frac{\\sqrt{\\sum_{i=2}^{14} |c_i|^2}}{\\sqrt{\\sum_{i=1}^{15} |c_i|^2}}=0.193\n\\]\n\n\n\n\nThis concludes our example with the FFT.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/fourier.html#gabor-transform",
    "href": "signal/fourier.html#gabor-transform",
    "title": "8  Fourier Transform",
    "section": "8.5 Gabor Transform",
    "text": "8.5 Gabor Transform\nThe Fourier transform is computed as soon as all the values of a signal are sampled. It provides spectral information over the entire signal and not when in time certain frequencies occur, i.e. no temporal information. The Fourier transform can only characterize periodic and stationary signals. The time component is used during the integration and no longer present in the transformed signal.\nIf both is needed in order to generate a spectrogram (plot frequency versus time), the Gabor transform brings remedy. Technically it is the Fourier transform of the signal masked by a sliding window \\[\n\\mathcal{G}\\{f\\}(t, \\omega) = G_f(t, \\omega) = \\int_{-\\infty}^\\infty f(\\tau) \\mathrm{e}^{-\\mathrm{i}\\omega\\tau}\\overline{g}(\\tau - t)\\, \\mathrm{d}\\tau.\n\\] for the so called kernel \\(g(t)\\) a Gaussian bell curve is often used \\[\ng(t - \\tau) =  \\mathrm{e}^{-\\tfrac{(t-\\tau)^2}{a^2}}.\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnp.random.seed(6020)\n# Parameters\nN = 1024\na, b = 0, 1/4\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nf1 = 50\nf2 = 120\nfun = lambda t: np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)\n\ngauss = lambda x, a: np.exp(-np.pow(x, 2) / a**2)\n\nf_clean = fun(t) + np.random.randn(len(t))  \ngauss = gauss(t - 0.125, 0.025) * 4.8\n\n# Figures\nplt.figure(0)\nplt.plot(t, f_clean, label=\"Signal\")\nplt.plot(t, gauss, label=r\"$g(t-\\tau)$\")\n#plt.xlabel(\"Time [s]\")\n#plt.ylabel(r\"$f$\")\nplt.xticks([0.1, 0.125, 0.15], [r\"$\\tau - a$\", r\"$\\tau$\", r\"$\\tau + a$\"])\nplt.xlim(t[0], t[-1])\nplt.ylim(-5, 5)\nplt.yticks([])\nplt.legend(loc=1)\nplt.gca().set_aspect(5e-3)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.9: Signal with Gaussian kernel\n\n\n\n\n\nFigure 8.9 illustrates the sliding time window with the spread as \\(a\\) and the center \\(\\tau\\). The inverse is given as \\[\nf(t) = \\mathcal{G}^{-1}\\{G_f(t, \\omega)\\} = \\frac{1}{2 \\pi \\|g\\|^2} = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty G_f(\\tau, \\omega) \\mathrm{e}^{\\mathrm{i}\\omega\\tau}\\overline{g}(t - \\tau)\\, \\mathrm{d}\\omega\\, \\mathrm{d}t.\n\\]\nAs with the Fourier transform the application of the Gabor transform is usually done in its discrete form.\n\n\n\n\n\n\n\nDefinition 8.8 (Discrete Gabor Transform) For discrete time \\(\\tau = k \\Delta t\\) and frequencies \\(\\nu = j \\Delta \\omega\\) the discretized kernel function has the form \\[\ng_{j,k} = \\mathrm{e}^{{\\mathrm{i}2 \\pi j\\Delta\\omega t}} g(t- k\\Delta t)\n\\] and the discrete Gabor transform \\[\n\\mathcal{G}\\{f\\}_{j,k} = \\langle f, g_{j,k} \\rangle = \\int_{-\\infty}^\\infty f(\\tau)\\overline{g}_{j,k}(\\tau)\\, \\mathrm{d}\\tau.\n\\]\n\n\n\n\nThe following example is taken from (Brunton and Kutz 2022, 77–78).\n\n\n\n\n\n\n\nExample 8.8 (Gabor transform for a quadratic chirp) As example we use an oscillating cosine function where the frequency of the oscillation increases as a quadratic function in time: \\[\nf(t) = \\cos(2\\pi\\, t\\, h(t)), \\quad \\text{with}\\quad h(t) = h_0 + (h_1 - h_0)\\frac{t}{3 t_1^2},\n\\] where the frequency shifts from \\(h_0\\) to \\(h_1\\) between \\(t=0\\) and \\(t=t_1\\).\nThe majority of the following code is from (Brunton and Kutz 2022, Code 2.9).\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 2048\na, b = 0, 2\nt = np.linspace(a, b, N, endpoint=False)\ndt = t[1] - t[0]\nh0 = 50\nh1 = 250\nt1 = 2\nx = np.cos(2 * np.pi * t * (h0 + (h1 - h0) * np.power(t,2) / (3 * t1**2)))\n\nplt.figure(0)\nN = len(x)\nfreq = (1 / (dt * N)) * np.arange(N)\nPSD_noise = np.abs(np.fft.fft(x))**2 / N\nplt.plot(freq[:N//2], PSD_noise[:N//2])\nplt.xlabel(\"Frequency [Hz]\")\nplt.gca().set_aspect(10)\n\nplt.figure(1)\nplt.specgram(x, NFFT=128, Fs=1/dt, noverlap=120, cmap='jet')\nplt.colorbar()\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Frequency [Hz]\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Power spectral density of the signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Spectrogram.\n\n\n\n\n\n\n\nFigure 8.10: Quadratic chirp signal\n\n\n\nWe can see a clear peak at 50Hz but no information of time is given, where else in the spectrogram we see how the frequency progresses in time.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the analysis of time-frequency we see Heisenberg’s uncertainty principle at work. We can not attain high resolution simultaneously in both time and frequency domain. In the time domain we have perfect resolution but no information about the frequencies, in the Fourier domain the time information is not present.\nThe spectrogram resolves both but with reduced resolution. This effect manifests differently at different locations of the chirp signal. At the left where the frequency is almost constant, the band becomes narrower, whereas at the right it becomes wider due to the increased blur in horizontal direction. The product of the uncertainties of these quantities has a minimal value. In this context time and frequency domain respectively represent the extremes.\n\n\nThe Fourier transform always assumes a harmonic nature in the signal it is applied to. Therefore, it is best suited for e.g. music, rotating machines or vibrations. For other signals, especially with discontinuous parts (Gibbs phenomenon) there are more adequate bases functions. We look at these in the next section Chapter 10 but first we look at the Laplace transform Chapter 9.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nCooley, James W., and John W. Tukey. 1965. “An Algorithm for the Machine Calculation of Complex Fourier Series.” Mathematics of Computation 19 (90): 297–301. https://doi.org/10.1090/s0025-5718-1965-0178586-1.\n\n\nGauß, Carl Friedrich. 1866. “Theoria Interpolationis Methodo Nova Tractata.” Göttingen: Königliche Gesellschaft der Wissenschaften.\n\n\nMeyberg, Kurt, and Peter Vachenauer. 1992. Höhere Mathematik 2. Springer-Lehrbuch. New York, NY: Springer.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fourier Transform</span>"
    ]
  },
  {
    "objectID": "signal/laplace.html",
    "href": "signal/laplace.html",
    "title": "9  Laplace Transform",
    "section": "",
    "text": "The Fourier Transform is only possible for well-behaved functions that are Lebesgue integrable, i.e. \\(f(x) \\ in L^1[(-\\infty, \\infty)]\\). This excludes functions like the exponential \\(\\mathrm{e}^{\\lambda t}\\) or the Heaviside function.\nIn order to tackle these badly behaved  functions we can use the Laplace transform, which is basically a one sided Fourier-like transform.\n\n\n\n\n\n\n\nDefinition 9.1 (The Laplace Transform) A function \\(f\\,:\\, [0, \\infty) \\to \\mathbb{R}\\) is called laplace transposable if \\[\nF(s) = \\mathcal{L}\\{f(t)\\} := \\int_{0}^{\\infty} f(t)\\mathrm{e}^{-s t}\\, \\mathrm{d}t\n\\] exists for all \\(s\\in\\mathrm{R}\\). In this case we call \\(F(s) \\equiv \\mathcal{L}\\{f(t)\\}\\) the Laplace transform of \\(f(t)\\).\nThe inverse Laplace transform is defined as \\[\nf(t) = \\mathcal{L}^{-1}\\{F(s)\\} = \\frac{1}{s \\pi \\mathrm{i}} \\int_{\\gamma - \\mathrm{i} \\infty}^{\\gamma + \\mathrm{i} \\infty} F(x)\\mathrm{e}^{s t}\\, \\mathrm{d}s\n\\] for the complex valued \\(s=\\gamma + \\mathrm{i} \\omega\\).\nIt is quite common to use the capitalized function name for the Laplace transform, as we did here.\n\n\n\n\nTo give a better understanding of theses formula we follow the derivation from (Brunton and Kutz 2022, sec. 2.5).\nLet us consider the function \\(f(t)=\\mathrm{e}^{\\lambda t}\\). As mentioned before we can not directly use the Fourier transform as the function is not bounded. In order to trap the function for \\(t\\to\\infty\\) we multiply by \\(\\mathrm{e}^{-\\gamma t}\\) where \\(\\gamma\\) is damping more that \\(f\\) grows. In order to handle \\(t\\to-\\infty\\) we multiply by the Heaviside function \\[\nH(t) =\n\\begin{cases}\n0, &t \\leq 0,\\\\\n1, &t &gt; 0,\n\\end{cases}\n\\] and transform it into a one-sided function as in definition Definition 9.1 required. We end up with the function \\[\n\\underline{f}(t) = f(t) \\mathrm{e}^{-\\gamma t}H(t) =\n\\begin{cases}\n0, &t \\leq 0,\\\\\nf(t)\\mathrm{e}^{-\\gamma t}, &t &gt; 0.\n\\end{cases}\n\\] Taking the Fourier transform we get \\[\n\\begin{aligned}\n\\hat{\\underline{f}}(\\omega) = \\mathcal{F}\\{\\underline{f}(t)\\} &=\n\\int_{-\\infty}^\\infty \\underline{f}(t)\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t =\n\\int_{0}^\\infty f(t)\\mathrm{e}^{-\\gamma t}\\mathrm{e}^{-\\mathrm{i}\\omega t}\\, \\mathrm{d}t = \\\\\n&= \\int_{0}^\\infty f(t)\\mathrm{e}^{-(\\gamma + \\omega) t}\\, \\mathrm{d}t =\n\\int_{0}^\\infty f(t)\\mathrm{e}^{-s t}\\, \\mathrm{d}t = \\\\\n&= \\mathcal{L}\\{f(t)\\} = F(s)\n\\end{aligned}\n\\] giving rise to the Laplace transform.\nTo get the inverse we start with the inverse Fourier transform of \\(\\hat{\\underline{f}}(\\omega)\\) \\[\n\\underline{f}(t) = \\mathcal{F}^{-1}\\{\\hat{\\underline{f}}(\\omega)\\} = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\hat{\\underline{f}}(\\omega) \\mathrm{e}^{\\mathrm{i}\\omega t}\\, \\mathrm{d}\\omega.\n\\] Multiply both sides with \\(\\mathrm{e}^{\\gamma t}\\) we get \\[\n\\begin{aligned}\nf(t)H(t) = \\underline{f}(t)\\mathrm{e}^{\\gamma t} &= \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\mathrm{e}^{\\gamma t} \\hat{\\underline{f}}(\\omega) \\mathrm{e}^{\\mathrm{i}\\omega t}\\, \\mathrm{d}\\omega = \\\\\n&=\n\\frac{1}{2\\pi} \\int_{-\\infty}^\\infty \\hat{\\underline{f}}(\\omega) \\mathrm{e}^{(\\gamma + \\mathrm{i}\\omega) t}\\, \\mathrm{d}\\omega\n\\end{aligned}\n\\] By a change of variables \\(s=\\gamma + \\mathrm{i}\\omega\\) we get \\(\\mathrm{d}\\omega = \\tfrac{1}{\\mathrm{i}} \\mathrm{d}s\\) and \\[\nf(t)H(t) = \\frac{1}{s \\pi \\mathrm{i}} \\int_{\\gamma - \\mathrm{i} \\infty}^{\\gamma + \\mathrm{i} \\infty} F(x)\\mathrm{e}^{s t}\\, \\mathrm{d}s = \\mathcal{L}^{-1}\\{F(s)\\}.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nForm this derivation we see that the Laplace transform is a generalized Fourier transform for a broader spectrum of functions.\nSometimes the Laplace transform is simpler than the Fourier transform. In particular this is the case for the Dirac delta function that has infinitely many frequencies in Fourier domain but is constant \\(1\\) in Laplace domain. We will use this to compute the impulse response of systems with forcing.\n\n\nA lot of the properties we saw for the Fourier transform carry over to the Laplace transform.\n\nLinearity \\[\n\\mathcal{L}\\{\\alpha f(t) + \\beta g(t)\\} =\n\\alpha \\mathcal{L}\\{f(t)\\} + \\beta \\mathcal{L}\\{g(t)\\} =\n\\alpha F(s)+ \\beta G(s).\n\\]\nConjugation \\[\n\\mathcal{L}\\{\\overline{f(t)}\\} = \\overline{\\hat{f}(\\overline{s})}.\n\\]\nScaling, for \\(\\alpha \\geq 0\\) \\[\n\\mathcal{L}\\{f(\\alpha t)\\} = \\frac{1}{\\alpha}F\\left(\\frac{s}{\\alpha}\\right).\n\\]\nDrift in time, for \\(a\\in\\mathbb{R}\\) \\[\n\\mathcal{L}\\{f(t - a)H(t - a)\\} = \\mathrm{e}^{-a s}F(s)\n\\] and \\[\n\\mathcal{L}\\{f(t)H(t - a)\\} = \\mathrm{e}^{-a s}\\mathcal{L}\\{f(t+a)\\}.\n\\]\nDrift in frequency, for \\(a\\in\\mathbb{R}\\) \\[\n\\mathrm{e}^{a t} \\mathcal{L}\\{f(t)\\} = F(s - a).\n\\]\nDerivative in time \\[\n\\mathcal{L}\\{\\partial_t f(t)\\} = s F(s) - f(0)\n\\] We are going to prove this by going through the lines \\[\n\\begin{aligned}\n\\mathcal{L}\\left\\{\\frac{d}{d\\,t}f(t)\\right\\} &= \\int_{0}^\\infty f'(t)\\mathrm{e}^{-s t}\\, \\mathrm{d}t \\\\\n&= \\left[f(t)\\mathrm{e}^{-s t}\\right]_{0}^\\infty - \\int_{0}^\\infty f(t) (- s\\mathrm{e}^{-s t})\\, \\mathrm{d}t \\\\\n&= -f(0) + s \\mathcal{L}\\{f(t)\\}\n\\end{aligned}\n\\] For higher derivatives we get \\[\n\\mathcal{L}\\{\\partial_t^n f(t)\\} = s^n F(s) - \\sum_{k=1}^n s^{n-k} f^{(k-1)}(0).\n\\]\nIntegral in time \\[\n\\mathcal{L}\\left\\{\\int_0^t f(\\tau)\\, \\mathrm{d}\\tau\\right\\} = \\frac{1}{s} F(s).\n\\]\nDerivative in frequency \\[\n\\mathcal{L}\\{t^n f(t)\\} = (-1)^n \\partial_\\omega^n F(s)\n\\]\nIntegral in frequency \\[\n\\mathcal{L}\\left\\{\\frac{1}{t} f(t)\\right\\} = \\int_s^\\infty F(u)\\, \\mathrm{d}u.\n\\]\nThe convolution of two functions is defined as \\[\n(f \\ast g)(t) = \\int_{0}^{\\infty}f(\\tau) g(t - \\tau)\\, \\mathrm{d}\\tau,\n\\] and for the Laplace transform \\[\n\\mathcal{L}\\{(f \\ast g)(t)\\} = F(s) \\cdot G(s).\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nThe following examples are mostly from Meyberg and Vachenauer (1992).\n\n\n\n\n\n\n\n\n\nExample 9.1 (Laplace transform of some basic functions)  \n\nFor the constant \\(f(t) = 1\\) function we get \\[\n\\mathcal{L}\\{f(t)\\} = \\frac{1}{s},\\quad s&gt;0.\n\\]\nFor the exponential \\(f(t) = \\mathrm{e}^{\\lambda t}\\) we get \\[\n\\mathcal{L}\\{f(t)\\} = \\frac{1}{s- \\lambda},\\quad s&gt;\\lambda\\in\\mathcal{R}.\n\\]\nFor the cosine \\(f(t) = \\cos{\\omega t}\\) we get \\[\n\\mathcal{L}\\{f(t)\\} = \\frac{s}{s^2 + \\omega^2},\\quad s&gt;0.\n\\]\nFor the sine \\(f(t) = \\sin{\\omega t}\\) we get \\[\n\\mathcal{L}\\{f(t)\\} = \\frac{\\omega}{s^2 + \\omega^2},\\quad s&gt;0.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nExample 9.2 (Laplace transform of a step function) For a rectangle with width \\(a\\geq 0\\) and height \\(1\\) we get \\[\nf(t) = H(t) - H(t-a)\n\\] we get \\[\n\\mathcal{L}\\{f(t)\\} = \\frac{1 -  \\mathrm{e}^{-a s}}{s}\n\\]\nWe can extend this to a step function \\[\ng(t) = \\sum_{n=0}^\\infty H(t-na)\n\\] and we get \\[\n\\mathcal{L}\\{g(t)\\} = \\frac{1}{s (1- \\mathrm{e}^{-a s})}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 9.3 (Cam drive) A cam drive has an elastic and lightly damped punch \\(S\\) with eigenfrequency \\(\\omega = 2\\).\n\n\n\n\n\n\nFigure 9.1: Cam drive schematics\n\n\n\nWe excite the system with a half-wave sine of half the eigenfrequency. \\[\n\\begin{aligned}\nx_A(t) &= \\frac{1}{2}(\\sin t + |\\sin t|) \\\\\n&=\n\\begin{cases}\n\\sin(t), & 2 n \\pi \\leq t \\le (2n+1)\\pi,\\\\\n0 , & (2n+1)\\pi \\leq t \\le (2n + 2)\\pi,\n\\end{cases}\n\\quad \\text{for}\\quad n = 0, 1, 2, \\ldots \\\\\n&= \\sum_{n=0}^\\infty H(t - n \\pi) \\sin(t-n\\pi).\n\\end{aligned}\n\\]\nFor the excitement of \\(S\\) the initial value problem \\[\n\\ddot{x} + 4 x = x_A(t), \\quad x(0) = \\dot{x}(0) = 0,\n\\] is given.\nVia the Laplace transform we can solve for \\(X(s)\\) in \\[\ns^2 X(s) + 4 X(s) = \\sum_{n=0}^\\infty \\frac{1}{1+s^2} \\mathrm{e}^{-n \\pi s}\n\\] and obtain \\[\nX(s) = \\frac{1}{(1+s^2)(s^2 + 4)} \\sum_{n=0}^\\infty \\mathrm{e}^{-n \\pi s}.\n\\]\nVia the inverse Laplace transform we can get back to \\(x(t)\\) \\[\nx(t) = \\frac{1}{6}(\\sin(t) + |\\sin(t)|)- \\frac{\\sin(2t)}{6}\\sum_0^\\infty H(t-n\\pi)\n\\] and without the heavy side function \\[\nx(t) = \\begin{cases}\n\\frac{1}{3} \\sin (t) - \\frac{2n+1}{6} \\sin(2t), & 2 n \\pi \\leq t \\le (2n+1)\\pi, \\\\\n- \\frac{n+1}{3} \\sin(2t), &(2n+1)\\pi \\leq t \\le (2n + 2)\\pi.\n\\end{cases}\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nheavy_side = lambda t: np.ones_like(t) * (t&gt;=0)\ng = lambda n, t: (np.heaviside(t - 2 * n * np.pi, 1) - \\\n                  np.heaviside(t - (2 * n + 1) * np.pi, 1))\nh = lambda n, t: (np.heaviside(t - (2 * n + 1) * np.pi, 1) - \\\n                  np.heaviside(t - (2 * n + 2) * np.pi, 1))\n\nxa = lambda t: 1 / 2 * (np.sin(t) + np.abs(np.sin(t)))\n\nfun1 = lambda n, t: (1 / 3 * np.sin(t) - (2 * n + 1) / 6 * \\\n                     np.sin(2 * t)) * g(n, t)\nfun2 = lambda n, t:-1 * (n + 1) / 3 * np.sin(2 * t) * h(n, t)\nfunn = lambda n, t: fun1(n, t) + fun2(n, t) \nfun = lambda t: funn(0, t) + funn(1, t) + funn(2, t) + funn(3, t)\n\nt = np.linspace(0, 8 * np.pi, 1024)\ni = np.argmax(funn(0, t))\nj = np.argmax(funn(3, t))\nk = (funn(3, t[j]) - funn(0, t[i])) / (t[j] - t[i])\nline = lambda t: k * (t - t[i]) + funn(0, t[i])\n\nplt.plot(t, xa(t), \"--\", label=r\"$x_A$\")\nplt.plot(t, fun(t), label=r\"$x$\")\nplt.plot(t, line(t), \"k:\")\nplt.xlabel(r\"t\")\nplt.ylabel(r\"x\")\nplt.legend()\nplt.gca().set_aspect(8*np.pi / (3 * 3.0))\nplt.ylim([-1.5, 1.5])\n\n\n\n\n\n\n\n\nFigure 9.2: Exciting function and response, we indicate the growth rate as a dotted line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 9.4 (Electric Circuit) For the electric components \\(R\\) (resistor), \\(C\\) capacitor, and \\(L\\) coil we can use the Laplace transform to show how voltage \\(u(t)\\) and current \\(i(t)\\) is transformed.\nWe need to assume that \\(i(0)=0\\) than we get\n\n\n\nTable 9.1: Laplace transform for common electrical components\n\n\n\n\n\n\n\n\n\n\ncomponent\n\\(u(t)\\) and \\(i(t)\\)\nLaplace transform\n\n\n\n\nR\n\\(u_R(t) = R i(t)\\)\n\\(U_r(s) = R I(s)\\)\n\n\nC\n\\(u_C(t) = \\frac{1}{x} \\int_0^t i(\\tau)\\, \\mathrm{d}\\tau\\)\n\\(U_C(s) = \\frac{1}{sC}I(s)\\)\n\n\nL\n\\(u_L(t) = L \\frac{\\mathrm{d} i}{\\mathrm{d} t}\\)\n\\(U_L(s) = sL I(s)\\)\n\n\n\n\n\n\nAn Ohm’s law applies for \\[\nU(s) = Z(s) I(s), \\quad Z(s) \\in \\{R, \\frac{1}{Cs}, Ls\\}\n\\]\nFor a RCL electric circuit\n\n\n\n\n\n\nFigure 9.3: RCL oscillating circuit\n\n\n\nwe have \\[\nu(t) = u_R(t) + u_c(t) + u_L(t)\n\\] and we get for the Laplace transformed current \\(I(s)\\) \\[\nI(s) = \\frac{U(s)}{R+\\frac{1}{sC}+ sL} = \\frac{s C U(s)}{s^2 LC + s RC + 1} = Z(s) U(s)\n\\] and \\[\ni(t) = \\mathcal{L}^{-1}\\{z(t)\\} \\ast u(t).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 9.5 (The Linear Time Invariant transfer function of LTI Systems) In signal and and control theory the transfer function \\(H(s)\\) is used the describe a system. If we assume that at \\(t=0\\) we have an initial state (zero state) we get the relation between input signal \\(x(t)\\) and output signal \\(y(t)\\) in the Laplace space as \\[\nY(s) = H(s) Y(s)\n\\] and consequently \\[\ny(t) = h(t) \\ast x(t)\n\\] where \\(h(t)= \\mathcal{L}^{-1}\\{H(s)\\}\\) is called the impulse response.\n\n\n\n\n\n\nFigure 9.4: Linear Time invariant System\n\n\n\nThe relation between \\(x\\) and \\(y\\) can often be described via a linear differential equation with constant coefficients. In these cases \\[\n\\alpha_n y^{(n)} + \\alpha_{n-1} y^{(n-1)} + \\cdots + \\alpha_1 \\dot{y} + \\alpha_0 y = x(t)\n\\] and for \\(y(0) = \\dot{y}(0) = \\cdots = y^{(n)}(0) = 0\\) we get \\[\nH(s) = \\frac{1}{\\alpha_n s^n + \\alpha_{n-1}s^{n-1} + \\cdots \\alpha_1 s + \\alpha_0}.\n\\]\nWe can rewrite this as \\[\n\\alpha_n(s H(s) - \\frac{1}{\\alpha_n}) + \\alpha_{n-1}s^{n-1} H(s) + \\cdots + \\alpha_1 s H(s) + \\alpha_0 H(s) = 0\n\\] and with the properties of the Laplace transform we get \\[\n\\alpha_n h^{(n)} + \\alpha_{n-1} h^{(n-1)} + \\cdots + \\alpha_1 \\dot{h} + \\alpha_0 h = 0\n\\] with \\(h(0) = \\dot{h}(0) = \\cdots = h^{(n-2)}(0) = 0\\), and \\(h^{(n-1)}(0) \\tfrac{1}{\\alpha_n}\\).\nThis tells us the answer to the zero input signal \\(x(t) = 0\\) is the solution of above initial value problem with initial state as an impulse of size \\(\\tfrac{1}{\\alpha_n}\\).\nIn terms of the Dirac-delta function \\(\\delta(t)\\) the function \\(h(t)\\) is the answer to the initial value problem \\[\n\\alpha_n h^{(n-1)} + \\alpha_{n-1} h^{(n-1)} + \\cdots + \\alpha_1 \\dot{h} + \\alpha_0 h = \\delta(t).\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 9.6 (The RC low pass filter) The RC low pass filter\n\n\n\n\n\n\n\n\n\n(a) First order RC low pass\n\n\n\n\n\n\n\n\n\n(b) Second order RC low pass\n\n\n\n\n\nFigure 9.5: The electric circuit for the low pass filters.\n\n\n\nhas the transfer function \\[\nH(s) = \\frac{1}{sRC + 1}\n\\] and the impulse response \\[\nh(t) = \\frac{1}{RC}\\mathrm{e}^{-\\frac{t}{RC}}.\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nRC = 1 \nfun = lambda t: 1 / RC * np.exp(-t / RC) * np.heaviside(t, 1)\nt = np.linspace(-0.5, 8.5, 1024)\n\nplt.plot(t, fun(t))\nplt.text(-0.4, 0.96, r\"$\\frac{1}{RC}$\")\nplt.xlabel(r\"t\")\nplt.ylabel(r\"y\")\nplt.gca().set_aspect(9 / (3 * 1.0))\n\n\n\n\n\n\n\n\nFigure 9.6: Impulse response for the first order RC low pass filter\n\n\n\n\n\nIf we extend this to a second order low pass filter Figure 9.5 (b) we get \\[\nH(s) = \\frac{1}{(sRC + 1)^2}\n\\] and the impulse response \\[\nh(t) = \\frac{1}{(RC)^2}t\\mathrm{e}^{-\\frac{t}{RC}}.\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nRC = 1 \nfun = lambda t: 1 / RC**2 * t * np.exp(-t / RC) * np.heaviside(t, 1)\nt = np.linspace(-0.5, 8.5, 1024)\n\nplt.plot(t, fun(t))\nplt.text(-0.5, 0.95/(np.exp(1)), r\"$\\frac{1}{RC \\mathrm{e}}$\")\nplt.text(0.8, -0.05, r\"$RC$\")\nplt.plot([-0.1, 1, 1], [1/np.exp(1), 1/np.exp(1), 0], \"k:\")\nplt.xlabel(r\"t\")\nplt.ylabel(r\"y\")\nplt.gca().set_aspect(9 / (3 * 0.37))\n\n\n\n\n\n\n\n\nFigure 9.7: Impulse response for the first order RC low pass filter\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nMeyberg, Kurt, and Peter Vachenauer. 1992. Höhere Mathematik 2. Springer-Lehrbuch. New York, NY: Springer.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Laplace Transform</span>"
    ]
  },
  {
    "objectID": "signal/wavelet.html",
    "href": "signal/wavelet.html",
    "title": "10  Wavelet transform",
    "section": "",
    "text": "With Wavelets we extend the concept of the Fourier analysis to general orthogonal bases. This extensions is done in such a way that we can do a multi-resolution decomposition and thus partially overcome the uncertainty principal discussed before.\nWavelets are both, local and orthogonal. The whole family of a wavelet are generated by scaling and translating a mother wavelet \\(\\psi(t)\\) as \\[\n\\psi_{a,b}(t) = \\frac{1}{\\sqrt{a}} \\psi\\left(\\frac{t-b}{a}\\right),\n\\] where the parameters \\(a\\) and \\(b\\) are responsible for scaling and translating, respectively.\nThe simplest example is the so called Haar wavelet.\n\n\n\n\n\n\n\nExample 10.1 (The Haar wavelet) The mother wavelet is defined as the step function \\[\n\\psi(t) =\n\\begin{cases}\n\\begin{array}{rl}\n1, & \\text{for}\\, 0 \\leq t \\le \\tfrac12\\\\\n-1, & \\text{for}\\, \\tfrac12 \\leq t \\le 1\\\\\n0, & \\text{else}\n\\end{array}\n\\end{cases}\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 1000\na, b = -0.25, 1.25\nt = np.linspace(a, b, N, endpoint=False)\nmother = lambda t: np.heaviside(t, 1) - np.heaviside(t - 1/2, 1) - \\\n                   (np.heaviside(t - 1/2, 1) - np.heaviside(t - 1, 1))\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a)\n\nplt.figure(0)\nplt.plot(t, psi(t, 1, 0))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{1,0}$\")\nplt.gca().set_aspect(0.125)\n\nplt.figure(1)\nplt.plot(t, psi(t, 1/2, 0))\nplt.xlabel(\"x\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, 0}$\")\nplt.gca().set_aspect(0.125)\n\nplt.figure(2)\nplt.plot(t, psi(t, 1/2, 1/2))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, \\frac{1}{2}}$\")\nplt.gca().set_aspect(0.125)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scaling 1 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaling 1/2 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scaling 1/2 and translation 1/2.\n\n\n\n\n\n\n\nFigure 10.1: Haar wavelets for the first two levels of multi resolution.\n\n\n\nNote that the Haar wavelets are orthogonal and provide a hierarchical basis for a signal.\n\n\n\n\n\n\n\n\n\n\n\nExample 10.2 (The Maxican hat wavelet) The mother wavelet is defined as the is the negative normalized second derivative of a Gaussian function, \\[\n\\psi(t) = (1 - t)^2\\, \\mathrm{e}^{-\\tfrac{t}{2}}\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\n%config InlineBackend.figure_formats = [\"svg\"]\n\nN = 1000\na, b = -5, 5\nt = np.linspace(a, b, N, endpoint=False)\nmother = lambda t, d: (1 - np.pow(t, 2)) * np.exp(-1/2 * np.pow(t, 2))\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a, 2)\n\nplt.figure(0)\nplt.plot(t, psi(t, 1, 0))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{1,0}$\")\nplt.gca().set_aspect(1.5)\n\nplt.figure(1)\nplt.plot(t, psi(t, 1/2, 0))\nplt.xlabel(\"x\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, 0}$\")\nplt.gca().set_aspect(1.5)\n\nplt.figure(2)\nplt.plot(t, psi(t, 1/2, 1/2))\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\psi_{\\frac{1}{2}, \\frac{1}{2}}$\")\nplt.gca().set_aspect(1.5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scaling 1 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scaling 1/2 and translation 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scaling 1/2 and translation 1/2.\n\n\n\n\n\n\n\nFigure 10.2: Mexican hat wavelets for the first two levels of multi resolution.\n\n\n\n\n\n\n\nIf we have a wavelet \\(\\psi\\), we can generate a new wavelet by convolution \\[\n\\psi \\ast \\phi\n\\], for a bounded and integrable function \\(\\phi\\).\n\n\n\n\n\n\n\nDefinition 10.1 (Continuous Wavelet Transform) The continuous wavelet transform is given by \\[\n\\mathcal{W}_\\psi\\{f\\}(a, b) = \\langle f, \\psi_{a, b} \\rangle = \\int_{-\\infty}^\\infty f(t)\\overline{\\psi}_{a,b}\\, \\mathrm{d}t,\n\\] this is only true for a bounded wavelet \\(\\psi\\) \\[\nC_\\psi = \\int_{-\\infty}^\\infty \\frac{|\\hat{\\psi}(\\tau)|^2}{|\\tau|}\\, \\mathrm{d}\\tau,\n\\] i.e. a wavelet with \\(C_\\psi &lt; \\infty\\). In this case also the inverse transform exists and is defined as \\[\nf(t) = \\frac{1}{C_\\psi} \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty\\mathcal{W}_\\psi\\{f\\}(a, b)\\psi_{a, b}(t)\\frac{1}{a^2} \\, \\mathrm{d}a\\, \\mathrm{d}b,\n\\]\n\n\n\n\nFrom the continuous wavelet transform we go on to the discrete wavelet transform as similar for the Fourier transforms we have seen we will hardly ever have the entire signal at hand for the transformation.\n\n\n\n\n\n\n\nDefinition 10.2 (Discrete Wavelet Transform) The discrete wavelet transform is given by \\[\n\\mathcal{W}_\\psi\\{f\\}(j, k) = \\langle f, \\psi_{j, k} \\rangle = \\int_{-\\infty}^\\infty f(t)\\overline{\\psi}_{j, k}\\, \\mathrm{d}t,\n\\] where \\(\\psi_{j, k}\\) is a discrete family of wavelets \\[\n\\psi_{j, k}(t) = \\frac{1}{a^j}\\psi\\left(\\frac{t - k b}{a^j}\\right).\n\\] The inverse is than given by \\[\nf(t) = \\sum_{j,k = -\\infty}^\\infty \\mathcal{W}_\\psi\\{f\\}(j, k) \\psi_{j, k}(t).\n\\] Which is nothing else than expressing the function in the wavelet family. If this family of wavelets is orthogonal (as e.g. the Haar wavelet) it is possible to expand the function \\(f\\) uniquely as they form a basis.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere also exists a fast wavelet transform that reduces the computational complexity from \\(\\mathcal{O}(N\\log N)\\) to \\(\\mathcal{O}(N)\\) by cleverly reusing parts of the inner product computation.\n\n\n\n\n\n\n\n\n\nExample 10.3 (Signal analysis with the Haar wavelet) To start and get an idea how the analysis works we use an instructive example. We have a piecewise constant function as \\(v = [3, 1, 0, 4, 0, 6, 9, 9]^{\\mathsf{T}}\\) \\[\nf(x) = \\sum_{i=1}^{8} v_i \\chi_{[i-1, i)},\n\\] where \\(\\chi_{[c, d)}\\) is the indicator function that is 1 on the interval \\([c, d)\\) and zero everywhere else.\nNow let us proceed through the levels of the transform to see what is happening, we select \\(a=2\\) and \\(b=1\\):\n\\[\n\\mathcal{W}\\{f\\}(1, :) =\n\\left(\n                        \\begin{array}{cccccccc}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0\\\\\n                            0&0&0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}\\\\\n                            \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0\\\\\n                            0&0&0&0&0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}\\\\\n                        \\end{array}\n                    \\right)\n                    \\left[\n                        \\begin{array}{c}\n                            3\\\\\n                            1\\\\\n                            0\\\\\n                            4\\\\\n                            0\\\\\n                            6\\\\\n                            9\\\\\n                            9\n                        \\end{array}\n                    \\right]=\\left[\n                            \\begin{array}{c}\n                                2\\sqrt{2}\\\\\n                                2\\sqrt{2}\\\\\n                                3\\sqrt{2}\\\\\n                                9\\sqrt{2}\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\n                            \\end{array}\n                        \\right]\n\\]\nWe than apply the next level just for the first half \\[\n\\mathcal{W}\\{f\\}(2, :) =\n\\left(\n                        \\begin{array}{cccccccc}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0\\\\\n                            \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                            0&0&\\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0\\\\\n                            0&0&0&0&1&0&0&0\\\\\n                            0&0&0&0&0&1&0&0\\\\\n                            0&0&0&0&0&0&1&0\\\\\n                            0&0&0&0&0&0&0&1\n                        \\end{array}\n                    \\right)\n                    \\left[\n                            \\begin{array}{c}\n                                2\\sqrt{2}\\\\\n                                2\\sqrt{2}\\\\\n                                3\\sqrt{2}\\\\\n                                9\\sqrt{2}\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\n                            \\end{array}\n                        \\right]=\\left[\n                            \\begin{array}{c}\n                                4\\\\\n                                12\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\n                            \\end{array}\n                        \\right]\n\\] and our final step \\[\n\\mathcal{W}\\{f\\}(3, :) =\n\\left(\n                        \\begin{array}{cccccccc}\n                            \\frac{1}{\\sqrt{2}}&\\frac{1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                                \\frac{1}{\\sqrt{2}}&\\frac{-1}{\\sqrt{2}}&0&0&0&0&0&0\\\\\n                                0&0&1&0&0&0&0&0\\\\\n                                0&0&0&1&0&0&0&0\\\\\n                                0&0&0&0&1&0&0&0\\\\\n                                0&0&0&0&0&1&0&0\\\\\n                                0&0&0&0&0&0&1&0\\\\\n                                0&0&0&0&0&0&0&1\n                        \\end{array}\n                    \\right)\n                    \\left[\n                            \\begin{array}{c}\n                                4\\\\\n                                12\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\n                            \\end{array}\n                        \\right]=\\left[\n                            \\begin{array}{c}\n                                8\\sqrt{2}\\\\\n                                -4\\sqrt{2}\\\\\n                                0\\\\\n                                -6\\\\\n                                \\sqrt{2}\\\\\n                                -2\\sqrt{2}\\\\\n                                -3\\sqrt{2}\\\\\n                                 0\n                            \\end{array}\n                        \\right]\n\\]\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pywt\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\nN = 1000\na, b = 0, 8\nt = np.linspace(a, b, N, endpoint=False)\nv = np.array([3, 1, 0, 4, 0, 6, 9, 9])\nchi = lambda t, a, b: np.heaviside(t - a, 1) - np.heaviside(t - b, 1)\nmother = lambda t: chi(t, 0, 1/2) - chi(1/2, 1)\npsi = lambda t, a, b: 1 / np.sqrt(a) * mother((t - b) / a, 2)\n\ndef f(t, v, spread=1):\n    y = np.zeros(t.shape)\n    for i, x in enumerate(v):\n        y += x * chi(t, i * spread, (i + 1) * spread)\n    return y\n\nX = np.zeros((4, len(v)))\nfor i in range(0, 4):\n    x = pywt.wavedec(v, wavelet=\"Haar\", level=i)\n    X[i, :] = np.concatenate(x) \n\nplt.figure(0)\nplt.plot(t, f(t, v), label=\"Signal\")\n#plt.plot(t, f(t, X[0, :], 1))\nplt.plot(t, f(t, X[1, 0:5], 2), label=\"Level 1\")\nplt.plot(t, f(t, X[2, 0:3], 4), label=\"Level 2\")\nplt.plot(t, f(t, X[3, 0:1], 8), label=\"Level 3\")\nplt.xlabel(\"t\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10.3: Haar wavelets for the first three levels of multi resolution analysis.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Wavelet transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html",
    "href": "signal/twodim.html",
    "title": "11  Two-Dimensional Transform",
    "section": "",
    "text": "11.1 Fourier\nThere is nothing preventing us from extending the transforms we discussed before to 2D. One of the most common applications are image processing.\nIf we apply FFT to a matrix \\(X\\in\\mathrm{R}^{m\\times n}\\) we can simply apply the 1D version to every row and than to every column of the resulting matrix. The other way round will produce the same final result.\nThis is shown in the code below but note we should use the more efficient np.fft.fft2.\nOf course we can use this to compress the image by removing small values from the transform.\nWe can also use the FFT for de-noising and filtering of signals. It is rather simple to just eliminate certain frequency bands in the frequency domain.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html#fourier",
    "href": "signal/twodim.html#fourier",
    "title": "11  Two-Dimensional Transform",
    "section": "",
    "text": "Show the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\n\nCshift = np.zeros_like(A, dtype='complex')\nC = np.zeros_like(A, dtype='complex')\nfor j in range(A.shape[0]):\n    C[j, :] = np.fft.fft(A[j, :])\n    Cshift[j,:] = np.fft.fftshift(np.copy(C[j, :]))\n\nRshift = np.zeros_like(A, dtype='complex')\nR = np.zeros_like(A, dtype='complex')\nD = np.zeros_like(C)\nfor j in range(A.shape[1]):\n    R[:, j] = np.fft.fft(A[:, j])\n    Rshift[:, j] = np.fft.fftshift(np.copy(R[:, j]))\n    D[:, j] = np.fft.fft(C[:, j])\n\nmyplot(A)\nmyplot(np.log(np.abs(Cshift)))\nmyplot(np.log(np.abs(Rshift)))\nmyplot(np.fft.fftshift(np.log(np.abs(D))))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Image after applying FFT on each individual row\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Image after applying FFT on each individual column\n\n\n\n\n\n\n\n\n\n\n\n(d) Row and column wise FFT (order does not matter)\n\n\n\n\n\n\n\nFigure 11.1: Image of MCI I and the row/column wise FFT.\n\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_fft = np.fft.fft2(A)\nA_fft_sort = np.sort(np.abs(A_fft.reshape(-1)))\nmyplot(A)\n\nfor c in (0.05, 0.01, 0.002):\n    thresh = A_fft_sort[int(np.floor((1 - c) * len(A_fft_sort)))]\n    A_fft_th = A_fft * (np.abs(A_fft) &gt; thresh)\n    A_th = np.fft.ifft2(A_fft_th).real\n    myplot(A_th)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) 5% of FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 1% of FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n(d) 0.2% of FFT coefficients\n\n\n\n\n\n\n\nFigure 11.2: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.\n\n\n\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\nnp.random.seed(6020)\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_noise = A + (200 * np.random.randn(*A.shape)).astype('uint8')\nA_noise_fft = np.fft.fft2(A_noise)\nA_noise_fft_shift = np.fft.fftshift(A_noise_fft)\nF = np.log(np.abs(A_noise_fft_shift) + 1)\n\nmyplot(A_noise)\nmyplot(F)\n\nnx, ny = A.shape\nX, Y = np.meshgrid(np.arange(-ny/2 + 1, ny / 2 + 1),\n                   np.arange(-nx / 2 + 1, nx / 2 + 1))\nR2 = np.power(X, 2) + np.power(Y, 2)\nind = R2 &lt; 150**2\nA_noise_fft_shift_filter = A_noise_fft_shift * ind\nF_filter = np.log(np.abs(A_noise_fft_shift_filter) + 1)\n\nA_filter = np.fft.ifft2(np.fft.fftshift(A_noise_fft_shift_filter)).real\nmyplot(A_filter)\nmyplot(F_filter)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Noisy image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Noisy FFT coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Filtered/De-noised image\n\n\n\n\n\n\n\n\n\n\n\n(d) Filtered/De-noised FFT coefficients\n\n\n\n\n\n\n\nFigure 11.3: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "signal/twodim.html#wavelet",
    "href": "signal/twodim.html#wavelet",
    "title": "11  Two-Dimensional Transform",
    "section": "11.2 Wavelet",
    "text": "11.2 Wavelet\nSimilar to the FFT also the Wavelet transform is used in much the same situations.\nBefore we go on and apply the wavelet transform in the same situations we show how the multi level approach looks like for an image. For the image we use the Daubechies 1 wavelets.\n\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport pywt\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nn = 2\nA = rgb2gray(im)\ncoeffs = pywt.wavedec2(A, wavelet=\"db1\", level=n)\n\ncoeffs[0] /= np.abs(coeffs[0]).max()\narr, coeff_slices = pywt.coeffs_to_array(coeffs)\n\nplt.imshow(arr, cmap='gray', vmin=-0.25, vmax=0.75)\nplt.axis(\"off\")\nplt.gca().set_aspect(1)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11.4: First three levels of the discrete wavelet transform.\n\n\n\n\n\nOf course we can use this to compress the image by removing small values from the transform.\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\nimport pywt\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nw = \"db1\"\nA = rgb2gray(im)\ncoeffs = pywt.wavedec2(A, wavelet=w, level=4)\n\ncoeff_arr, coeff_slices = pywt.coeffs_to_array(coeffs)\nCsort = np.sort(np.abs(coeff_arr.reshape(-1)))\nmyplot(A)\nfor c in (0.05, 0.01, 0.002):\n    thresh = Csort[int(np.floor((1 - c) * len(Csort)))]\n    Cfilt = coeff_arr * (np.abs(coeff_arr) &gt; thresh)\n\n    coeffs_filt = pywt.array_to_coeffs(Cfilt, coeff_slices, output_format='wavedec2')\n    A_r = pywt.waverec2(coeffs_filt, wavelet=w)\n    myplot(A_r.astype('uint8'))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) 5% of wavelets\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 1% of wavelets\n\n\n\n\n\n\n\n\n\n\n\n(d) 0.2% of wavelets\n\n\n\n\n\n\n\nFigure 11.5: Image of MCI I and the reconstruction with various amounts of wavelets.\n\n\n\nFor ne-noising filters are applied or we and this is going to be the subject for a lecture more focused on image processing.",
    "crumbs": [
      "Signal Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Two-Dimensional Transform</span>"
    ]
  },
  {
    "objectID": "sensing/index.html",
    "href": "sensing/index.html",
    "title": "Sparsity and compression",
    "section": "",
    "text": "We have seen in the sections before that our signals and data can often be expressed in an optimal way by changing basis. Moreover, this often results in sparse data and therefore gives us the opportunity for compression. By expressing our data in this basis most coefficients are zero or small.\nThe zero coefficients give rise to sparsity and the almost zero coefficients allow us to compress the data further without loosing too much information. We have seen this in the Eigendecomposition, the singular value decomposition, within regression choices, the Fourier transform, the Wavelet transform, and in other such transformations not covered here.\nRecent development in mathematics have given rise to the field of compressed sensing, where not high dimensional signals are collected and transformed or compressed but we start by a acquiring compressed signals and solve for the sparsest high-dimensional signal that is consistent with the collected data.\nHere we will discuss sparsity and compression and give an outlook on compressed sensing. We have already seen multiple examples and we will use this chapter to contextualize these results and combine them to give rise to new ideas and further aspects.\nIt is worth mentioning that quite often sparsity gives rise to so called parsimonious models that avoid overfitting and remain interpretable because they have a low number of terms. This fits neatly into the discussion of Occam’s razor: the simplest explanation is in general the correct one. Simple can also mean the least coefficients and this means sparsity. Furthermore, it can also help to create more robust algorithms are outliers have less influence.\n\n\n\n\n\n\nImportant\n\n\n\nParts of this section are based on (Brunton and Kutz 2022, chap. 3).\n\n\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Sparsity and compression"
    ]
  },
  {
    "objectID": "sensing/sparsity.html",
    "href": "sensing/sparsity.html",
    "title": "12  Sparsity and Compression",
    "section": "",
    "text": "As we have seen before and maybe know from our own experience, most image and audio signals are highly compressible. Here compressible means we can find a basis that allows for a sparse reprehension of our signal. Let us put this into a small definition.\n\n\n\n\n\n\n\nDefinition 12.1 (\\(K\\)-sparse data) A compressible signal \\(x\\in \\mathbb{R}^n\\) may be written in a sparse vector \\(s\\in \\mathbb{R}^n\\) with a basis transformation (see Definition 1.8 for the formal definition) expressed by the matrix \\(B\\in\\mathbb{R}^{n\\times n}\\) and \\[\nx = B s.\n\\] The vector \\(s\\) is called \\(K\\)-sparse if it contains exactly \\(K\\) non-zero elements.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that this does not imply that \\(B\\) is sparse.\n\n\nIf the basis is generic such as the Fourier or Wavelet basis, i.e. we do not need to store the matrix \\(B\\) but can generate it on the spot, we only need to store the \\(K\\) non-zero elements of \\(s\\) to reconstruct the original signal.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we have seen in Chapter 11 images (and audio) signals are highly compressible in the Fourier and Wavelet basis with view most entries small or zero. By setting the small value to zero we reduce the density further without a high loss of quality.\nWe see this in JPEG compression for images and MP3 compression for audio signals. If we stream an audio signal on view an image on the web we only need to transfer \\(s\\) and not \\(x\\), saving bandwidth and storage as we go.\n\n\nWe have seen in Figure 4.1 that we can use the SVD to reduce the size as well. The downside here is that we need to store \\(U\\) and \\(V\\) (Definition 4.2) even if we reduce the rank. This is rather inefficient. On the other hand, we have used SVD in Section 4.2.2 with the Eigenfaces example how we can create a basis with SVD that can be used to classify an entire class of images - human faces. Storing the basis matrices in this case is comparable cheap and it allows us to use certain aspects of the downsampling for learning purposes.\nWe also need to stress that SVD and Fourier are unitary transformations which make the move into and from the basis cheep. This is the basis for a lot of computation seen in the field of compressed sensing and compression in general.\n\n\n\n\n\n\nNote\n\n\n\nThe driving factors for compression are audio, image and video, but also raw data compression as seen with zip, 7z and all the other available algorithms.\nIt is wrong to assume that we do not see this in engineering applications. High dimensional differential equations usually have a solution on a low dimensional manifolds and therefore imply that sparsity can be seen here to.\n\n\nLet us return to image compression and follow along with the examples given in (Brunton and Kutz 2022, chaps. 3, pp98–101).\nWe can use the code provided earlier. We move from Figure 12.1 (a) to Figure 12.1 (b) via \\(\\mathcal{F}\\). From Figure 12.1 (b) to Figure 12.1 (d) we only keep the highest 5% of our values and move to Figure 12.1 (c) via \\(\\mathcal{F}^{-1}\\).\n\nShow the code for the figure\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nim = np.asarray(iio.imread(\n    \"https://www.mci.edu/en/download/27-logos-bilder?\"\n    \"download=618:mci-eu-web\"))\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\ndef myplot(A):\n    plt.figure()\n    plt.imshow(A, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.gca().set_aspect(1)\n\nA = rgb2gray(im)\nA_fft = np.fft.fft2(A)\nA_fft_sort = np.sort(np.abs(A_fft.reshape(-1)))\nmyplot(A)\n\nmyplot(np.log(np.abs(np.fft.fftshift(A_fft)) + 1))\nc = 0.05\nthresh = A_fft_sort[int(np.floor((1 - c) * len(A_fft_sort)))]\nA_fft_th = A_fft * (np.abs(A_fft) &gt; thresh)\nA_th = np.fft.ifft2(A_fft_th).real\nmyplot(A_th)\nmyplot(np.log(np.abs(np.fft.fftshift(A_fft_th)) + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Fourier coefficients\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Compressed image\n\n\n\n\n\n\n\n\n\n\n\n(d) Truncated FFT coefficients (5%)\n\n\n\n\n\n\n\nFigure 12.1: Image of MCI I and the reconstruction with various amounts of FFT coefficients left.\n\n\n\nIn order to get an idea why the Fourier transform is useful in this scenario we look at the image as a surface.\n\n\n\n\n\n\nNote\n\n\n\nIn order to make this feasible for interactive rendering we use only the upper left quarter of the image.\n\n\n\n\nShow the code for the figure\nimport plotly.graph_objects as go\nimport numpy as np\n%config InlineBackend.figure_formats = ['svg']\n\nB = np.transpose(A[:int(A.shape[0]/2):5, :int(A.shape[1]/2):5])\ny = np.arange(B.shape[0])\nx = np.arange(B.shape[1])\n\nX,Y = np.meshgrid(x,y)\nfig = go.Figure()\nfig.add_trace(go.Surface(z=B, x=X, y=Y))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 12.2: Upper left quarter of the MCI I image as a 3D surface.\n\n\n\n\nAs can be seen in Figure 12.2 we can express the clouds with view modes and even the raster of the building seams to fit this model nicely.\nIt is not very surprising to have such structure in a so called natural image. The image or pixel space is big, very big. For an \\(n \\times n\\) black and white image there are \\(2^{n^2}\\) possible images. So for a \\(20 \\times 20\\) image we already have \\(2^{400}\\) possible images which a number with a number with 121 digits and it is assumed that there are (only) about \\(10^{82}\\) atoms in the universe.\n\n\n\n\n\n\nFigure 12.3: Illustration to show the fastness of pixel (image) space in comparison to images we can make sense of aka natural images.\n\n\n\nSo finding structure in images, especially in images with high resolution is not surprising. The rest is basically random noise. Most of the dimensions of pixel space are only necessary if we want to encode this random noise images but for a cloud with some mountains and a building only need some dimensions and are therefore highly compressible.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Sparsity and Compression</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html",
    "href": "sensing/compsensing.html",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "13.1 The theoretic basics of compressed sensing\nWe use compression in everyday life extensively. Nevertheless, it almost always requires us to have the high resolution data/measurement and than compress it down. Here we discuss the reverse, where we collect relatively few compressed or random measurements and then infer a sparse representation.\nUntil recent developments the computation was non-polynomial (NP) hard problem but now there are algorithms to reconstruct the full signal with high probability using convex algorithms.\nAs we will see, the selection of the matrix \\(C\\) is important to allow for the optimization to be such that we do not need brute-force to look for \\(\\check{s}\\). This brute-force search is combinatorial in \\(n\\) and \\(K\\), for a fixed (known) \\(K\\) but significantly larger for unknown \\(K\\).\nUnder certain condition on \\(C\\) we can relax the conditions to a convex \\(\\ell_1\\)-minimization \\[\n\\check{s} = \\operatorname{argmin}_s \\| s \\|_1, \\quad \\text{subject to} \\quad y = C B s.\n\\tag{13.1}\\]\nIn order to have Equation 13.1 converge with high properbility we need to following assumptions to be met (precise description will follow):\nThe idea of the two conditions is to make \\(CB\\) act more or less like a unitary transformation on \\(K\\)-sparse vectors \\(s\\), preserving relative distance between vectors and allowing for the \\(\\ell_1\\) convex optimization. We will see this in therms of the restricted isometry property (RIP).\nWe know of the Shannon-Nyquist sampling theorem:\ntelling us that we need a sampling rate of at least double the highest frequency to recover the signal properly. However, this is only true for signals with broadband frequency content and this is hardly ever the case for uncompressed signals. As we expect an uncompressed signal can be expressed as a sparse signal in the correct basis we can relax the Shannon-Nyquist theorem. Nevertheless, we need precise timing for our measurements and the recovery is not guaranteed, it is possible with high probability.\nChapter 7 actually serve as our first example and shows neatly how the \\(\\ell_1\\) norm promotes sparsity whereas the \\(\\ell_2\\) solution stays dense. We can also use this example to give an insight to what with high probability means. We created a random matrix in this example with 5 times more rows than columns, unless we are very unlucky and we have a lot of linear dependency between the rows we will have infinitely many solution with high probability.\nNow that we have seen compressed sensing in action we need to bring in the theoretical basics that form this theory.\nThe key feature is to look into the geometry of sparse vectors and how these vectors are transformed via random measurements. More precisely, for large enough \\(p\\) (amount of measurements) our matrix \\(D = CB\\) of Definition 13.1 preserves the distance and inner product structure of sparse vectors. In turn this means, we need to find a matrix \\(C\\) such that \\(D\\) is a near-isometry map on sparse vectors.\nIf \\(D\\) behaves as a near isometry, it is possible to solve \\[\ny = D s\n\\] for the sparsest vector \\(s\\) using convex \\(\\ell_1\\) minimization.\nFurthermore, a general rule is, the more incoherent the measurements are the smaller we can choose \\(p\\).\nIt is difficult to compute the \\(\\delta_K\\) in the RIP and as \\(C\\) may be selected at random there is more information included in the statistical properties of \\(\\delta_K\\) for a family of matrices \\(C\\). In general, increasing \\(p\\) ill decrease \\(\\delta_K\\), same as having incoherence vectors and both improve the properties of \\(CB\\) by bringing it closer to an isometry.\nLuckily, there exist generic sampling matrices \\(C\\) that are sufficiently incoherent with resect to nearly all transform bases. In particular, Gaussian and Bernoulli random measurement matrices satisfy Definition 13.2 for a generic \\(B\\) (with high probability).",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#the-theoretic-basics-of-compressed-sensing",
    "href": "sensing/compsensing.html#the-theoretic-basics-of-compressed-sensing",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "Note\n\n\n\nAn isometry map is a map that is distance preserving, i.e. the distance between to points is not changed under this map.\n\\[\n\\| a - b \\| = \\| f(a) - f(b) \\|\n\\]\nA unitary map is a map that preserves distance and angle between vectors, i.e. \\[\n\\langle a, b \\rangle = \\langle f(a), f(b) \\rangle\n\\] For a linear map \\(f(x) = Ux\\) this results in \\(U^\\mathrm{H}U = UU^\\mathrm{H} = I\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 13.2 (Restricted Isometry Property (RIP)) For \\(p\\) incoherent measurements the matrix \\(D=CB\\) satisfies a restricted isometry property (RIP) for sparse vectors \\(s\\)\n\\[\n(1-\\delta_K)\\|s\\|_2^2 \\leq \\|C B s\\|_2^2 \\leq (1+\\delta_K)\\|s\\|_2^2,\n\\] with the restricted isometry constant \\(\\delta_K\\). For a small enough \\(\\delta_K\\) \\(CB\\) acts as a near-isometry on \\(K\\)-sparse vectors.\nIn particular, for \\(\\delta_K &lt; 1\\) and therefore \\((1-\\delta_K) &gt; 0\\) and \\((1+\\delta_K) &gt; 0\\) this means the norm induced by \\(CB\\) is equivalent to the two norm on the \\(K\\)-sparse vectors.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#sparse-regression",
    "href": "sensing/compsensing.html#sparse-regression",
    "title": "13  Compressed Sensing",
    "section": "13.2 Sparse Regression",
    "text": "13.2 Sparse Regression\nLet us return to regression for a moment an in particular to the LASSO method introduced in Section 7.1.1. We have seen that \\(\\ell_1\\), i.e. the \\(\\|\\cdot\\|_1\\), promotes sparsity and we can use this to create a more robust method that rejects outliers.\nWe split up our points into a training and test set (the classic 80:20 split). For the test set an varying the parameter \\(\\lambda_1\\) through a range of values we create a fit with the training set and test against the test set.\n\n\n\n\n\n\n\nExample 13.2 (LASSO fit for a regression problem) For our \\[\nA x = b\n\\] problem, we consider a problem the dimensions as \\(200\\) observations with \\(10\\) candidate predictions. This results in a matrix \\(A\\in\\mathcal{R}^{200 \\times 10}\\) and we select the vector \\(b\\in\\mathcal{R}^{200}\\) as the linear combination of exactly two of theses \\(10\\) candidates. As a result, the vector \\(x\\) is \\(2\\)-sparse by construction, and the aim is to recover this. In order to give the algorithm something to work on we add noise to \\(b\\) resulting in no zero element in \\(b\\).\nTo perform a \\(10\\) fold cross validation 1 for the LASSO method we use the features of sklearn 2.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nimport pandas as pd\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\nm = 200\nn = 10\n\nA = np.random.randn(m, n)\nx = np.array([0, 0, 1, 0, 0, 0, -1, 0, 0, 0])\nb = A @ x + 2 * np.random.randn(m)\n\n# k-cross validation for the Lasso method\ncv = 10\nlassoCV = linear_model.LassoCV(cv=cv, random_state=6020, tol=1e-8)\nlassoCV.fit(A, b)\n# Recompute the lasso method for the optimal lambda selected\nlasso_best = linear_model.Lasso(alpha=lassoCV.alpha_, random_state=6020)\nlasso_best.fit(A, b)\n\n# Plotting\nplt.figure()\nLmean = lassoCV.mse_path_.mean(axis=-1)\nerror = [np.min(lassoCV.mse_path_, axis=-1), \n         np.max(lassoCV.mse_path_, axis=-1)] / np.sqrt(cv)\nplt.errorbar(lassoCV.alphas_, Lmean, yerr=error, ecolor=\"lightgray\")\nplt.plot(lassoCV.alpha_,\n         Lmean[lassoCV.alphas_==lassoCV.alpha_],\n         \"go\", mfc='none')\nplt.xscale(\"log\")\nplt.ylabel(\"Means Square Error\")\nplt.xlabel(r\"$\\lambda_1$\")\nplt.gca().invert_xaxis()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 13.3: Cross-validation mean square error of Lasso fit, the green circle marks the optimal choice, the blue line the mean error of the k-folds and the error bar the maximal and minimal error form the k-folds.\n\n\n\n\n\nThe resulting coefficients for the best fit with lasso are\n\n\narray([ 0.        ,  0.        ,  1.0978465 ,  0.        , -0.        ,\n       -0.        , -1.06495486,  0.        ,  0.        ,  0.        ])\n\n\nand a appropriate fit for our toy example.\n(Compare Brunton and Kutz 2022, 115)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is also possible to build a dictionary and and look for a sparse representation in this dictionary.\nAn example how this can be done with the Eigenfaces example can be found in (Brunton and Kutz 2022, 117)",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#robust-principal-component-analysis-rpca",
    "href": "sensing/compsensing.html#robust-principal-component-analysis-rpca",
    "title": "13  Compressed Sensing",
    "section": "13.3 Robust Principal Component Analysis (RPCA)",
    "text": "13.3 Robust Principal Component Analysis (RPCA)\nWe discussed the principal component analysis in Section 4.2 as an application for the SVD. Similar as regression, also the PCA is sensitive to outliers and corrupt data. In Candès et al. (2011) an algorithm to make it more robust was developed.\nThe main idea of the paper is to decompose a matrix \\(X\\) into a structured low-rank matrix \\(L\\) and a sparse matrix \\(S\\) containing the outliers and corrupt data \\[\nX = L + S.\n\\] If we can recover the principal components of \\(L\\) from \\(X\\) we have a robust method as the perturbation of \\(S\\) has little influence.\nIt might not be immediately obvious but applications of this split are video surveillance where the background is represented by \\(L\\) and the foreground objects in \\(L\\), face recognition with the eigenfaces in \\(L\\) and shadows, occlusions (like glasses or masks) are in \\(S\\).\nThe task boils down to an optimization problem of the form \\[\n\\min_{L, S} \\operatorname{rank}(L) + \\| S \\|_0 \\quad\\text{subject to}\\quad L+S = X,\n\\tag{13.2}\\] where unfortunately both parts are not convex but we can search for it with high probability using \\[\n\\min_{L, S} \\|L\\|_{\\star} + \\lambda \\| S \\|_1 \\quad\\text{subject to}\\quad L+S = X.\n\\tag{13.3}\\] In Equation 13.3 \\(\\|\\cdot\\|_{\\star}\\) is called the nuclear norm consisting of the sum of all singular values and we use it as our proxy for the \\(\\operatorname{rank}\\).\nEquation 13.3 is called principal component pursuit (PCP) and in Candès et al. (2011) the authors show that Equation 13.3 converges to Equation 13.2 for\n\n\\(\\lambda = 1/\\sqrt{\\max(n,m)}\\) for \\(X^{m \\times n}\\),\n\\(L\\) is not sparse,\n\\(S\\) is not low-rank, where we assume that the entries do not span a low-dimensional column space.\n\nWe can solve the PCP with an augmented Lagrange multiplier algorithm such as \\[\n\\mathcal{L}(L, S, Y) = \\|L\\|_{\\star} + \\lambda \\| S \\|_1 + \\langle Y, X - L - S \\rangle + \\frac{\\mu}{2} \\|X-L-S\\|_F^2.\n\\]\nThis is an iterative method where we solve for \\(L_k\\) and \\(S_k\\), update \\(Y_{k+1} = Y_k + \\mu (X-L_k - S_k)\\) and check for convergence. For this specific problem the alternations method (ADM) provides a simple procedure to solve for \\(L\\) and \\(S\\).\n\n\n\n\n\n\n\nExample 13.3 (Robust principal component analysis with Yale B dataset) The following implementation of RPCA can be found in (Brunton and Kutz 2022, 121) with the same application to the eigenfaces dataset.\n\n\nRobust principal component analysis and helper functions\ndef shrink(X, tau):\n    Y = np.abs(X) - tau\n    return np.sign(X) * np.maximum(Y, np.zeros_like(Y))\n\n\ndef SVT(X, tau):\n    U, S, VT = np.linalg.svd(X, full_matrices=False)\n    out = U @ np.diag(shrink(S, tau)) @ VT\n    return out\n\n\ndef RPCA(X):\n    n1, n2 = X.shape\n    mu = n1 * n2 / (4 * np.sum(np.abs(X.reshape(-1))))\n    lambd = 1 / np.sqrt(np.maximum(n1, n2))\n    thresh = 10**(-7) * np.linalg.norm(X)\n    \n    S = np.zeros_like(X)\n    Y = np.zeros_like(X)\n    L = np.zeros_like(X)\n    count = 0\n    while (np.linalg.norm(X - L - S) &gt; thresh) and (count &lt; 1000):\n        L = SVT(X - S + (1 / mu) * Y, 1 / mu)\n        S = shrink(X - L + (1 / mu) * Y, lambd / mu)\n        Y = Y + mu*(X - L - S)\n        count += 1\n    return L, S\n\n\n\n\nShow the code for the figure\nimport numpy as np\nimport numpy.linalg as LA\nimport scipy\nimport requests\nimport io\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = ['svg']\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nresponse = requests.get(\n    \"https://github.com/frankhuettner/Data_Driven_Science_Julia_Demos\"\n    \"/raw/refs/heads/main/DATA/allFaces.mat\")\n\ndata = scipy.io.loadmat(io.BytesIO(response.content))\nfaces = data[\"faces\"]\nm = int(data[\"m\"][0,0])\nn = int(data[\"n\"][0,0])\nnfaces = np.ndarray.flatten(data['nfaces'])\n\nXX = faces[:,:nfaces[0]]\nim = np.asarray(iio.imread(\n    \"https://raw.githubusercontent.com/dynamicslab/databook_python\"\n    \"/refs/heads/master/DATA/mustache.jpg\"))\nA = np.round(rgb2gray(im)/255).astype(\"uint8\")\nX = np.append(XX, (XX[:, 2] * A.T.flatten()).reshape((-1, 1)), axis=1)\n\nL, S = RPCA(X)\n\nfor index in [2, 3, -1]:\n    plt.figure()\n    plt.imshow(np.reshape(X[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n    plt.figure()\n    plt.imshow(np.reshape(L[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n    plt.figure()\n    plt.imshow(np.reshape(S[:, index], (m, n)).T, cmap=\"gray\")\n    plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(X\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(L\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(S\\) for image 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) \\(X\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n(e) \\(L\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n(f) \\(S\\) for image 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) \\(X\\) for image 3 with comic disguise\n\n\n\n\n\n\n\n\n\n\n\n(h) \\(L\\) for image 3 with comic disguise\n\n\n\n\n\n\n\n\n\n\n\n(i) \\(S\\) for image 3 with comic disguise\n\n\n\n\n\n\n\nFigure 13.4: RPCA decomposition for the Yale B dataset.\n\n\n\nIn Figure 13.4 we can see that RPCA effectively filters out occluded regions and shadows form \\(X\\) in \\(L\\). The missing part is filled in with the most consistent low-rank feature from the provided dataset. In our case we include all faces from the first person plus the third image with a fake moustache.\nWe need to stress, that we can not use this setup to reconstruct a different face obscured by a moustache.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#sparse-sensor-placement",
    "href": "sensing/compsensing.html#sparse-sensor-placement",
    "title": "13  Compressed Sensing",
    "section": "13.4 Sparse Sensor Placement",
    "text": "13.4 Sparse Sensor Placement\nSo far we have looked how to reconstruct a signal with random measurements in a generic basis. But how about placing sensors at the correct points to reconstruct the signal with high probability. This can dramatically reduce the amount of data to measure.\nWe can set tailored sensors for a particular library (our basis) instead of random sensors in a generic library.\nAmongst other things, this can be used to reconstruct faces, or classify signals, see (Brunton and Kutz 2022, chap. 3.8) and references within.\nTo see this in action we use the Python package PySensors and the example Sea Surface Temperature (SST) sensors from their documentation, see the docs accessed on the 28th of November 2024.\n\n\n\n\n\n\n\nExample 13.4 (Sea Surface Temperature (SST) sensors) For a given dataset of sea surface temperature as training data we would like to place (sparse) sensors optimal that allow us to reconstruct the temperature at any other location. This is achieved with the SSPOR algorithm from Manohar et al. (2018).\n\nShow the code for the figure\nfrom ftplib import FTP\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport netCDF4\nimport pysensors as ps\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\n\n# Import and save data locally\nftp = FTP('ftp.cdc.noaa.gov')\nftp.login()\nftp.cwd('/Datasets/noaa.oisst.v2/')\n\nfilenames = ['sst.wkmean.1990-present.nc', 'lsmask.nc']\n\nfor filename in filenames:\n    localfile = open(filename, 'wb')\n    ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n    localfile.close()\n\nftp.quit()\n\nf = netCDF4.Dataset('sst.wkmean.1990-present.nc')\nlat,lon = f.variables['lat'], f.variables['lon']\nSST = f.variables['sst']\nsst = SST[:]\n\nf = netCDF4.Dataset('lsmask.nc')\nmask = f.variables['mask']\n\nmasks = np.bool_(np.squeeze(mask))\nsnapshot = float(\"nan\")*np.ones((180,360))\nsnapshot[masks] = sst[0,masks]\n\nplt.figure()\nplt.imshow(snapshot, cmap=plt.cm.coolwarm)\nplt.xticks([])\nplt.yticks([])\nX = sst[:,masks]\nX = np.reshape(X.compressed(), X.shape)\n\n# Compute optimal sensor placement\nmodel = ps.SSPOR(\n    basis=ps.basis.SVD(n_basis_modes=25),\n    n_sensors=25\n)\nmodel.fit(X)\nsensors = model.get_selected_sensors()\n\n# Plot sensor locations\ntemp = np.transpose(0 * X[1,:])\ntemp[sensors] = 1\nimg = 0 * snapshot\nimg[masks] = temp\nplt.figure()\nplt.imshow(snapshot, cmap=plt.cm.coolwarm)\nindx = np.where(img==1)\nplt.scatter(indx[1], indx[0], 8, color='black')\nplt.xticks([])\nplt.yticks([])\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sea surface temperature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal learned sensor placements to recover sea surface temperature\n\n\n\n\n\n\n\nFigure 13.5: Finding optimal sensor placements to recover the sea surface temperature.\n\n\n\n(Compare docs of PySensors accessed on the 28th of November 2024)\n\n\n\n\nThis concludes our excursion into compressed sensing.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.\n\n\nCandès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. 2011. “Robust Principal Component Analysis?” J. ACM 58 (3). https://doi.org/10.1145/1970392.1970395.\n\n\nManohar, Krithika, Bingni W. Brunton, J. Nathan Kutz, and Steven L. Brunton. 2018. “Data-Driven Sparse Sensor Placement for Reconstruction: Demonstrating the Benefits of Exploiting Known Patterns.” IEEE Control Systems Magazine 38 (3): 63–86. https://doi.org/10.1109/MCS.2018.2810460.\n\n\nNeedell, D., and J. A. Tropp. 2009. “CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate Samples.” Applied and Computational Harmonic Analysis 26 (3): 301–21. https://doi.org/https://doi.org/10.1016/j.acha.2008.07.002.",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "sensing/compsensing.html#footnotes",
    "href": "sensing/compsensing.html#footnotes",
    "title": "13  Compressed Sensing",
    "section": "",
    "text": "We split our data into \\(10\\) sets, use 9 for the training and 1 for test and average over the results.↩︎\nInstall the package via pdm add scikit-learn.↩︎",
    "crumbs": [
      "Sparsity and compression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Compressed Sensing</span>"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Data science as we know it does not work without the solid foundations given via statistics and stochastic.\nWe have already seen some of these topics in our section about the basic properties of sets, 2  Data sets and we will build on this here to recall the most important results.",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "statistics/bayesian.html",
    "href": "statistics/bayesian.html",
    "title": "14  Bayesian Statistics",
    "section": "",
    "text": "14.1 Random variable\nIn the middle of the eighteenth century Joshua Bayes, a Presbyterian minister, set the ground work and as it so often happens was first not acknowledged by his peers.\nThe basic rule is often illustrated via a Venn diagram for sets.\nTranslated into a statistic problem this becomes: What is the probability, that a number is both, in set \\(A\\) and set \\(B\\).\nBefore we can write this down proper we need to introduce some notation. With \\(P(A)\\) we denote the probability of the event \\(A\\) and \\(P(A, B, \\ldots)\\) the probability of all events listed combined. Furthermore we call \\(P(\\neg A)\\) the probability of not \\(A\\), i.e. \\(P(\\neg A) = 1 - P(A)\\).\nNow what happens when the two events are not independent.\nTherefore, if we return to our Venn diagram in Figure 14.1 we can see this relation played out. As \\(P(A|B)\\) can be understood as the fraction of probability \\(B\\) that intersects with \\(A\\).\nLet us look at this with an example.\nA random variable \\(X\\) (often also called random quantity or stochastic variable) is a variable that is dependent on random events and is a function defined on a probability space \\((\\Omega, \\mathcal{F}, P)\\). \\(\\Omega\\) is called the sample space of all possible outcomes, \\(\\mathcal{F}\\) the event space (a set of outcomes in the sample space), and \\(P\\) is the probability function which assigns each event in the event space a probability as a number between \\(0\\) and \\(1\\).\nA very simple random variable is the one describing a coin toss. It is one for heads and zero for tails. To increase the complexity we can search for a random variable that tells us how often tails was the result for \\(n\\) coin tosses.\nTo get a better idea what \\((\\Omega, \\mathcal{F}, P)\\) and \\(X\\) is we should ask a the following question: How likely is the value of \\(X=2\\)?\nTo answer this question we need to find the probability of the event ${,:, X() = 2} that can be expressed via \\(P\\) as \\(P(X=2)=p_X(2)\\).\nIf we record all this probabilities of outcomes of \\(X\\) results in the probability distribution of \\(X\\). If this probability distribution is real valued we get the cumulative distribution function (CDF) \\[\nF_X(x) = P(X\\leq x).\n\\]",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "statistics/bayesian.html#random-variable",
    "href": "statistics/bayesian.html#random-variable",
    "title": "14  Bayesian Statistics",
    "section": "",
    "text": "14.1.1 Discrete random variables\nWe have already seen a discrete random variable as the number of heads for \\(n\\) coin tosses, other possibilities are the number of children for a person.\n\n\n\n\n\n\n\nExample 14.2 (Coin toss) For the coin toss we have \\(\\Omega=\\{heads, tails\\}\\) and \\[\nX(\\omega) =\n\\begin{cases}\n1, \\quad \\text{if}\\, \\omega = heads,\\\\\n0, \\quad \\text{if}\\, \\omega = tails.\n\\end{cases}\n\\] for a fair coin the function \\[\nF_X(x) =\n\\begin{cases}\n\\frac12, \\quad \\text{if}\\, y = 1,\\\\\n\\frac12, \\quad \\text{if}\\, y = 0.\n\\end{cases}\n\\]\nOf course this does not mean we will get exactly \\(\\frac12\\) for \\(n\\) tosses but with the rule of large numbers we see that it will end up there for a lot of flips\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\nnp.random.seed(6020)\nimport scipy.special\n\nx = range(1, 1_001, 2)\ny = []\nfor n in x:\n    y.append(np.sum(np.random.rand(n) &lt; 0.5) / n)\n\nplt.plot(x, y)\nplt.plot([0, max(x)], [1/2,  1/2])\nplt.gca().set_aspect(1000 / (3))\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14.2: Rule of large numbers for coin toss, coin toss simulated by a random numbers smaller or grater than 0.5.\n\n\n\n\n\n\n\n\n\n\n\n14.1.2 Continuous random variable\nIf the cumulative distribution function is continuous everywhere we get have a continuous random variable. For such variables it makes more sense to not look for an exact equality with a number but rather, what is the probability that \\(X\\) lies between \\([a, b]\\).\nExample could be the height of a person (if we can measure continuously). In this example it the probability that a person is exactly \\(185m\\) is zero but the question if somebodies hight is in \\([180, 190)\\) makes sense.\n\n\n\n\n\n\nNote\n\n\n\nWe introduced the Laplace transform in Chapter 9. It finds an application in probability theory where the Laplace transform of a random variable \\(X\\) is defined as the expected value of \\(X\\) with probability density function \\(f\\), i.e.\n\\[\n\\mathcal{L}\\{f\\}(s) = \\mathrm{E}\\left[\\mathrm{e}^{- s X}\\right].\n\\]\nFurthermore, we can recover the copulative distribution function of a continuous random variable \\(X\\) as \\[\nF_X(x) = \\mathcal{L}^{-1}\\left\\{ \\frac{1}{s} \\mathrm{E}\\left[\\mathrm{e}^{- s X}\\right]\\right\\}(x) = \\mathcal{L}^{-1}\\left\\{ \\frac{1}{s} \\mathcal{L}\\{f\\}(s)\\right\\}(x).\n\\]",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "statistics/bayesian.html#probability-distributions",
    "href": "statistics/bayesian.html#probability-distributions",
    "title": "14  Bayesian Statistics",
    "section": "14.2 Probability distributions",
    "text": "14.2 Probability distributions\nIt is instructive to look at some of the most common probability distributions so that we know what we can describe by these them.\n\n\n\n\n\n\n\nExample 14.3 (Binomial distribution) The binomial distribution is a discrete probability function with parameters \\(n\\) and \\(p\\). It describes the number of successes in a sequence of \\(n\\) independent experiments, where \\(p\\) describes the probability of the experiment being a success.\nThe probability density function is \\[\nf(x) = \\left(\\begin{array}{c} n\\\\ x\\end{array}\\right) p^k (1-p)^{n-k},\n\\] with \\[\n\\left(\\begin{array}{c} n\\\\ x\\end{array}\\right) = \\frac{n!}{k!(n-k)!}.\n\\] It measures the probability that we get exactly \\(x\\) successes in \\(n\\) independent trials.\nThe corresponding CDF is \\[\nF(x) = \\sum_{i=0}^{\\lfloor x \\rfloor}\\left(\\begin{array}{c} n\\\\ i\\end{array}\\right) p^i(1-p)^{n-i}.\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n%config InlineBackend.figure_formats = [\"svg\"]\n\n\nf = lambda n, p, x: np.vectorize(lambda x: math.comb(n, x))(x) * \\\n                    np.pow(p, x) * np.pow(1 - p, n - x)\n\nhelper = lambda n, p, x: np.sum(f(n, p, np.arange(0, x + 1)))\ncfd = lambda n, p, x: np.vectorize(lambda x: helper(n, p, x))(x)\n\nx = np.arange(0, 40)\nplt.figure()\nplt.plot(x, f(20, 0.5, x), \"o\", label=r\"$n=20, p=\\frac{1}{2}$\")\nplt.plot(x, f(20, 0.7, x), \"x\", label=r\"$n=20, p=\\frac{7}{10}$\")\nplt.plot(x, f(40, 0.5, x), \"d\", label=r\"$n=40, p=\\frac{1}{2}$\")\nplt.gca().set_aspect(40/(3 * 0.2))\nplt.legend()\nplt.figure()\nplt.plot(x, cfd(20, 0.5, x), \"o\", label=r\"$n=20, p=\\frac{1}{2}$\")\nplt.plot(x, cfd(20, 0.7, x), \"x\", label=r\"$n=20, p=\\frac{7}{10}$\")\nplt.plot(x, cfd(40, 0.5, x), \"d\", label=r\"$n=40, p=\\frac{1}{2}$\")\nplt.gca().set_aspect(40/(3 * 1))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability density function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative distribution function.\n\n\n\n\n\n\n\nFigure 14.3: Binomial distribution - basic functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.4 (Bernoulli distribution) The Bernoulli distribution is a discrete probability function which takes the value \\(1\\) with probability \\(p\\) and the value \\(0\\) with probability \\(q=1-p\\). It is how we can model a coin toss or anything that is expressed in a yes-no question.\nIt is the special case of a Binomial distribution Example 14.3 with \\(n=1\\).\nThe probability density function becomes \\[\nf(x) = p^x (1-p)^{1-x} \\quad\\text{for}\\quad x\\in \\{0,1\\}\n\\] and the corresponding CDF \\[\nF(x) = \\begin{cases}\n0 \\quad &\\text{if}\\, x \\leq 0,\\\\\n1-p \\quad &\\text{if}\\, 0\\leq x \\le 1,\\\\\n1 \\quad &\\text{if}\\, x \\geq 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 14.5 (Continuous uniform distribution) The continuous uniform distribution is also called the rectangular distribution and describes an outcome that lies between certain bounds.\nThe probability density function is \\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b-a} \\quad &\\text{if}\\_ a \\leq x \\leq b,\\\\\n0 \\quad &\\text{else.}\n\\end{cases}\n\\] and the corresponding CDF \\[\nF(x) =\n\\begin{cases}\n\\frac{x-a}{b-a} \\quad &\\text{if}\\_ a \\leq x \\leq b,\\\\\n0 \\quad &\\text{else.}\n\\end{cases}\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\na = 0\nb = 1\nf = lambda x: np.astype((x &gt;= a) & (x &lt; b), float) / (b - a)\ncfd = lambda x: (x - a) / (b - a) * np.astype((x &gt;= a) & (x &lt; b) , float) + \\\n                np.astype((x &gt;= b) , float)\nx = np.linspace(-1, 2, 1024)\nplt.figure()\nplt.plot(x, f(x))\nplt.gca().set_aspect(3/(3 * 1))\nplt.figure()\nplt.plot(x, cfd(x))\nplt.gca().set_aspect(3/(3 * 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability density function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative distribution function.\n\n\n\n\n\n\n\nFigure 14.4: Continuous uniform distribution - basic functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.6 (Normal distribution) The normal distribution or Gaussian distribution is a continuous probability function for a real valued random variable.\nThe probability density function is \\[\nf(x) = \\mathcal{N}[\\mu, \\sigma^2](x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\mathrm{e}^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n\\] where \\(\\mu\\) is called the mean and the parameter \\(\\delta^2\\) is the called the variance. The standard deviation of the distribution is \\(\\sigma\\). For \\(\\mu = 0\\) and \\(\\sigma^2=1\\) it is called the standard normal distribution.\nThe corresponding CDF of the standard normal distribution is \\[\n\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x \\mathrm{e}^{-\\frac{t^2}{2}}\\, \\mathrm{d}t,\n\\] and the generic form is \\[\nF(x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).\n\\] \\(\\Phi(x)\\) is expressed in terms of the error function \\[\n\\Phi(x) = \\frac12\\left(1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right), \\quad\\text{with}\\quad \\operatorname{erf}(x) = \\frac{1}{\\sqrt{2}}\\int_{0}^x\\mathrm{e}^{-t^2}\\, \\mathrm{d}t.\n\\]\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n%config InlineBackend.figure_formats = [\"svg\"]\n\nf = lambda mu, sigma, x: 1 / np.sqrt(2 * np.pi * sigma**2) * \\\n    np.exp(- (x - mu)**2 / (2 * sigma**2))\n\nphi = np.vectorize(lambda x: 1/2 * (1 + math.erf(x / np.sqrt(2))))\ncfd = lambda mu, sigma, x: phi((x - mu) / sigma)\nx = np.linspace(-3, 3, 1024)\nplt.figure()\nplt.plot(x, f(0, 1, x),\n         label=r\"$\\mu=0$, $\\sigma^2=1$\")\nplt.plot(x, f(0, np.sqrt(0.1), x),\n         label=r\"$\\mu=0$, $\\sigma^2=0.1$\")\nplt.plot(x, f(1/2, np.sqrt(0.5), x),\n         label=r\"$\\mu=\\frac{1}{2}$, $\\sigma^2=0.5$\")\nplt.gca().set_aspect(6/(3 * 1.25))\nplt.legend()\nplt.figure()\nplt.plot(x, cfd(0, 1, x),\n         label=r\"$\\mu=0$, $\\sigma^2=1$\")\nplt.plot(x, cfd(0, np.sqrt(0.1), x),\n         label=r\"$\\mu=0$, $\\sigma^2=0.1$\")\nplt.plot(x, cfd(1/2, np.sqrt(0.5), x),\n         label=r\"$\\mu=\\frac{1}{2}$, $\\sigma^2=0.5$\")\nplt.gca().set_aspect(6/(3 * 1))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability density function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative distribution function.\n\n\n\n\n\n\n\nFigure 14.5: Normal distribution - basic functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 14.7 (Inverse \\(\\Gamma\\) distribution) The main use of the inverse \\(\\Gamma\\) distribution is in Bayesian statistics.\nThe probability density function is given by \\[\nf(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{x^{\\alpha+1}}\\mathrm{e}^{-\\frac{\\beta}{x}},\n\\] for the shape parameter \\(\\alpha\\), the scale parameter \\(\\beta\\) and the name giving \\(\\Gamma\\) function.\nThe corresponding CDF is \\[\nF(x) = \\frac{\\Gamma\\left(\\alpha, \\frac{\\beta}{x}\\right)}{\\Gamma(\\alpha)}\n\\] where the numerator is called the upper incomplete gamma function.\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport scipy\n%config InlineBackend.figure_formats = [\"svg\"]\n\nf = lambda alpha, beta, x: np.pow(beta, alpha) / math.gamma(alpha) * \\\n                           np.pow(1/x, 1 + alpha) * np.exp(-beta/x)\n\ncfd = lambda alpha, beta, x: scipy.special.gdtr(alpha, beta, x)\nx = np.linspace(1e-3, 3, 1024)\nplt.figure()\nplt.plot(x, f(1, 1, x), label=r\"$\\alpha=1$, $\\beta=1$\")\nplt.plot(x, f(3, 1, x), label=r\"$\\alpha=3$, $\\beta=1$\")\nplt.plot(x, f(3, 0.5, x), label=r\"$\\alpha=3$, $\\beta=0.5$\")\nplt.gca().set_aspect(4/(3 * 5))\nplt.legend()\nplt.figure()\nplt.plot(x, cfd(1, 1, x), label=r\"$\\mu=0$, $\\sigma^2=1$\")\nplt.plot(x, cfd(3, 1, x), label=r\"$\\mu=0$, $\\sigma^2=0.1$\")\nplt.plot(x, cfd(3, 0.5, x), label=r\"$\\mu=\\frac{1}{2}$, $\\sigma^2=0.5$\")\nplt.gca().set_aspect(6/(3 * 1))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Probability density function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Cumulative distribution function.\n\n\n\n\n\n\n\nFigure 14.6: Inverse Gamma distribution - basic functions.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "statistics/bayesian.html#bayes-theorem-in-action",
    "href": "statistics/bayesian.html#bayes-theorem-in-action",
    "title": "14  Bayesian Statistics",
    "section": "14.3 Bayes’ theorem in action",
    "text": "14.3 Bayes’ theorem in action\nWhat Bayes was after was the basic question of data science: find the parameters of a model given the outcome data together with the model. The theorem can be used as an update for the probability we have with additional information and therefore fitting our parameters.\nBefore we get to an example let us consider the terms of the theorem once more.\n\nThe prior expresses our initial believe of the outcome. If we know nothing about it we can use an non-informative prior like the uniform distribution.\nThe likelihood is similar to a probability distribution but the integral is not 1. As the name suggest it is the likelihood of a certain experiment result as a function of the parameters of the model. If we select the likelihood as a conjugate to the prior we know the shape of the posterior and this can speed up the calculation.\nThe denominator are there to normalize the posterior so that the posterior becomes a probability distribution with integral 1.\nThe posterior are our main result of the so called Bayers inference. If we have multiple samples we can use it as the prior for the next.\n\nLet us look at some more elaborate examples to show how Bayes theorem can be used.\n\n14.3.1 Election prediction as instructive example\nHere we give a rudimentary introduction to election forecasts (this example follows the notes Mehrle (2024)).\n\n\n\n\n\n\nNote\n\n\n\nSo far our probabilities where not continuous functions. For this example we need continuous functions and to reflect this in the notation we use small letters.\n\n\nFirst we need to get a relative vote for party \\(x\\) which might come from the last elections or a prior poll. We assume \\(\\mu = 20\\%\\) and express the uncertainty in our estimate with a normal distribution with standard deviation of \\(\\sigma = 5\\%\\).\nThe resulting probability distribution becomes \\[\np(x) = \\mathcal{N}[\\mu, \\sigma^2](x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\mathrm{e}^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\] and in our case we get \\(p(x) = \\mathcal{N}[0.2, 0.05^2](x)\\).\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnormaldist = lambda mu, sigma, x: 1 / np.sqrt(2 * np.pi * sigma**2) * \\\n    np.exp(- (x - mu)**2 / (2 * sigma**2))\n\np = lambda x: normaldist(0.2, 0.05, x)\n\nx = np.linspace(0, 1, 1024)\nplt.figure()\nplt.plot(x, p(x))\nplt.grid(\"major\")\nplt.gca().set_aspect(1/(3 * 8))\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$p(x)$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14.7: Probability density for our initial guess of the vote for party x.\n\n\n\n\n\nNow we improvement our estimate with the help of Bayes’ theorem by asking random people what they will vote.\n\n\n\n\n\n\nNote\n\n\n\nOf course this needs to be done in the proper way to make sure it is not biased. We assume to not be biased.\n\n\nThe probability that exactly \\(k\\) people of a random sample of size \\(n\\) vote for party \\(x\\) is a best described with a binomial distribution \\[\np\\left(\\frac{k}{n} | x\\right) = \\left(\\begin{array}{c} n\\\\ k\\end{array}\\right) x^k (1-x)^{n-k}.\n\\]\n\n\nShow the code for the figure\nimport scipy\n\nbino = lambda k, n, x: scipy.special.comb(n, k) * np.pow(x, k) * \\\n                       np.pow(1-x, n - k)\np2 = lambda x: bino(2, 10, x)\n\nx = np.linspace(0, 1, 1024)\nplt.figure()\nplt.plot(x, p2(x))\nplt.grid(\"major\")\nplt.gca().set_aspect(1)\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$p(\\frac{5}{15}|x)$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14.8: Probability density of k=5 positives out of n=15 samples.\n\n\n\n\n\nIn order to compute \\(p(\\frac{k}{n})\\) we need to integrate our two results \\[\np\\left(\\frac{k}{n}\\right) = \\int_0^1 p\\left(\\frac{k}{n} | x\\right) p(x)\\,\\mathrm{d}x\n\\]\nSo for a given sample size \\(n\\) and positive samples \\(k\\) we get a new distribution.\n\n\nShow the code for the figure\np3 = lambda k, n: scipy.integrate.quad(lambda x: bino(k, n, x) * p(x), 0, 1)[0]\n\np4 = lambda k, n ,x: p(x) * bino(k, n, x) / p3(k, n)\n\nx = np.linspace(0, 1, 1024)\nplt.figure()\nplt.plot(x, p4(5, 15, x), label=r\"$n=5, k=15$\")\nplt.plot(x, p4(50, 150, x), \":\", label=r\"$n=50, k=150$\")\nplt.plot(x, p4(0, 15, x), \"-.\", label=r\"$n=0, k=15$\")\nplt.legend()\nplt.grid(\"major\")\nplt.gca().set_aspect(1 / (3 * 14))\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$p(x|\\frac{k}{n})$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14.9: Parameter likelihood after a survey with k=5 positives out of n=15 samples (solid - blue) and k=15 out of n=150 samples (dotted).\n\n\n\n\n\nIn Figure 14.9 we can see that the peak moves from \\(0.2\\) closer to \\(\\tfrac13\\) (our pooling results) and the bigger our sample size gets the more likely our change is (we get closer).\n\n\n\n\n\n\nWarning\n\n\n\nA point that is often criticised regarding the Bayesian inference we applied here is that it is highly dependent on the initial guess (Figure 14.7).\n\n\n\n\n14.3.2 Material properties\nTo identify material properties such as strength we can also use Bayes’ theorem.\n\n\n\n\n\n\nImportant\n\n\n\nThis example follows the notes Mehrle (2024) but as we do not have the data the plots are not generated interactively.\n\n\nAgain we start with an initial guess, in this case for the yield stress with \\(90\\%\\) of the samples endure in our experiment.\nFor steel S235JR this is \\(R_{aH,0} = 235 \\mathrm{MPa}\\). We use a Gaussian normal distribution with \\(\\mu_0 = 270\\mathrm{MPa}\\) and \\(\\sigma_0 = 27.3 \\mathrm{MPa}\\).\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_formats = [\"svg\"]\n\nnormaldist = lambda mu, sigma, x: 1 / np.sqrt(2 * np.pi * sigma**2) * \\\n    np.exp(- (x - mu)**2 / (2 * sigma**2))\nFinv = lambda mu, sigma, p: mu + sigma * np.sqrt(2) * \\\n                            scipy.special.erfinv(2 * p - 1) \n\nmu_0 = 270\nsigma_0 = 27.31\nRaH_0 = Finv(mu_0, sigma_0, 0.1)\n\np = lambda x: normaldist(mu_0, sigma_0, x)\n\nx = np.linspace(0, 400, 1024)\nplt.figure()\nplt.plot(x, p(x))\nplt.fill_between(x, p(x), where=(x &gt; RaH_0))\nmy_max = np.round(np.max(p(x)), 3)\nplt.plot([RaH_0, RaH_0], [0, my_max])\nplt.grid(\"major\")\nplt.gca().set_aspect(400/(3 * my_max))\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$p(R_{aH})$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14.10: Initial assumption for the yield stress of the material, the coloured part represents the 90% of samples that have a yield stress larger than 235.\n\n\n\n\n\nThis time our parameters \\(\\mu\\) and \\(\\sigma\\) are uncertain and need to be modified, furthermore, they do not follow a binomial distribution but our assumption is that they are better described by a normal distribution \\[\np(\\mu) = \\mathcal{N}(\\mu_0, \\sigma_{\\mu_0}),\\quad\\text{with}\\quad \\sigma_{\\mu_0} = \\frac{\\mu_0}{10},\n\\] and \\[\np(\\sigma) = \\mathcal{N}(\\sigma_0, \\sigma_{\\sigma_0}),\\quad\\text{with}\\quad \\sigma_{\\sigma_0} = \\frac{\\sigma_0}{10}.\n\\]\nThe likelihood of drawing a sample with yield strength \\(R_{aH}^{(i)}\\) we calculate the dependent probabilities separate by fixing the current values of \\(\\mu = \\check{\\mu}\\) and \\(\\sigma = \\check{\\sigma}\\), respectively. \\[\np\\left(R_{aH}^{(i)} | \\mu \\right) = \\mathcal{N}[\\mu, \\check{\\sigma}](R_{aH}^{(i)})\n\\] and \\[\np\\left(R_{aH}^{(i)} | \\sigma \\right) = \\mathcal{N}[\\check{\\mu}, \\sigma](R_{aH}^{(i)})\n\\] and with Bayes’ theorem we can compute \\[\np\\left( \\mu | R_{aH}^{(i)}\\right)  = \\frac{p\\left(R_{aH}^{(i)} | \\mu \\right) p(\\mu)}{\\int p\\left(R_{aH}^{(i)} | \\mu \\right) p(\\mu)\\,\\mathrm{d}\\mu},\n\\] and \\[\np\\left( \\sigma | R_{aH}^{(i)}\\right)  = \\frac{p\\left(R_{aH}^{(i)} | \\sigma \\right) p(\\sigma)}{\\int p\\left(R_{aH}^{(i)} | \\sigma \\right) p(\\sigma)\\,\\mathrm{d}\\sigma},\n\\]\nNow by computing the inverse from our above probabilities we can update our \\(\\mu_0\\) and \\(\\sigma_0\\) accordingly. This we can continue for several samples.\n\n\n\n\n\n\n\n\nFigure 14.11: Update after 10 and 100 iterations\n\n\n\n\n\nThe mean converges relatively fast and stays but the deviation still changes after 100 sample drawn. This is due to the fact that the two variables are not independent of each other and the assumption that of a normal distribution is not a good estimate for \\(\\sigma\\).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "statistics/bayesian.html#conjugate-priors",
    "href": "statistics/bayesian.html#conjugate-priors",
    "title": "14  Bayesian Statistics",
    "section": "14.4 Conjugate priors",
    "text": "14.4 Conjugate priors\nThe above updates are relatively expensive in terms of computing and are not handy. The idea is to find the updates analytically.\nIn general this is not likely to happen but for certain combinations of density functions the convert neatly into each other under Bayers’ theorem.\nFor the normal distribution this is the case and the update becomes \\[\n\\frac{1}{\\sigma_{k+1}^2} = \\frac{1}{\\sigma_{k}^2} + \\frac{1}{\\sigma^2},\n\\] and \\[\n\\mu_{k+1} = \\left(\\frac{\\mu_k}{\\sigma_{k}^2} + \\frac{\\mu}{\\sigma^2}\\right)\\sigma^2_{k+1},\n\\]\n\n\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. London, England: SAGE Publications.\n\n\nMehrle, Andreas. 2024. “Data Science.” Management Center Innsbruck, Reader for the Course.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "statistics/kalman.html",
    "href": "statistics/kalman.html",
    "title": "15  Kalman Filter",
    "section": "",
    "text": "The Kalman Filter is one of the most important contributions to engineering sciences in the 20th century and it helped to develop the space program.\nThe filter is an iterative process to estimate the parameters of that describe some system. In engineering terms it is used to estimate the underlying states of a state space model via a two-step predictor-corrector approach. For both, the state and the uncertainty \\(P\\) in this state, normal distributions are assumed and the updates are computed analytically.\n\n\n\n\n\n\nImportant\n\n\n\nWe follow Faragher (2012) for this introduction.\n\n\nThe algorithm assumes that we have a state at time \\(k\\) that evolved from a prior state at time \\(k-1\\) from the following equation \\[\nx_k = F_k x_{k-1} + B_k u_k + w_k,\n\\tag{15.1}\\] with \\(x_k\\) being the state vector, \\(u_k\\) contains the control inputs like position, velocity, etc., \\(F_k\\) is called the state transition matrix, \\(B_k\\) the control input matrix, and last \\(w_k\\) contains the noise of the process. The process noise \\(w_k\\) is assumed to be drawn from a zero mean multivariate normal distribution with covariance matrix \\(Q_k\\). Furthermore, we measure the system according to the model \\[\nz_k = H_k x_k + \\nu_k\n\\] for \\(z_k\\) being the measurements, \\(H_k\\) the transformation matrix and \\(\\nu_k\\) contains the measurement noise.\nFor this introduction we us a simple example with a train going along a track. Therefore, \\[\n\\begin{aligned}\nx_k &= \\left[\n    \\begin{array}{c} \\mathrm{x}_k \\\\ \\dot{\\mathrm{x}}_k\\end{array}\n\\right].\n\\end{aligned}\n\\] As the train can be accelerated or the driver can use the break this will be our input parameters. We can model them as a function \\(f_k\\) and the train mass is constant with \\(m\\). As a result \\(u_t\\) has the form \\[\nu_k = \\frac{f_k}{m}.\n\\] We assume that to move from state \\(k-1\\) to \\(k\\) the time \\(\\Delta t\\) is needed. Hence, \\[\n\\begin{aligned}\n\\mathrm{x}_k &= \\mathrm{x}_{k-1} + (\\dot{\\mathrm{x}}_{k-1} \\Delta t) + \\frac{f_k (\\Delta t)^2}{2m}\\\\\n\\dot{\\mathrm{x}}_k &= \\dot{\\mathrm{x}}_{k-1}\\frac{f_k \\Delta t}{m}.\n\\end{aligned}\n\\] or in matrix form \\[\nx_k = \\left[\n    \\begin{array}{c} \\mathrm{x}_k \\\\ \\dot{\\mathrm{x}}_k\\end{array}\n\\right] =\n\\left[\n    \\begin{array}{cc} 1 & \\Delta t\\\\ 0 & 1 \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} \\mathrm{x}_{k-1} \\\\ \\dot{\\mathrm{x}}_{k-1}\\end{array}\n\\right]\n+\n\\frac{f_k}{m}\n\\left[\n    \\begin{array}{c} \\frac{(\\Delta t)^2}{2} \\\\ \\Delta t\\end{array}\n\\right]\n\\] and we can see the matrices of our initial problem statement. As it is not possible to observe the true state of the system directly we use the Kalman filter as an algorithm to determine an estimate of \\(x_k\\). As the system includes uncertainties the state is given in terms of probability density functions rather than discrete values.\nSo how can we derive an estimate for the current step from known steps and the uncertainty we have in the system. This uncertainty is model as normal distribution and the parameters are stored in the covariance matrix \\(P_k\\) (zero mean is assumed). As mentioned before we need to do a prediction and the standard equation for the Kalman filter is \\[\n\\hat{x}_{k|k-1} = F_k \\hat{x}_{k-1|k-1} + B_k u_k,\n\\tag{15.2}\\] and \\[\nP_{k|k-1} = F_k P_{k-1|k-1} F_k^{\\mathsf{T}}+ Q_k.\n\\] Here \\(\\star_{k|k-1}\\) denotes the state at time \\(t_k\\) derived from the state at time \\(t_{k-1}\\).\nTo get the second equation we need to know that the variance associated with the prediction \\(\\hat{x}_{k|k-1}\\) of the unknown true value \\(x_k\\) is given by \\[\nP_{k|k-1} = E\\left[(x_k - x_{k|k-1})(x_k - x_{k|k-1})^{\\mathsf{T}}\\right].\n\\] If we now compute the difference of Equation 15.1 and Equation 15.2 we note that the state estimation errors as well as the process noise are uncorrelated and therefore their correlation is zero.\nThe measurement update equations are given by \\[\n\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k(z_k - H_k \\hat{x}_{k|k-1})\n\\] and \\[\nP_{k|k} = P_{k|k-1} + K_k H_k P_{k|k-1}\n\\] for \\[\nK_k = P_{k|k} H_k^{\\mathsf{T}}(H_kP_{k|-1}H_k^{\\mathsf{T}} + R_k)^{-1}.\n\\] Here, \\(R_k\\) is the covariance of the measurement noise.\nTo make (more) sense of the above formula we should consider the train example in more detail.\nAt time \\(t=0\\) (and \\(k=0\\)) our tain is at a specific position but due to measurement uncertainty this position is expressed as a probability distribution. As mentioned before we use a normal Gaussian distribution with mean \\(\\mu_0\\) and variance \\(\\delta_0^2\\).\n\n\n\n\n\n\nNote\n\n\n\nGoing back to our model description before, we have \\(\\hat{x}_{1|0} = \\mu_0\\) and \\(P_{1|0}=\\sigma_0^2\\).\n\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n%config InlineBackend.figure_formats = [\"svg\"]\n\nf = lambda mu, sigma, x: 1 / np.sqrt(2 * np.pi * sigma**2) * \\\n                         np.exp(- (x - mu)**2 / (2 * sigma**2))\n\nx = np.linspace(0, 2.5, 1024)\nplt.figure()\nplt.plot(x, f(1, 0.1, x), label=r\"t=0\")\nplt.gca().set_aspect(2.5 / (3 * 5))\nplt.ylim([0, 5])\nplt.legend()\nplt.figure()\nplt.plot(x, f(1.6, 0.15, x), label=r\"t=1\")\nplt.gca().set_aspect(2.5 / (3 * 5))\nplt.legend()\nplt.ylim([0, 5])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Position at time t=0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Position at time t=1.\n\n\n\n\n\n\n\nFigure 15.1: Position of the train\n\n\n\nNow we can predict the position of the train at \\(t=1\\) (\\(k=1\\)) with the data we know at time \\(t=0\\) like its maximum speed, possible acceleration and deceleration, etc.. This is can be done via Equation 15.1. We can see a possible outcome in Figure 15.1 (b) where the train has moved in positive direction but the variance has increased indicating the increased uncertainty in the position du to the fact that we do not have noise from the acceleration or deceleration happening in the span \\(\\Delta t\\).\nNow at time \\(t=1\\) we can measure the position of the train. We do so via a radio signal and of course this process is also having some noise added to it resulting in a Gaussian probability to the position.\n\n\n\n\n\n\nImportant\n\n\n\nA key feature of the normal (Gaussian) distribution is that fact that the product of two Gaussian distributions is again a Gaussian distribution.\nThis process can be repeated over and over again and we always end up with a Gaussian distribution. This is a main feature in the Kalman filter.\n\\[\n\\begin{aligned}\n\\mathcal{N}[\\mu_1, \\sigma_1](x) \\cdot \\mathcal{N}[\\mu_2, \\sigma_2](x) &=\n\\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}}\\mathrm{e}^{-\\frac{(x - \\mu_1)^2}{2\\sigma_1^2}} \\cdot\n\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}\\mathrm{e}^{-\\frac{(x - \\mu_2)^2}{2\\sigma_2^2}} \\\\\n&=\n\\frac{1}{2 \\pi \\sqrt{\\sigma_1^2\\sigma_2^2}}\\mathrm{e}^{-\\left(\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} + \\frac{(x - \\mu_2)^2}{2\\sigma_2^2}\\right)}\\\\\n&=\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\mathrm{e}^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\\\\n&= \\mathcal{N}[\\mu, \\sigma](x)\n\\end{aligned}\n\\] with \\[\n\\mu = \\frac{\\mu_1 \\sigma_2^2 + \\mu_2 \\sigma_1^2}{\\sigma_1^2 + \\sigma_2^2} = \\mu_1 + \\frac{\\sigma_1^2 (\\mu_2 - \\mu_1)}{\\sigma_1^2 + \\sigma_2^2}\\qquad\n\\sigma^2 = \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2} = \\sigma_1^2 - \\frac{\\sigma_1^4}{\\sigma_1^2 + \\sigma_2^2}\n\\tag{15.3}\\]\n\n\nLet us use this to get an estimate of our train after the measurement by multiplying the two Gaussian distributions, as seen in Figure 15.2.\n\n\nShow the code for the figure\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n%config InlineBackend.figure_formats = [\"svg\"]\n\nf = lambda mu, sigma, x: 1 / np.sqrt(2 * np.pi * sigma**2) * \\\n    np.exp(- (x - mu)**2 / (2 * sigma**2))\n\nupdate = lambda mu, sigma: ((mu[0] * np.pow(sigma[1], 2) + mu[1] * \n                             np.pow(sigma[0], 2)) / \\\n                            (np.sum(np.pow(sigma, 2))),\n                            np.prod(sigma) / np.sqrt(np.sum(np.pow(sigma, 2))))\n\nx = np.linspace(0, 2.5, 1024)\nplt.figure()\nplt.plot(x, f(1.6, 0.2, x), label=r\"predictor\")\nplt.plot(x, f(2, 0.1, x), label=r\"corrector\")\nplt.plot(x, f(*update([1.6, 2.0], [0.2, 0.1]), x), label=r\"updated position\")\nplt.gca().set_aspect(2.5 / (3 * 5))\nplt.legend()\nplt.ylim([0, 5])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 15.2: Position of the train at t=1 with measurement and resulting distribution.\n\n\n\n\n\nThe new distribution considers the information of our guess and the update provided by the measurement and therefore our best estimation of the current position of the train. Equation 15.3 represents the update step of the Kalman filter Equation 15.2.\nTo bring in all the components of the Kalman filter we assume that the measurement and the prediction is not in the same domain and therefore we need the matrix \\(H_k\\).\nTo do so assume that the signal for the estimate is a radio signal that travels with the speed of light \\(c\\) and therefore we need to transform \\(\\mathcal{N}[\\mu_2, \\sigma_2](x)\\) as \\(\\mathcal{N}[\\frac{\\mu_2}{c}, \\frac{\\sigma_2}{c}](x)\\) and our update becomes\n\\[\n\\mu = \\mu_1 + K (\\mu_2 - H \\mu_1) \\qquad \\sigma^2 = \\sigma_1^2 - KH\\sigma_1^2,\n\\] with \\[\nH = \\frac{1}{c} \\qquad K = \\frac{H\\sigma_1^2}{H^2\\sigma_1^2+\\sigma_2^2}.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nGoing back to our model description before, we have \\(z_{k} = \\mu_2\\) and \\(R_{k}=\\sigma_2^2\\), \\(H=H_k\\), \\(K=K_k\\).\n\n\nThis the qualities given in the notes it is easy to see how the Kalman filter fits into our little illustration.\nThere are three main applications for the Kalman filter in engineering which differ mainly in the input provide to the model:\n\nMeasurement - since by definition the input is not known, it is neglected in the model (\\(u_k = 0\\))\nControl - the input is calculated by the controller and therefore known and considered (\\(u_k = -K_k x_k\\))\nSensor fusion - the input is provided by a measurement from a complementary sensor (\\(u_k = z_{k}\\) )\n\n\n\n\n\nFaragher, Ramsey. 2012. “Understanding the Basis of the Kalman Filter via a Simple and Intuitive Derivation.” IEEE Signal Processing Magazine 29 (5): 128–32.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Kalman Filter</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "This concludes our introduction to the basis of data science with a focus on engineering topics.\nThese notes introduced the main concepts with a clear focus of accurate mathematical representation and close illustration with programmatic examples.\nIf we need to sum up the main concept that is present in large sections of these notes it is that the correct representation is key in finding good concepts of computer based processing, To this extend the second key concept is to make sure the concepts can be handled via a computer and the student, so theory and programmatic application go hand in hand.\nBoth concepts stay true if we move to more evolved data science methods in further classes.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nCandès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. 2011.\n“Robust Principal Component Analysis?” J. ACM 58\n(3). https://doi.org/10.1145/1970392.1970395.\n\n\nCooley, James W., and John W. Tukey. 1965. “An Algorithm for the\nMachine Calculation of Complex Fourier Series.” Mathematics\nof Computation 19 (90): 297–301. https://doi.org/10.1090/s0025-5718-1965-0178586-1.\n\n\nDowney, Austin, and Laura Micheli. 2024. “Vibration Mechanics: A\nPractical Introduction for Mechanical, Civil, and Aerospace\nEngineers.” https://doi.org/10.5281/ZENODO.12539013.\n\n\nFaragher, Ramsey. 2012. “Understanding the Basis of the Kalman\nFilter via a Simple and Intuitive Derivation.” IEEE Signal\nProcessing Magazine 29 (5): 128–32.\n\n\nGauß, Carl Friedrich. 1866. “Theoria Interpolationis Methodo Nova\nTractata.” Göttingen: Königliche Gesellschaft der Wissenschaften.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.\n\n\nInden, Michael. 2023. Python Lernen – Kurz & Gut -.\nSebastopol: O’Reilly.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics.\nLondon, England: SAGE Publications.\n\n\nManohar, Krithika, Bingni W. Brunton, J. Nathan Kutz, and Steven L.\nBrunton. 2018. “Data-Driven Sparse Sensor Placement for\nReconstruction: Demonstrating the Benefits of Exploiting Known\nPatterns.” IEEE Control Systems Magazine 38 (3): 63–86.\nhttps://doi.org/10.1109/MCS.2018.2810460.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis 3e. 3rd ed.\nSebastopol, CA: O’Reilly Media. https://wesmckinney.com/book/.\n\n\nMehrle, Andreas. 2024. “Data Science.”\nManagement Center Innsbruck, Reader for the Course.\n\n\nMeyberg, Kurt, and Peter Vachenauer. 1992. Höhere\nMathematik 2. Springer-Lehrbuch. New York, NY: Springer.\n\n\nNeedell, D., and J. A. Tropp. 2009. “CoSaMP: Iterative Signal\nRecovery from Incomplete and Inaccurate Samples.” Applied and\nComputational Harmonic Analysis 26 (3): 301–21. https://doi.org/https://doi.org/10.1016/j.acha.2008.07.002.\n\n\nRitchie, Hannah, Lucas Rodés-Guirao, Edouard Mathieu, Marcel Gerber,\nEsteban Ortiz-Ospina, Joe Hasell, and Max Roser. 2023. “Population\nGrowth.” Our World in Data.\n\n\nVasiliev, Yuli. 2022. Python for Data Science - a Hands-on\nIntroduction. München: No Starch Press.",
    "crumbs": [
      "References"
    ]
  }
]