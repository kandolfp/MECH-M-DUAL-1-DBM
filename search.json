[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basics of Data Science",
    "section": "",
    "text": "Warning\n\n\n\nThe notes presented here are still under construction and can change without warning.\n\n\n\nPreface\nThese are the lecture notes for the Grundlagen der Datenbasierten Methoden class, part of the MCI | The Entrepreneurial School master for Mechatronik - Automation, Robotics & AI in the winter term 2024/25.\n\n\nAcknowledgements\nWe want to thank the open source community for providing excellent tutorial and guides for Data Science topics in and with Python on the web. Individual sources are cited at the appropriate spot throughout document at hand.\nWe want to thank Mirjam Ziselsberger for testing, checking, suggestions and general proofreading.\nThese notes are build with Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In this class we are going to look at the basics behind modern day Data Science. We will always try to not only discuss the theory but also use Python to illustrate it and work on it programmatically.\nFor the class we will orient ourself on the first couple of chapters of Brunton and Kutz (2022), where we will highlight sections with lose similarity.\n\n\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science and Engineering - Machine Learning, Dynamical Systems, and Control. 2nd ed. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html",
    "href": "basics/linearalgebra.html",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "1.1 Notation\nWe will refer to \\[\nv \\in \\mathbb{R}^{n}\n\\quad\n\\Leftrightarrow\n\\quad\nv = \\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right], \\quad v_i \\in \\mathbb{R},\n\\] as a vector \\(v\\) with \\(n\\) elements. The set \\((\\mathbb{R}^n, + ,\\cdot)\\) forms a so called vector space with the vector addition \\(+\\) and the scalar multiplication \\(\\cdot\\).\nv = np.array([1, 2, 3, 4])\n# show the shape\nprint(f\"{v.shape=}\")\n# access a single element\nprint(f\"{v[0]=}\")\n# use slicing to access multiple elements\nprint(f\"{v[0:3]=}\")\nprint(f\"{v[2:]=}\")\nprint(f\"{v[:2]=}\")\nprint(f\"{v[0::2]=}\")\n\nalpha = 0.5\nw = alpha * v\nprint(f\"{w=}\")\n\nv.shape=(4,)\nv[0]=np.int64(1)\nv[0:3]=array([1, 2, 3])\nv[2:]=array([3, 4])\nv[:2]=array([1, 2])\nv[0::2]=array([1, 3])\nw=array([0.5, 1. , 1.5, 2. ])\nFrom vectors we can move to matrices, where \\[\nA \\in \\mathbb{R}^{m\\times n}\n\\quad\n\\Leftrightarrow\n\\quad A = (a_{ij}) = \\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & a_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & a_{mn} \\\\  \\end{array}\n\\right],\\quad a_{ij} \\in \\mathbb{R}\n\\] is called a \\(m \\times n\\) (\\(m\\) times \\(n\\)) matrix. If its values are real numbers we say it is an element of \\(\\mathbb{R}^{m\\times n}\\).\nA = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8],\n              [9, 10, 11, 12]])\n# show the shape\nprint(f\"{A.shape=}\")\n# access a single element\nprint(f\"{A[0, 0]=}\")\n# use slicing to access multiple elements\nprint(f\"{A[0, :]=}\")\nprint(f\"{A[:, 2]=}\")\n\nA.shape=(3, 4)\nA[0, 0]=np.int64(1)\nA[0, :]=array([1, 2, 3, 4])\nA[:, 2]=array([ 3,  7, 11])\nConsequently we can say that a vector is a \\(n \\times 1\\) matrix. It is sometimes also referred to as column vector and its counterpart a \\(1 \\times n\\) matrix as a row vector.\nIf we want to refer to a row or a column of a matrix \\(A\\) we will use the following short hands:\nWe can multiply a matrix with a vector, as long as the dimensions fit. Note that usually there is no \\(\\cdot\\) used to indicate multiplication: \\[\nAv =\n\\left[\n    \\begin{array}{cccc} a_{11} & a_{12} & \\dots & a_{1n} \\\\\n                        a_{21} & a_{22} & \\dots & A_{2n} \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        a_{m1} & a_{m2} & \\dots & A_{mn} \\\\  \\end{array}\n\\right]\n\\left[\n    \\begin{array}{c} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{array}\n\\right]\n= A_{-1} v_1 + A_{-2} v_2 + \\dots + A_{-n} v_n\n\\] The result is a vector but this time in \\(\\mathbb{R}^m\\).\nIn Python the * operator is usually indicating multiplication. Unfortunately, in numpy it is interpreted as element wise multiplication, so we use @ for multiplications between vector spaces.\nw = A @ v\n# show the shape\nprint(f\"{w.shape=}\")\n# show the result\nprint(f\"{w=}\")\n# Doing the same by hand this is tricky\nw_tilde = np.zeros(A.shape[0])\nfor i, bb in enumerate(v):\n    w_tilde += A[:, i] * bb\nprint(f\"{w_tilde=}\")\n\nw.shape=(3,)\nw=array([ 30,  70, 110])\nw_tilde=array([ 30.,  70., 110.])\nAs we can see from the above equation, we can view the matrix \\(A\\) as a linear mapping or linear function between two vector spaces, namely from \\(\\mathbb{R}^{n}\\) to \\(\\mathbb{R}^{m}\\).\nA linear mapping of special interest to us is the transpose of a matrix defined by turning rows into columns and vice versa: \\[\n(A^T)_{ij} = A_{ji}.\n\\] Consequently, the transpose of a (row) vector is a column vector.\nprint(f\"{A=}\")\nprint(f\"{A.shape=}\")\nB = A.transpose()\nprint(f\"{B=}\")\nprint(f\"{B.shape=}\")\n\nA=array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\nA.shape=(3, 4)\nB=array([[ 1,  5,  9],\n       [ 2,  6, 10],\n       [ 3,  7, 11],\n       [ 4,  8, 12]])\nB.shape=(4, 3)\nWith this operation we can define the dot product or scalar product of two vectors \\(v\\) and \\(w\\) as \\[\\langle v, w\\rangle = v \\cdot w = v^T w = \\sum_i v_i w_i\\]\nv = np.array([1, 2, 3, 4])\nw = np.array([1, 1, 1, 1])\n# alternatively we can define w with\nw = np.ones(v.shape)\nalpha = np.vdot(v, w)\nprint(f\"{alpha=}\")\n\nalpha=np.float64(10.0)\nWe also have the outer product defined as: \\[\nv w^T = \\left[\n    \\begin{array}{cccc} v_1 w_1 & v_1 w_2 & \\dots & v_1 w_n \\\\\n                        v_2 w_1 & v_2 w_2 & \\dots &v_2 w_n \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        v_n w_1 & v_n w_2 & \\dots & v_n w_n \\\\  \\end{array}\n\\right]\n\\]\nC = np.outer(v, w)\nprint(f\"{C=}\")\n\nC=array([[1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [4., 4., 4., 4.]])\nWe can also multiply matrices \\(A\\) and \\(B\\) by applying the matrix vector multiplication to each column vector of \\(B\\), or a bit more elaborated:\nFor an \\({m \\times p}\\) matrix \\(A\\) and an \\({p \\times n}\\) matrix \\(B\\) the matrix-matrix multiplication (\\(\\mathbb{R}^{m\\times p} \\times \\mathbb{R}^{p\\times n} \\to \\mathbb{R}^{m\\times n}\\)) \\[C=AB \\quad \\Rightarrow\\quad c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj}\\] forms a \\({m \\times n}\\) matrix.\nC = A @ A.transpose()\nprint(f\"{C=}\")\nD = A.transpose() @ A\nprint(f\"{D=}\")\n\nC=array([[ 30,  70, 110],\n       [ 70, 174, 278],\n       [110, 278, 446]])\nD=array([[107, 122, 137, 152],\n       [122, 140, 158, 176],\n       [137, 158, 179, 200],\n       [152, 176, 200, 224]])\nFrom the above Python snippet we can easily see that matrix-matrix multiplication is not commutative.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#notation",
    "href": "basics/linearalgebra.html#notation",
    "title": "1  Linear Algebra",
    "section": "",
    "text": "Definition - Vector space\n\n\n\nFor a set \\(V\\) over a field \\(F\\) with the vectors \\(u, v, w \\in V\\) and the scalars \\(\\alpha, \\beta \\in F\\) the following assumptions need to hold true to form a vector space:\nFor the vector addition:\n\nassociativity: \\[ u + (v + w) = (u + v) +w,\\]\ncommutativity: \\[u + v = v + u,\\]\nthere exists an identity element \\(0\\in \\mathbb{R}^n\\), i.e. the zero vector \\[v + 0 =  v,\\]\nthere exists an inverse element \\[v + w =  0\\quad \\Rightarrow w\\equiv -v,\\] usually it is denoted by \\(-v\\).\n\nFor the scalar multiplication:\n\nassociativity: \\[\\alpha(\\beta v) = (\\alpha\\beta)v,\\]\nthe multiplicative identity element \\(1\\in\\mathbb{R}\\) \\[1 v = v\\]\ndistributivity with respect to vector addition: \\[\\alpha(u + v) = \\alpha u + \\alpha v,\\]\ndistributivity of the scalar addition \\[(\\alpha + \\beta)v = \\alpha v + \\beta v.\\]\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile in math we start indices with 1, Python starts with 0.\n\n\n\n\n\n\n\n\n Exercise - Vector space in Python\n\n\n\n\n\nCreate some vectors and scalars with np.array and check the above statements with + and *.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe will use capital letters for matrices and small letters for vectors.\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix as vector space?\n\n\n\n\n\nHow do we need to define \\(+\\) and \\(\\cdot\\) to say that \\((\\mathbb{R}^{m \\times n}, + ,\\cdot)\\) is forming a vector space?\nDoes np.array, +, * fulfil the properties of a vector space?\n\n\n\n\n\n\\(A_{i-}\\) for row \\(i\\),\n\\(A_{-j}\\) for _column \\(j\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition - linear map\n\n\n\nA linear map between vector spaces are mappings or functions that preserve the structure of the vector space. For two vector spaces \\(V\\) and \\(W\\) over a filed \\(F\\) the mapping \\[T: V \\to W\\] is called linear if\n\nfor \\(v, w \\in V\\) \\[T(v + w) = T(v) + T(w)\\]\nfor \\(v \\in V\\) and \\(\\alpha \\in F\\) \\[T(\\alpha v) = \\alpha T(v).\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(\\mathbb{R}^n\\) is an euclidean vector space the above function is also called the inner product.\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise - Matrix multiplication?\n\n\n\n\n\nShow that the matrix multiplication is:\n\nassociative\n(left and right) distributive\nbut not commutative",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#norms",
    "href": "basics/linearalgebra.html#norms",
    "title": "1  Linear Algebra",
    "section": "1.2 Norms",
    "text": "1.2 Norms\n\n\n\n\n\n\nDefinition - norm\n\n\n\nA norm is a mapping from a vector space \\(V\\) to the field \\(F\\) into the real numbers.\n\\[\\| \\cdot \\|: V \\to \\mathbb{R}_0^+, v \\mapsto \\|v\\|\\] if it fulfils for \\(v, w\\in V\\) and \\(\\alpha \\in F\\) the following\n\npositivity: \\[ \\|v\\| = 0 \\Rightarrow v = 0, \\]\nabsolute homogeneity: \\[ \\| \\alpha v \\| = |\\alpha| \\| v \\|, \\]\nsubadditivity (often called the triangular inequality) \\[ \\| v + w\\| \\leq  \\| v \\| + \\| w \\|.\\]\n\n\n\nThere are multiple norms that can be useful for vectors. The most common are:\n\nthe one norm \\[ \\| v \\|_1 = \\sum_i |v_i|,\\]\nthe two norm (euclidean norm) \\[ \\| w \\| = \\| v \\|_2 = \\sqrt{\\sum_i |x_i|^2} = \\sqrt{\\langle v, v \\rangle},\\]\nmore general the \\(p\\)-norms (for \\(1\\leq p \\le \\infty\\)) \\[ \\| v \\|_p = \\left(\\sum_i |v_i|^p\\right)^{\\frac1p},\\]\nthe \\(\\infty\\) norm \\[ \\| v \\|_\\infty = \\max_i |v_i|.\\]\n\nAn for metrics:\n\nthe one norm (column sum norm) \\[ \\| A \\|_1 = max_j \\sum_i |A_{ij}|,\\]\nthe Frobeniusnorm \\[ \\| A \\| = \\| A \\|_F = \\sqrt{sum_i sum_j |A_{ij}|^2}\\]\nThe \\(p\\) norms are defined \\[ \\| A \\|_p = \\left(sum_i sum_j |A_{ij}|^p\\right)^{\\frac1p}\\]\nthe \\(\\infty\\) norm (row sum norm) \\[ \\| A \\|_1 = max_i \\sum_j |A_{ij}|,\\]\n\n\n# The norms can be found in the linalg package of numpy\nfrom numpy import linalg as LA\nnorm_v = LA.norm(v)\nprint(f\"{norm_v=}\")\nnorm_v2 = LA.norm(v, 2)\nprint(f\"{norm_v2=}\")\nnorm_A = LA.norm(A, 1)\nprint(f\"{norm_A=}\")\nnorm_Afr = LA.norm(A, \"fro\")\nprint(f\"{norm_Afr=}\")\n\nnorm_v=np.float64(5.477225575051661)\nnorm_v2=np.float64(5.477225575051661)\nnorm_A=np.float64(24.0)\nnorm_Afr=np.float64(25.495097567963924)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function norm from the numpy.linalg package can be used to compute other norms or properties as well, see docs.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#basis-of-vector-spaces",
    "href": "basics/linearalgebra.html#basis-of-vector-spaces",
    "title": "1  Linear Algebra",
    "section": "1.3 Basis of vector spaces",
    "text": "1.3 Basis of vector spaces\nAs we will be using the notion of basis vector or a basis of a vector space we should introduce them properly.\n\n\n\n\n\n\nDefinition - basis\n\n\n\nA set of vectors \\(\\mathcal{B} = \\{b_1, \\ldots, b_r\\} \\in \\mathbb{R}^n\\) is called:\n\nlinear independent if \\[ \\sum_{j=1}^r \\alpha_j b_j = 0 \\Rightarrow \\alpha_1 = \\alpha_2 = \\cdots = \\alpha_r = 0,\\]\nspans \\(\\mathbb{R}^n\\) if \\[ v = \\sum_{j=1}^r \\alpha_j b_j, \\quad \\forall v \\in \\mathbb{R}^n, \\alpha_1, \\ldots, \\alpha_r \\in \\mathbb{R}\\]\n\nA set \\(\\mathcal{B}\\) is called a basis of a vector space if it is linear independent and spans the entire vector space. The size of the basis, i.e. the number of vectors in the basis, is called the dimension of the vector space.\n\n\nThe standard basis of \\(\\mathbb{R}^n\\) are the vectors \\(e_i\\) that are zero everywhere except for index \\(i\\).\nThe easiest to create one, lets say \\(e_3 \\in \\mathbb{R}^3\\), in Python is by calling\n\n# We need to keep the index shift in mind\nn = 3\ne_3 = np.zeros(n)\ne_3[3-1] = 1\nprint(f\"{e_3=}\")\n\ne_3=array([0., 0., 1.])\n\n\n\n\n\n\n\n\n Example - Standard basis\n\n\n\n\n\n\nn = 3\ne_1 = np.zeros(n)\ne_1[0] = 1\ne_2 = np.zeros(n)\ne_2[1] = 1\ne_3 = np.zeros(n) \ne_3[2] = 1\n\nx = np.random.rand(n)\nprint(f\"{x=}\")\ny = x[0] * e_1 + x[1] * e_2 + x[2] * e_3\nprint(f\"{y=}\")\nnp.testing.assert_allclose(x, y)\nnp.testing.assert_equal(e_1, e_2)\n\nx=array([0.24817926, 0.06548696, 0.24662433])\ny=array([0.24817926, 0.06548696, 0.24662433])\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[11], line 14\n     12 print(f\"{y=}\")\n     13 np.testing.assert_allclose(x, y)\n---&gt; 14 np.testing.assert_equal(e_1, e_2)\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.6/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 2 / 3 (66.7%)\nMax absolute difference among violations: 1.\nMax relative difference among violations: 1.\n ACTUAL: array([1., 0., 0.])\n DESIRED: array([0., 1., 0.])\n\n\n\n\n\n\nWe call \\[\nI_n = \\left[\n    \\begin{array}{cccc} 1 & 0 & 0 & 0 \\\\\n                        0 & 1 & \\dots & 0 \\\\  \n                        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n                        0 & 0 & \\dots & 1 \\\\  \\end{array}\n\\right]\n\\in \\mathbb{R}^{n \\times n}\n\\] the identity matrix. Note, the index \\(n\\) is often omitted as it should be clear from the dimensions of the matrix. We can also think of it as the matrix that has all the standard basis vectors combined.\n\nn = 4\nI_4 = np.eye(n)\nprint(f\"{I_4=}\")\n\nI_4=array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "href": "basics/linearalgebra.html#the-inverse-of-a-matrix",
    "title": "1  Linear Algebra",
    "section": "1.4 The inverse of a matrix",
    "text": "1.4 The inverse of a matrix\n\n\n\n\n\n\nDefinition - inverse\n\n\n\nFor a square matrices \\(A, X\\in \\mathbb{R}^{n\\times n}\\) that satisfy \\[ A X = X A = I_n \\] we call \\(X\\) the inverse of \\(A\\) and denote it by \\(A^{-1}\\).\n\n\nThe following holds tro for the inverse of matrices:\n\nthe inverse of a product is product of the inverses: \\[ (AB)^{-1} = B^{-1}A^{-1},\\]\nthe inverse of the transpose is the transpose of the inverse \\[ (A^{-1})^T = (A^{T})^{-1} \\equiv A^{-T}.\\]\n\n\nA = np.random.rand(3, 3)\nprint(f\"{A=}\")\nX = LA.inv(A)\nprint(f\"{X=}\")\nprint(f\"{A @ X=}\")\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\nnp.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\nA=array([[0.75353011, 0.31472233, 0.52310984],\n       [0.84373962, 0.19187513, 0.22710254],\n       [0.99197547, 0.06657532, 0.25957067]])\nX=array([[-0.82443739,  1.11395883,  0.68685892],\n       [-0.14903231,  7.68486848, -6.42327137],\n       [ 3.1888947 , -6.22813982,  2.87507122]])\nA @ X=array([[ 1.00000000e+00,  1.90955744e-16, -7.81001225e-17],\n       [-1.39287224e-16,  1.00000000e+00, -1.37509464e-16],\n       [-1.22168620e-16, -4.52678765e-17,  1.00000000e+00]])\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[13], line 6\n      4 print(f\"{X=}\")\n      5 print(f\"{A @ X=}\")\n----&gt; 6 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n      7 np.testing.assert_equal(A @ X, np.eye(A.shape[0]))\n\n    [... skipping hidden 1 frame]\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/_utils/__init__.py:85, in _rename_parameter.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)\n     83             raise TypeError(msg)\n     84         kwargs[new_name] = kwargs.pop(old_name)\n---&gt; 85 return fun(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile /opt/hostedtoolcache/Python/3.12.6/x64/lib/python3.12/contextlib.py:81, in ContextDecorator.__call__.&lt;locals&gt;.inner(*args, **kwds)\n     78 @wraps(func)\n     79 def inner(*args, **kwds):\n     80     with self._recreate_cm():\n---&gt; 81         return func(*args, **kwds)\n\nFile ~/work/MECH-M-DUAL-1-DBM/MECH-M-DUAL-1-DBM/.venv/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\n    884         err_msg += '\\n' + '\\n'.join(remarks)\n    885         msg = build_err_msg([ox, oy], err_msg,\n    886                             verbose=verbose, header=header,\n    887                             names=names,\n    888                             precision=precision)\n--&gt; 889         raise AssertionError(msg)\n    890 except ValueError:\n    891     import traceback\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 7 / 9 (77.8%)\nMax absolute difference among violations: 2.22044605e-16\nMax relative difference among violations: 2.22044605e-16\n ACTUAL: array([[ 1.000000e+00,  1.909557e-16, -7.810012e-17],\n       [-1.392872e-16,  1.000000e+00, -1.375095e-16],\n       [-1.221686e-16, -4.526788e-17,  1.000000e+00]])\n DESIRED: array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nFor now, this concludes our introduction to linear algebra. We will come back to more in later sections.\n\n\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix Computations. 4th ed. Johns Hopkins Studies in the Mathematical Sciences.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Collection of interesting reads",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "summary.html#collection-of-interesting-reads",
    "href": "summary.html#collection-of-interesting-reads",
    "title": "Summary",
    "section": "",
    "text": "Semantic Versioning How to design the version of your project.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brunton, Steven L., and J. Nathan Kutz. 2022. Data-Driven Science\nand Engineering - Machine Learning, Dynamical Systems, and Control.\n2nd ed. Cambridge: Cambridge University Press.\n\n\nGolub, Gene H., and Charles F. Van Loan. 2013. Matrix\nComputations. 4th ed. Johns Hopkins Studies in the Mathematical\nSciences.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "basics/index.html",
    "href": "basics/index.html",
    "title": "Basics",
    "section": "",
    "text": "In this section we are going to discuss a lot of the mathematical basics in form of Linear Algebra as well as some topics of statistics of sets, and visualisation of sets in Python.",
    "crumbs": [
      "Basics"
    ]
  }
]