---
lightbox: true
---
# Fourier Transform {#sec-signal-fourier}

The fourier transform helps us convert a signal from the time domain to the frequency domain.
In this section our main concern is going to be one dimensional signals, but the concepts can be applied to multiple dimensions.

Before we can start defining the _Fourier Series_ we need to extend our notion of vector space to functions space.
This is done with _Hilbert_ spaces. 
The computational rules follow the same principal as in @def-vectorspace, what we want to investigate is the inner product.

::: {.callout appearance="simple"}
:::: {#def-hilbert-innerproduct} 
## Hilbert inner product
The _Hilbert inner product_ of two functions $f(x)$ and $g(x)$ is defined for $x \in [a, b]$ as:
$$
\langle f(x), g(x)\rangle = \int_a^b f(x) \overline{g}(x)\, \mathrm{d}x,
$$
where $\overline{g}(x)$ denotes the complex conjugate. 
::::
:::

At first this looks strange but it is closely related to our already known @def-dotproduct.

As a first step, if we move from real to complex vector spaces the transpose is replaced by the conjugate transposed or hermit transpose, in notation the $^\mathbf{T}$ becomes $^\mathbf{H}$.

Now consider a discrete version of $f$ and $g$ at regular intervals $\Delta x = \frac{b-a}{n-1}$ where 
$$
f_i = f(x_i) = f(a + (i - 1) \Delta x),\quad i = 1, \ldots n,
$$
same for $g_i$ and accordingly $x_1 = a + 0 \Delta x = a$ and $x_n = a + (n - 1)\Delta x = b$.

The inner product is than
$$
\langle f, g \rangle = \langle\left[\begin{array}{c}f_1 \\ f_2 \\ \vdots \\ f_n \end{array}\right], \left[\begin{array}{c}g_1 \\ g_2 \\ \vdots \\ g_n \end{array}\right] \rangle = \sum_{i=1}^n f_i \overline{g}_i = \sum_{i=1}^n f(x_i)\overline{g}(x_i).
$$

As this sum will increase by increasing $n$ we should normalize it by the factor $\Delta x$.

$$
\frac{b-a}{n-1} \langle g, f \rangle = \sum_{i=1}^n f(x_i)\overline{g}(x_i) \Delta x.
$$
If we now increase $n\to \infty$ we get $\Delta x \to 0$ and the sum transforms into the integral. 

::: {.callout appearance="simple"}
:::: {#def-hilbert-innerproduct} 
## Hilbert two norm
The _Hilbert two norm_ of the function $f(x)$ is defined for $x \in [a, b]$ as:
$$
\|f\|_2 = \sqrt{\langle f(x), g(x)\rangle} = \left(\int_a^b f(x) \overline{f}(x)\,\mathrm{d}x\right)^{\frac12},
$$
where $\overline{f}(x)$ denotes the complex conjugate. 

The set of all functions with bounded norm defines the Hilbert space $L^2(a, b)$, i.e. the set of all square integrable functions.
This space is also called the space of _Lebesgue_ integrable functions.
::::
:::

Similar as we saw projection in vector spaces related to the inner product this is true here as well. 

::: {.callout appearance="simple"}
:::: {#def-fourier-periodic} 
## Periodic function

We call a function $f\,:\, \mathbb{R} \to \mathbb{R}$ periodic with a period of $L>0$, $L$-periodic for short, if 
$$
f(x + L) = f(x),\quad\forall x \in \mathbb{R}.
$$

The following holds true for $L$-periodic functions:

1. If $L$ is a period than $nL$ for $n = 1, 2, 3, \ldots$ is a period as well.
1. If $f$ and $g$ are $L$-periodic, than $\alpha f + \beta g$ are $L$-periodic, for $\alpha, \beta \in \mathbb{C}$.
1. I $f$ is $L$-periodic it follows that $\forall a\in\mathbb{R}$
$$
\int_a^{a+T}f(x)\,\mathrm{d}x = \int_0^{T}f(x)\,\mathrm{d}x.
$$
1. If $f$ is $L$-periodic than $F(x)=f(\frac{x}{\omega})$ with $\omega = \frac{2\pi}{L}$ is $2\pi$-periodic.
::::
:::

The Fourier series is nothing else as the projection of a function with an integer period on the domain $[a, b]$ onto the orthogonal basis defined by the sine and cosine functions.

## Fourier Series

In Fourier analysis the first result is stated for a periodic and piecewise smooth function $f(x)$.

::: {.callout appearance="simple"}
:::: {#def-fourier-series} 
## Fourier Series
For a $L$-periodic function $f(x)$ we can write
$$
f(x) = \frac{a_0}{2} + \sum_{k=1}^\infty \left(a_k \cos\left(\omega kx\right)+ b_k \sin\left(\omega kx\right)\right),
$$ {#eq-fs}
for 
$$
\begin{align}
a_k = \frac{2}{L}\int_0^L f(x) \cos\left(\omega kx\right)\, \mathrm{d}x,\\
b_k = \frac{2}{L}\int_0^L f(x) \sin\left(\omega kx\right)\, \mathrm{d}x.
\end{align}
$$
where we can view the last two equations as the projection onto the orthogonal basis $\{\cos(kx), \sin(kx)\}_{k=0}^\infty$, i.e.
$$
\begin{align}
a_k = \frac{1}{\|\cos\left(\omega kx\right)\|_2^2} \langle f(x), \cos\left(\omega kx\right)\rangle, \\
b_k = \frac{1}{\|\sin\left(\omega kx\right)\|_2^2} \langle f(x), \sin\left(\omega kx\right)\rangle.
\end{align}
$$

If we perform a partial reconstruction by truncating the series at $M$ we get 
$$
\hat{f}_M(x) = \frac{a_0}{2} + \sum_{k=1}^M \left(a_k \cos\left(\omega kx\right)+ b_k \sin\left(\omega kx\right)\right).
$$
:::
::::

With the help of Euler's formula:
$$
\mathrm{e}^{\mathrm{i} kx} = \cos(kx) + \mathrm{i} \sin(kx)
$$ {#eq-euler}
we can rewrite @eq-fs as
$$
f(x) = \sum_{k=-\infty}^\infty c_k \mathrm{e}^{\omega\mathrm{i} kx}
$$
with 
$$
c_k = \frac{1}{L} \int_0^L f(x) \mathrm{e}^{-\omega\mathrm{i} kx}\, \mathrm{d}x.
$$
and for $n=1, 2, 3, \ldots$
$$
c_0 = \tfrac12 a_0, \quad c_n = \tfrac12 (a_n - \mathrm{i} b_n), \quad c_{-n} = \tfrac12 (a_n + \mathrm{i} b_n).
$$

::: {.callout-note}
If $f(x)$ is real valued than $c_k = \overline{c}_{-k}$.
:::

::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-fourier-hat}

## Fourier Series of Hat functions

We test the Fourier Series with two different hat functions.
The first represents a triangle with constant slope up and down, the second a rectangle with infinite slope in the corners.

```{python}
#| label: fig-fourier_hat
#| fig-cap: "Fourier transform of a two hat functions."
#| fig-subcap:
#|   - "Sawtooth function and the reconstruction with 7 nodes"
#|   - "Nodes of the reconstruction"
#|   - "Step function and the reconstruction with various nodes"
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

# Parameters
L = 2 * np.pi
M = 7
M2 = 50
N = 100
# Hat functions
fun = lambda x, L: 0 if abs(x) > L / 4 else (1 - np.sign(x) * x * 4 / L)
fun2 = lambda x, L: 0 if abs(x) > L / 4 else 1

# x and y for the functions
x = np.linspace(-L/2, L/2, N, endpoint=False) # <1>
dx = x[1] - x[0]
w = np.pi * 2 / L

f = np.fromiter(map(lambda x: fun(x, L), x), x.dtype)
f2 = np.fromiter(map(lambda x: fun2(x, L), x), x.dtype)

# Necessary functions
scalarproduct = lambda f, g, dx: dx * np.vecdot(f, g)
a_coeff = lambda n, f: 2 / L * scalarproduct(f, np.cos(w * n * x), dx)
b_coeff = lambda n, f: 2 / L * scalarproduct(f, np.sin(w * n * x), dx)

# f_hat_0
f_hat = np.zeros((M+1, N))
f_hat[0, :] = 1/2 * a_coeff(0, f)
f2_hat = np.zeros((M2+1, N))
f2_hat[0, :] = 1/2 * a_coeff(0, f2)

# Computation of the approximation
a = np.zeros(M)
b = np.zeros(M)
for i in range(M):
    a[i] = a_coeff(i+1, f)
    b[i] = b_coeff(i+1, f)
    f_hat[i+1, :] = f_hat[i, :] + \
        a[i] * np.cos(w * (i+1) * x) + \
        b[i] * np.sin(w * (i+1) * x)

for i in range(M2):
    f2_hat[i+1, :] = f2_hat[i, :] + \
        a_coeff(i+1, f2) * np.cos(w * (i+1) * x) + \
        b_coeff(i+1, f2) * np.sin(w * (i+1) * x)

# Figures
plt.figure(0)
plt.plot(x, f, label=r"$f$")
plt.plot(x, f_hat[-1, :], label=r"$\hat{f_7}$")
plt.xticks([])
plt.legend()
plt.gca().set_aspect(1.5)

plt.figure(1)
plt.plot(x, f_hat[0, :], label=rf"$a_{0}$")
for i in range(M):
    plt.plot(x, a[i] * np.cos(w * (i+1) * x), label=rf"$a_{i+1}\cos({i+1}\omega x)$")
plt.legend(ncol=np.ceil((M+1)/2), bbox_to_anchor=(1, -0.1))
plt.xticks([])
plt.gca().set_aspect(1.5)

plt.figure(2)
plt.plot(x, f2, label=r"$f$")
plt.plot(x, f2_hat[7, :], label=r"$\hat{f}_7$")
plt.plot(x, f2_hat[20, :], label=r"$\hat{f}_{20}$")
plt.plot(x, f2_hat[50, :], label=r"$\hat{f}_{50}$")
plt.xlabel(r"$x$")
plt.legend()
plt.gca().set_aspect(1.5)

plt.show()
```
1. Not including the endpoint is important, as this is part of the periodicity of the function.
::::
:::

::: {.callout-tip appearance="simple" collapse="true" icon=false}
## {{< fa pen-to-square >}} Exercise - Self implementation @exm-fourier-hat

Implement the code yourself by filling out the missing sections:

```{python}
#| code-fold: false
#| eval: false
#| code-summary: "Code fragment for implementation."
import numpy as np
import matplotlib.pyplot as plt

# Parameters
L = 2 * np.pi   # Interval
M = 7           # Nodes for the first function
M2 = 50         # Nodes for the second function
N = 101         # Interpolation points
# Hat functions
fun = lambda x, L: # tooth
fun2 = lambda x, L: # step

# x and y for the functions
x = # points for the evaluation ()
dx = x[1] - x[0]
w = np.pi * 2 / L

f = # fun evaluated at x
f2 = # fun2 evaluated at x

# Necessary functions
scalarproduct = lambda f, g, dx: # see definition in the notes
a_coeff = lambda n, f: # see definition in the notes
b_coeff = lambda n, f: # see definition in the notes

# f_hat_0
f_hat = np.zeros((M+1, N))
f_hat[0, :] = # a_0 for f
f2_hat = np.zeros((M2+1, N))
f2_hat[0, :] = # a_0 for f2

# Computation of the approximation
a = np.zeros(M)
b = np.zeros(M)
for i in range(M):
    a[i] = a_coeff(i+1, f)
    b[i] = b_coeff(i+1, f)
    f_hat[i+1, :] = f_hat[i, :] + \
        a[i] * np.cos(w * (i+1) * x) + \
        b[i] * np.sin(w * (i+1) * x)

for i in range(M2):
    f2_hat[i+1, :] = f2_hat[i, :] + \
        a_coeff(i+1, f2) * np.cos(w * (i+1) * x) + \
        b_coeff(i+1, f2) * np.sin(w * (i+1) * x)

# Figures
plt.figure(0)
plt.plot(x, f, label=r"$f$")
plt.plot(x, f_hat[-1, :], label=r"$\hat{f_7}$")
plt.xticks([])
plt.legend()
plt.gca().set_aspect(1.5)

plt.figure(1)
plt.plot(x, f_hat[0, :], label=rf"$a_{0}$")
for i in range(M):
    plt.plot(x, a[i] * np.cos(w * (i+1) * x), label=rf"$a_{i+1}\cos({i+1}\omega x)$")
plt.legend(ncol=np.ceil((M+1)/2), bbox_to_anchor=(1, -0.1))
plt.xticks([])
plt.gca().set_aspect(1.5)

plt.figure(2)
plt.plot(x, f2, label=r"$f$")
plt.plot(x, f2_hat[7, :], label=r"$\hat{f}_7$")
plt.plot(x, f2_hat[20, :], label=r"$\hat{f}_{20}$")
plt.plot(x, f2_hat[50, :], label=r"$\hat{f}_{50}$")
plt.xlabel(r"$x$")
plt.legend()
plt.gca().set_aspect(1.5)

plt.show()
```
:::

::: {.callout-note}
The phenomenon that the truncated Fourier series oscillates in @fig-fourier_hat-3 due to the discontinuity of the function is called the Gibbs phenomenon.
:::

## Fourier Transform

The Fourier Series is defined for $L$-periodic functions.
The Fourier transform extends this to functions with the domain extended to $\pm\infty$.

Let us start of with the series representation we already know:
$$
f(x) = \sum_{k=-\infty}^\infty c_k \mathrm{e}^{\mathrm{i} \omega k x}
$$
with the coefficients
$$
c_k = \frac{1}{2L} \int_{-L}^{L} f(x) \mathrm{e}^{-\mathrm{i} \omega_k x}\, \mathrm{d}x,
$$
with $\omega_k=\frac{k\pi}{L} = k\Delta \omega$.

If we now perform the transition for $L \to \infty$ resulting in $\Delta\omega \to 0$ and basically moving from discrete frequencies to a continuous set of frequencies.
This results in
$$
f(x) = \lim_{\Delta \omega \to 0} \sum_{k=-\infty}^\infty \frac{\Delta \omega}{2\pi}
\int_{-\tfrac{\pi}{\Delta \omega}}^{\tfrac{\pi}{\Delta \omega}} f(\xi) \mathrm{e}^{-\mathrm{i} k \Delta \omega \xi}\, \mathrm{d}\xi\,\, \mathrm{e}^{\Delta\omega\mathrm{i} kx}
$$
which is a Riemann integral and the kernel becomes the Fourier Transform of our function. 

::: {.callout appearance="simple"}
:::: {#def-fourier-transform} 
## Fourier Transform
A function $f\,:\, \mathbb{R} \to \mathbb{R}$ is called fourier transposable if
$$
\hat{f}(\omega) = \mathcal{F}\{f(x)\} = \int_{-\infty}^{\infty} f(x)\mathrm{e}^{-\mathrm{i} \omega x}\, \mathrm{d}x
$$
exists for all $\omega\in\mathbb{R}$.
In this case we call $\hat{f}(\omega) \equiv \mathcal{F}\{f(x)\}$ the **Fourier transform** of $f(x)$.

The **inverse Fourier transform** is defined as
$$
\mathcal{F}^{-1}\{\hat{f}(\omega)\} = \frac{1}{2 \pi}\int_{-\infty}^{\infty} \hat{f}(\omega)\mathrm{e}^{\mathrm{i} \omega x}\, \mathrm{d}\omega
$$
::::
:::

::: {.callout-note}

The pair $(f, \hat{f})$ is often called the _Fourier transform pair_. 

The two integrals converge, as long as both functions are Lebesgue integrable, i.e. 
$$\int_{-\infty}^\infty|f(x)|\, \mathrm{d}x \le \infty,$$
or $f, \hat{f} \in L^1[(-\infty, \infty)]$.
:::

As could be expected, the Fourier transform has properties that lead to computational advantages.

For tow functions $f, g \in L^1[(-\infty, \infty)]$ and $\alpha, \beta\in\mathbb{C}$ the following properties hold:

(@) **Linearity**
$$
\mathcal{F}\{\alpha f(x) + \beta g(x)\} = 
\alpha \mathcal{F}\{f(x)\} + \beta \mathcal{F}\{g(x)\} = 
\alpha \hat{f}(\omega)+ \beta \hat{g}(\omega),
$$
and 
$$
\mathcal{F}^{-1}\{\alpha \hat{f}(\omega) + \beta \hat{g}(\omega)\} = 
\alpha \mathcal{F}^{-1}\{\hat{f}(\omega)\} + \beta \mathcal{F}^{-1}\{\hat{g}(\omega)\} = 
\alpha f(x) + \beta g(x).
$$

(@) **Conjugation** 
$$
\mathcal{F}\{\overline{f(x)}\} = \overline{\hat{f}(-\omega)}.
$$

(@) **Scaling**, for $\alpha \neq 0$
$$
\mathcal{F}\{f(\alpha x)\} = \frac{1}{|\alpha|}\hat{f}\left(\frac{\omega}{\alpha}\right).
$$

(@) **Drift in time**, for $a\in\mathbb{R}$
$$
\mathcal{F}\{f(x - a)\} = \mathrm{e}^{-\mathrm{i}\omega a}\hat{f}(\omega).
$$

(@) **Drift in frequency**, for $a\in\mathbb{R}$

$$
\mathrm{e}^{\mathrm{i} a x} \mathcal{F}\{f(x - a)\} = \hat{f}(\omega - a).
$$

(@) If $f$ is **even** or **odd**, than $\hat{f}$ is even or odd, respectively.

(@) **Derivative in time**
$$
\mathcal{F}\{\partial_x f(x)\} = \mathrm{i} \omega \hat{f}(\omega)
$$
We are going to prove this by going through the lines
$$
\begin{align}
\mathcal{F}\left\{\frac{d}{d\,x}f(x)\right\} &= \int_{-\infty}^\infty f'(x)\mathrm{e}^{-\mathrm{i}\omega a}\, \mathrm{d}x \\
&= \left[f(x)\mathrm{e}^{-\mathrm{i}\omega a}\right]_{-\infty}^\infty - \int_{-\infty}^\infty -\mathrm{i} \omega f(x)\mathrm{e}^{-\mathrm{i}\omega a}\, \mathrm{d}x \\
&= \mathrm{i} \omega \int_{-\infty}^\infty f(x)\mathrm{e}^{-\mathrm{i}\omega a}\, \mathrm{d}x \\
&= \mathrm{i} \omega \mathcal{F}\{f(x)\}
\end{align}
$$
For higher derivatives we get
$$
\mathcal{F}\{\partial_x^n f(x)\} = \mathrm{i}^n \omega^n \hat{f}(\omega)
$$

(@) **Derivative in frequency**
$$
\mathcal{F}\{x^n f(x)\} = \mathrm{i}^n \partial_\omega^n\hat{f}(\omega)
$$

(@) The **convolution** of two functions is defined as 
$$
(f \ast g)(x) = \int_{-\infty}^{\infty}f(x - \xi) g(\xi)\, \mathrm{d}\xi,
$$
and for the Fourier transform
$$
\mathcal{F}\{(f \ast g)(x)\} = \hat{f} \cdot \hat{g}.
$$

(@) **Parseval's Theorem**
$$
\|f\|_2^2 = \int_{-\infty}^{\infty}|f(x)|^2\, \mathrm{d}x = \frac{1}{2 \pi}\int_{-\infty}^{\infty}|\hat{f}(\omega)|^2\, \mathrm{d}\omega
$$
stating that the Fourier Transform preserves the 2-norm up to a scaling factor.

## Discrete Fourier Transform

The Discrete Fourier Transform (DFT) is a way of approximating the Fourier transform on discrete vectors of data and it essentially a discretized version of the Fourier transform by sampling the function and numerical integration. 


::: {.callout appearance="simple"}
:::: {#def-fourier-dft} 

## Discrete-Fourier Transform
For equally spaced values $x_k = k\Delta x$, for $k\in\mathbb{Z}$ and $\Delta x>0$ and the discrete values of the function evaluations $f_k=f(x_k)$.
If the function is periodic with $L=N\Delta x$ than the discrete Fourier transform is given as
$$
\hat{f}_k = \sum_{j=0}^{N-1}f_j\, \mathrm{e}^{-\mathrm{i} j k\tfrac{2 \pi}{N}},
$${#eq-dft}
and its inverse (iDFT) as
$$
f_k = \frac{1}{N}\sum_{j=0}^{N-1}\hat{f}_j\, \mathrm{e}^{\mathrm{i} j k\tfrac{2 \pi}{N}}.
$$
::::
:::

As we can see, the DFT is a linear operator and therefore it can be written as a matrix vector product
$$
\left[
    \begin{array}{c} \hat{f}_1 \\ \hat{f}_2 \\ \hat{f}_3 \\ \vdots \\ \hat{f}_N \end{array}
\right]
=
\left[
    \begin{array}{ccccc} 1 & 1 & 1 & \dots & 1 \\
                         1 & \omega_N & \omega_N^2 & \dots & \omega_N^{N-1} \\  
                         1 & \omega_N^2 & \omega_N^4 & \dots & \omega_N^{2(N-1)} \\
                         \vdots & \vdots & \vdots & \ddots & \vdots\\
                         1 & \omega_N^{N-1} & \omega_N^{2(N-1)} & \dots & \omega_N^{(N-1)^2 } \\
    \end{array}
\right] 
\left[
    \begin{array}{c} f_1 \\ f_2 \\ f_3 \\ \vdots \\ f_N \end{array}
\right]
$$ {#eq-dft}
with $\omega_N = \exp({-\mathrm{i} \tfrac{2 \pi}{N}})$. 

::: {.callout-note}
The matrix of the DFT is a unitary Vandermonte matrix.
:::

As we can transfer the properties of the Fourier transform to the DFT we get the nice properties for sampled signals.

The downside of the DFT is that it does not scale well for large $N$ as the matrix-vector multiplication is $\mathcal{O}(N^2)$ and becomes slow. 

```{python}
#| code-fold: true
#| code-summary: "Show code for the computation of the DFT matrix."
import numpy as np
import matplotlib.pyplot as plt

# Parameters
N = 256
w = np.exp(-1j * 2 * np.pi / N )

J, K = np.meshgrid(np.arange(N), np.arange(N))
DFT = np.power(w, J*K)
```

## Fast Fourier Transform

In 1965, James W. Cooley (IBM) and John W. Tukey (Princeton) developed the so called _fast Fourier transform_ (FFT) that scales with $\mathcal{O}(N \log(N))$.
 which becomes almost linear for large enough $N$, see @Cooley1965.

::: {.callout-note}

To give an idea of what this change means and why this algorithm was a game changer.
Audio is most of the time sampled with 44.1kHz, i.e. 44 100 samples per second.
For a 10s audio clip the vector $f$ will have the length $N = 4.41 \times 10^5$.
The DFT computation (without generating the matrix) results ins approximately $2\times 10^{11}$ multiplications.
The FFT on the other hand requires $6\times 10^6$ leading to a speed-up of about $30 000$.

How this influenced our world we know from the use in our daily communication networks.

[Compare @Brunton2022, pp. 65-66]
:::

::: {.callout-note}
We should note that Cooley and Tukey where not the first to propose a FFT but the provided the formulation used today.
Gauss already formulated the FFT 150 years earlier in 1805 for orbital approximations.
Apparently, he did the necessary computations in his head and needed a fast algorithm so he developed the FFT.
Gauss being Gauss did not see this as something important and it did not get published until 1866 in his compiled notes, @Gauss.
:::

The main idea of the FFT is to exploit symmetries in the Fourier transform and to relate the $N$-dimensional DFT to a lower dimensional DFT by reordering the coefficients. 

::: {.callout appearance="simple"}
:::: {#def-fourier-fft} 

## Fast-Fourier Transform

If we assume that $N = 2^n$, i.e. a power of $2$, in particular $N=2M$, and $F_N$ denotes the matrix of @eq-dft for dimension $N$ and we have $\hat{f} = F_N f$ and $f = \tfrac1N \overline{F_N} \hat{f}$.
By splitting $f$ in the even and odd indices as 
$$e = [f_0, f_2, \ldots, f_{N-2}]^{\mathrm{T}}\in \mathbb{C}^{M}$$
and 
$$o = [f_1, f_3, \ldots, f_{N-1}]^{\mathrm{T}}\in \mathbb{C}^{M}$$
and for @eq-dft we get
$$
\begin{align}
\hat{f}_k &= \sum_{j=0}^{N-1}f_j\, \omega^{j k} = \sum_{j=0}^{M}f_{2j}\, \omega^{(2j) k} + \sum_{j=0}^{M}f_{2j+1}\, \omega^{(2j+1) k} \\
&= \sum_{j=0}^{M}e_j\, (\omega^2)^{j k} + \omega^k\sum_{j=0}^{M}o_{j}\, (\omega^2)^{j k}.
\end{align}
$$
If we further split $\hat{f}$ in an upper and lower part 
$$u = [\hat{f}_0, \hat{f}_2, \ldots, \hat{f}_{M-1}]^{\mathrm{T}}\in \mathbb{C}^{M}$$
and 
$$l = [\hat{f}_{M}, \hat{f}_{M+1}, \ldots, \hat{f}_{N-1}]^{\mathrm{T}}\in \mathbb{C}^{M}$$
and with the property $\omega^{k+M} = \omega^k \omega^M = - \omega^k$ we get

$$
\begin{align}
u_k &= \sum_{j=0}^{M-1}e_j\, (\omega^2)^{j k} + \omega^k\sum_{j=0}^{M}o_{j}\, (\omega^2)^{j k},\\
l_k &= \sum_{j=0}^{M-1}e_j\, (\omega^2)^{j k} - \omega^k\sum_{j=0}^{M}o_{j}\, (\omega^2)^{j k}.
\end{align}
$$
This results in the more visual matrix representation
$$
\hat{f}= F_N f = 
\left[
    \begin{array}{cc}
    I_M & D_M \\
    I_M & -D_M
    \end{array}
\right]
\left[
    \begin{array}{cc}
    F_M & 0 \\
    0 & F_M
    \end{array}
\right]
\left[
    \begin{array}{cc}
    f_{even}\\
    f_{odd}
    \end{array}
\right],
$$
for $I_M$ being the identity matrix in dimension $M$ and 
$$
D_M = 
\left[
    \begin{array}{ccccc} 1 & 0 & 0 & \dots & 0 \\
                         0 & \omega & 0 & \dots & 0 \\  
                         0 & 0 & \omega^2 & \dots & 0 \\
                         \vdots & \vdots & \ddots & \ddots &\vdots\\
                         0 & 0 & 0 & \dots & \omega^{(M-1)} \\
    \end{array}
\right]
$$
Now repeat this $n$ times.
::::
:::

::: {.callout-note}
If $N$ is not a power of $2$ padding is used to make the size fit by extending the vector with zeros.
:::

::: {.callout-note}
The original FFT paper [@Cooley1965] uses bit flipping and similar techniques to boost performance even more.
It can even be implemented to allow for in place computation to save storage.

IMAGE

[Compare @Meyberg1992-ws, pp. 331]
:::

### Examples for the FFT in action

In order to give an idea how FFT works in an application we follow the examples given in [@Brunton2022, pp. 66-76].


::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-fourier-fft-den}

## FFT for de-noising

For a signal consisting of two main frequencies $f_1 = 50$ and $f_2=120$ we construct a signal
$$
f(t) = \sin(2\pi f_1 t) + \sin(2\pi f_2 t)
$$
and add some Gaussian white noise `np.random.randn`.

We compute the FFT from the two signals and their power spectral density (PSD), i.e. 
$$
PSD(\hat{f})=\frac1N \|\hat{f}\|^2.
$$
We use the PSD to take all frequencies with a $PSD < 100$ out of our reconstruction as a filter. 
This removes noise from the signal.

```{python}
#| label: fig-fourier_fft_denoise
#| fig-cap: "Signal noise filter with FFT."
#| fig-subcap:
#|   - "Original clean signal and noisy signal."
#|   - "Scaled square norm of of the Fourier coefficients (PSD), only parts are shown."
#|   - "Original signal and de-noised signal."
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

np.random.seed(6020)
# Parameters
N = 1024
a, b = 0, 1/4
t = np.linspace(a, b, N, endpoint=False)
dt = t[1] - t[0]
f1 = 50
f2 = 120
fun = lambda t: np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)

f_clean = fun(t)
f_noise = fun(t) + 2.5 * np.random.randn(len(t))              # Add some noise

fhat_noise = np.fft.fft(f_noise)
fhat_clean = np.fft.fft(f_clean)

PSD_noise = np.abs(fhat_noise)**2 / N
PSD_clean = np.abs(fhat_clean)**2 / N

freq = (1 / (dt * N)) * np.arange(N)
L = np.arange(1, np.floor(N/4), dtype='int')

# Apply filter in spectral space
filter = PSD_noise > 100
PSDclean = PSD_noise * filter
fhat_filtered = filter * fhat_noise
f_filtered = np.fft.ifft(fhat_filtered)

# Figures
plt.figure(0)
plt.plot(t, f_noise, ":", label=r"Noisy")
plt.plot(t, f_clean, label=r"Clean")
plt.xlabel("Time [s]")
plt.ylabel(r"$f$")
plt.xlim(t[0], t[-1])
plt.ylim(-5, 5)
plt.legend(loc=1)
plt.gca().set_aspect(5e-3)

plt.figure(1)
plt.plot(freq[L], PSD_noise[L], label=r"Noisy")
plt.plot(freq[L], PSD_clean[L], label=r"Clean")
plt.xlabel("Frequency [Hz]")
plt.ylabel("PSD")
plt.xlim(0, int(freq[L[-1] + 1]))
plt.legend(loc=1)
plt.gca().set_aspect(1)

plt.figure(3)
plt.plot(t, np.real(f_filtered), label=r"Noisy")
plt.plot(t, f_clean, label=r"Clean")
plt.xlabel("Time [s]")
plt.ylabel(r"$f$")
plt.xlim(t[0], t[-1])
plt.ylim(-5, 5)
plt.legend(loc=1)
plt.gca().set_aspect(5e-3)
plt.show()
```

As can be seen in the @fig-fourier_fft_denoise-3, the reconstruction is not exact.
This is due to the fact that the reconstructed frequencies are not matched exactly plus we have some multiples that show up as well. 
In particular: 
```{python}
#| echo: false
import pandas as pd
df = pd.DataFrame({"Frequency": np.round(freq[filter], 3), "PSD": np.round(PSD_noise[filter], 3)})
df
```

Note: For @fig-fourier_fft_denoise-3 we discarded the imaginary part of the reconstruction.
::::
:::

::: {.callout-tip appearance="simple" collapse="true" icon=false}
## {{< fa pen-to-square >}} Exercise - DFT vs. FFT

Implement @exm-fourier-fft-den with DFT and FFT such that you can evaluate the runtime and create a plot showing the different runtime as well as check if the two produce the same result
:::

For the Fourier transform we stated that the multiplication with $\mathcal{F}\{\partial f\}=\mathrm{i}\omega\mathcal{F}\{f\}$, similarly we can derive a formula for the numerical derivative of a sampled function by multiplying each entry of the transformed vector by $\mathrm{i}\kappa$ for $\kappa=\tfrac{2\pi k}{N}$.
$\kappa$ is called the discrete wavenumber associated with the component $k$.

Let us explore this with the example stated in [@Brunton2022, p. 68-69].

::: {.callout-tip appearance="simple" collapse="true" icon=false}
:::: {#exm-fourier-fft-spectralderiv}

## Spectral derivative

We compute the so called spectral derivative for the function 
$$
\begin{align}
f(x) &= \cos(x)\exp\left(-\frac{x^2}{25}\right) \\
\partial_x f(x) &= -\sin(x)\exp\left(-\frac{x^2}{25}\right) - \frac{2}{25}x f(x)
\end{align}
$$

In order to porovide something to compare our results to we also compute the forward Euler finite-differences for the derivative
$$
\partial_x f(x_k) \approx \frac{f(x_{k+1}) - f(x_k)}{x_{k+1} - x_k}.
$$

```{python}
#| label: fig-fourier_fft_spectralderivative
#| fig-cap: "Computing the derivative of a function"
#| fig-subcap:
#|   - "Computation of the derivative with different methods"
#|   - "Accuracy of the methods for computing the derivative"
#|   - "Gibbs phenomenon for the spectral derivative for discontinuous functions"
#| layout-ncol: 1
#| code-fold: true
#| code-summary: "Show the code for the figure"
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_formats = ["svg"]

# Parameters
N = 128
a, b = -15, 15
L = b - a

fun = lambda x: np.cos(x) * np.exp(-np.power(x, 2) / 25)
dfun = lambda x: -(np.sin(x) * np.exp(-np.power(x, 2) / 25) + (2 / 25) * x * fun(x))

def fD(N, fun, dfun, a, b):
    x = np.linspace(a, b, N, endpoint=False)
    dx = x[1] - x[0]
    f = fun(x)
    df_DD = np.diff(f) / dx
    df_DD = np.append(df_DD, df_DD[-1])
    return df_DD, np.linalg.norm(df_DD - dfun(x)) / np.linalg.norm(df_DD)

def spD(N, fun, dfun, a, b):
    x = np.linspace(a, b, N, endpoint=False)
    f = fun(x)
    fhat = np.fft.fft(f)
    kappa = np.fft.fftfreq(N, (b - a) / (N * 2 * np.pi))
    df_hat = kappa * fhat * (1j)
    df_r = np.fft.ifft(df_hat).real
    return df_r, np.linalg.norm(df_r - dfun(x)) / np.linalg.norm(df_r)


# Finite differences
df_fD, e = fD(N, fun, dfun, a, b)
# Spectral derivative
df_spD, e = spD(N, fun, dfun, a, b)

# Figures
x = np.linspace(a, b, N, endpoint=False)
plt.figure(0)
plt.plot(x, dfun(x), label="Exact")
plt.plot(x, df_fD, "-.", label="Finite Differences")
plt.plot(x, df_spD, "--", label="Spectral")
plt.xlabel("x")
plt.ylabel(r"$\partial_x f$")
plt.legend(loc=1)
plt.gca().set_aspect(5)

plt.figure(1)
n = 19
M = range(3, n)
e_spD = np.ones(len(M))
e_fD = np.ones(len(M))
for i, j in enumerate(M):
    _, e_fD[i] = fD(2**j, fun, dfun, a, b)
    _, e_spD[i] = spD(2**j, fun, dfun, a, b)

plt.loglog(np.pow(2, M), e_fD, label="Finite differences")
plt.loglog(np.pow(2, M), e_spD, label="Spectral derivative")
plt.grid()
plt.xlabel("N")
plt.ylabel("Relative Error")
plt.legend(loc=1)
plt.gca().set_aspect(2.5e-1)

fun_saw = lambda x, L: 0 if abs(x) > L / 4 else (1 - np.sign(x) * x * 4 / L)
a, b = -np.pi, np.pi
L = b - a
fun2 = lambda x: np.fromiter(map(lambda x: fun_saw(x, L), x), x.dtype)
N = 2**10

x = np.linspace(a, b, N, endpoint=False)

plt.figure(2)
df_spD, _ = spD(N, fun2, fun2, a, b)
df_fD, _ = fD(N, fun2, fun2, a, b)
plt.plot(x, df_fD, "-.", label="Finite derivative")
plt.plot(x, df_spD, "--", label="Spectral derivative")
plt.plot(x, fun2(x), label=r"$f$")
plt.xlabel("x")
plt.xlim(-2, 2)
plt.legend(loc=1)
plt.gca().set_aspect(1)
```
As can be seen in @fig-fourier_fft_spectralderivative-2 we can reduce the error of both methods by increasing $N$.
Nevertheless, the spectral method is more accurate and converges faster.
::::
:::